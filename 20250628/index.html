<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>动手学深度学习1--预备知识 | 枫落繁花</title><meta name="keywords" content="Pytorch,Python,Deep Learning"><meta name="author" content="山河忽晚"><meta name="copyright" content="山河忽晚"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="动手学深度学习1--预备知识"><meta name="application-name" content="动手学深度学习1--预备知识"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="动手学深度学习1--预备知识"><meta property="og:url" content="https://www.dyeddie.top/20250628/index.html"><meta property="og:site_name" content="枫落繁花"><meta property="og:description" content="深度学习预备知识。"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://www.dyeddie.top/20250628/wallhaven-zy1wrg_compressed.jpg"><meta property="article:author" content="山河忽晚"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://www.dyeddie.top/20250628/wallhaven-zy1wrg_compressed.jpg"><meta name="description" content="深度学习预备知识。"><link rel="shortcut icon" href="/ye_source/favicon.png"><link rel="canonical" href="https://www.dyeddie.top/20250628/"><link rel="preconnect" href="//cdn.cbd.int"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//static.cloudflareinsights.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="codeva-5ZQ1tY7sUA"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?34e134fb3c7108fe3f44bc4e6f57c1f8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-3RZJMX45BS"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-3RZJMX45BS');
</script><script defer="defer" data-pjax="data-pjax" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;38de068f639745a0a8b37427ef001545&quot;}"></script><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: undefined,
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  root: '/',
  preloader: {"source":1},
  friends_vue_info: undefined,
  navMusic: false,
  mainTone: {"mode":"api","api":null,"cover_change":true},
  authorStatus: undefined,
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    simplehomepage: false,
    post: true
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 山河忽晚","link":"链接: ","source":"来源: 枫落繁花","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '枫落繁花',
  title: '动手学深度学习1--预备知识',
  postAI: '',
  pageFillDescription: '数据操作 + 预处理, 数组操作, 操作原理, 创建数组, 数组运算, 数组广播, 内存分配, 数据预处理, 创建数据集, 读取数据, 处理数据, 线性代数, 理论基础, 代码实现, 标量计算, 向量基础, 矩阵基础, 矩阵形状改变, 复制矩阵, 矩阵元素积, 求和运算, 指定轴求和, 平均值计算, 沿轴求和时维度不变, 沿轴累加求和, 向量点积计算, 矩阵与向量积计算, 矩阵x矩阵计算, 向量范数, 矩阵范数, 微积分, 理论基础, 标量导数, 亚导数, 向量梯度, 单向量梯度样例, 向量对向量的导数, 雅可比矩阵, 向量链式法制, 代码实现, 自动求导, 理论基础, 计算图, 两种模式, 反向累积总结, 复杂度, 代码实现学习内容深度学习基础线性神经网络多层感知机卷积神经网络循环神经网络注意力机制优化算法高性能计算并行多分布式计算机视觉目标检测语义分割自然语言处理词嵌入数据操作预处理维数组是机器学习和神经网络的主要数据结构标量表示一个类别向量一个特征向量矩阵一个样本的特征矩阵每一行表示一个样本每一列表示样本的一个特征图片宽高通道宽是列的个数有多少列高是行的个数有多少行通道包括三通道一个图片批量批量大小宽高通道批量大小一个视频批量批量大小时间宽高通道数组操作操作原理创建数组需要形状例如矩阵每个元素的数据类型例如位浮点数每个元素的值例如全是或者随机数访问元素创建数组导入张量表示一个由数值组成的数组这个数组可能有多个维度可以通过张量的属性来访问张量沿每个轴的长度的形状和张量中元素的总数只有一个维度这个维度的长度是中元素的种类数结果为一个标量数据值要想改变一个张量的形状而不改变元素数量和元素值可以调用函数改为行列赋值给使用全全其他常量或者从特定分布中随机采样的数字生成全元素生成全元素生成随机数元素通过提供包含数值的列表或嵌套列表来为所需张量中的每个元素赋予确定值数组运算常见的标准算术运算符和都可以被升级为按元素运算按元素方式可以应用更多的计算我们也可以把多个张量连结在一起把合并在一起等于几就会改变哪个维度相当于里的参数表示在第维行表示在第维列合并通过逻辑运算符构建二元张量对张量中的所有元素进行求和会产生一个单元素张量数组广播即使形状不同我们仍然可以通过调用广播机制来执行按元素操作广播机制自动复制自身可以用选择最后一个元素可以用选择第二个和第三个元素除读取外我们还可以通过指定索引来将元素写入矩阵为多个元素赋值相同的值我们只需要索引所有元素然后为它们赋值内存分配运行一些操作可能会导致为新结果分配内存执行原地操作如果在后续计算中没有重复使用我们也可以使用或来减少操作的内存开销转换为张量将大小为的张量转换为标量数据预处理是指如果我有一个原始数据我怎么把它读取进来使得我们通过机器学习的方法能够处理创建数据集创建一个人工数据集并存储在逗号分隔值文件表头房间数量房间路线名称房间价格表示未知的数读取数据从创建的文件中加载原始数据集一般读取文件使用库处理数据为了处理缺失的数据典型的方法包括插值法和删除法这里我们将考虑插值法指的均值对于中的类别值或离散值我们将视为一个类别对于字符型数值最好转变为数值类型把列里面所有出现的不同种类的值都变成一个特征给不同的类分别赋值为一个单独的特征现在和中的所有条目都是数值类型它们可以转换为张量格式传统计算会用位或位浮点数但是位浮点数对深度学习来讲计算比较慢一般指定数据类型为位浮点数线性代数理论基础标量一个值简单操作长度向量一行值向量的简单操作长度范数向量的长度就是向量的每个元素的平方求和再开根号范数以范数为例点乘正交矩阵简单操作矩阵加法矩阵数乘矩阵逐元素正弦运算特征向量和特征值不被矩阵改变方向的向量对称矩阵总是可以找到特征向量乘法矩阵乘以向量乘法矩阵乘以矩阵范数取决于如何衡量和的长度常见范数矩阵范数最小的满足上面公式的值范数特殊矩阵对称和反对称对称矩阵元素关于主对角线对称反对称矩阵主对角线元素为与元素互反正定如果一个矩阵是正定的那么它乘以任何一个列向量或行向量都大于等于向量范数平方欧几里得范数平方内积定义结果非负二次型推广刻画半正定正定矩阵对称时非零代入结果非负则半正定正交矩阵所有行都相互正交所有行都有单位长度可以写成置换矩阵置换矩阵是正交矩阵代码实现标量计算标量由只有一个元素的张量表示向量基础向量可以被视为标量值组成的列表通过张量的索引来访问任一元素访问张量的长度只有一个轴的张量形状只有一个元素矩阵基础通过指定两个分量和来创建一个形状为的矩阵矩阵的转置对称矩阵等于其转置矩阵形状改变就像向量是标量的推广矩阵是向量的推广一样我们可以构建具有更多轴的数据结构只要元素个数不发生变化就可以根据个数改变矩阵的形状行是最后一维有几列列是倒数第二维有几行复制矩阵给定具有相同形状的任意两个张量任何按元素二元运算的结果都将是相同形状的张量矩阵元素积两个矩阵的按元素乘法称为积数学符号求和运算计算向量元素的和表示任意形状张量的元素和指定轴求和指定张量沿哪一个轴来通过求和降低维度对的第维进行求和运算剩下的矩阵是去掉第维后的形状对的第维进行求和运算剩下的矩阵是去掉第维后的形状对的第维和第维都进行求和运算剩下的矩阵是去掉第维和第维后的形状平均值计算一个与求和相关的量是平均值或按某维度来计算沿轴求和时维度不变计算总和或均值时保持轴数不变时指不会把要求和的维度去掉而是把中对应的那个维度变成通过广播将除以沿轴累加求和某个轴计算元素的累积总和点积是相同位置的按元素乘积的和向量点积计算我们可以通过执行按元素乘法然后进行求和来表示两个向量的点积矩阵与向量积计算矩阵向量积是一个长度为的列向量其第个元素是点积矩阵矩阵计算我们可以将矩阵矩阵乘法看作简单地执行次矩阵向量积并将结果拼接在一起形成一个矩阵向量范数范数是向量元素平方和的平方根范数它表示为向量元素的绝对值之和矩阵范数矩阵的范数是矩阵元素平方和的平方根微积分矩阵的计算部分要清楚如何求导数理论基础标量导数导数是切线的斜率注不是的函数亚导数当不存在导数时怎么办将导数拓展到不可微的函数绝对值函数的导数函数的导数分段形式向量梯度将导数拓展到向量需要注意向量或矩阵的形状求导后梯度的形状按照分子布局符号如上图是列向量是标量求导后的结果是的形状列向量是列向量是标量求导后的结果行向量是向量是向量求导后的结果是矩阵如果是反过来的情况则称为分母布局符号单向量梯度样例是列向量是标量表达式补充说明标量与无关不是的函数标量是的函数数乘求导法则向量元素和对向量各元素求和后求导向量范数平方由求导符号说明零向量元素全是其转置行向量全向量元素全是其转置行向量用途向量微积分机器学习梯度推导基础规则表达式说明和是标量向量函数加法求导法则积是标量向量函数乘积求导法则内积是向量函数内积求导法则用途说明向量微积分基础规则用于机器学习如反向传播优化理论中推导梯度计算复杂函数对参数向量的导数向量对向量的导数雅可比矩阵给定雅可比矩阵定义首先把拆分后对向量求导再计算单独求导后的结果雅可比矩阵维度与说明常向量与无关零矩阵不随变化恒等映射单位矩阵自身对自身求导为单位矩阵矩阵向量乘法原矩阵线性变换导数是变换矩阵本身向量矩阵乘法矩阵转置转置保证导数维度补充条件维度故常量均与无关符号零矩阵单位矩阵用途向量微积分基础机器学习如反向传播控制理论中线性变换梯度推导的核心规则说明标量向量数乘数乘法则标量提至导数外矩阵向量乘法线性变换法则矩阵提至导数外向量和加法法则和的导数拆为导数和用途说明向量微积分基础规则用于机器学习如神经网络反向传播优化理论中推导线性仿射变换的梯度计算损失函数对参数的导数向量链式法制标量链式法则若复合函数经依赖则说明微积分核心求导法则是神经网络反向传播的数学基础分解复杂梯度为多层简单梯度乘积标量中间变量若标量标量维度拓展到向量向量中间变量若标量维度向量对向量的链式求导法则若经向量依赖于则维度说明矩阵矩阵矩阵合法性需满足维度匹配前导矩阵列数后续矩阵行数代码实现如果的导数存在这个极限被定义为定义通过令并让接近的数值结果接近为了对导数的这种解释进行可视化我们将使用定义几个函数使用格式在中显示绘图设置的图表大小设置的轴绘制数据点绘制函数及其在处的切线自动求导自动求导计算一个函数在指定值上的导数它有别于符号求导和数值求导理论基础计算图将代码分解成操作子将计算表示成一个无环图显示构造可实现框架隐式构造可实现框架两种模式核心原理链式法则正向积累反向累积又称反向传递反向累积总结构造计算图前向执行图存储中间结果反向从相反方向执行图去除不需要的枝复杂度计算复杂度是操作子个数通常正向和反向的代价类似内存复杂度因为需要存储正向的所有中间结果耗费计算资源的主要原因跟正向累积对比计算复杂度用来计算一个变量的梯度内存复杂度代码实现假设我们想对函数关于列向量求导在我们计算关于的梯度之前需要一个地方来存储梯度等价于默认值是现在计算通过调用反向传播函数来自动计算关于每个分量的梯度求导访问导数现在计算的另一个函数深度学习中我们的目的不是计算微分矩阵而是单独计算批量中每个样本的偏导数之和将某些计算移动到记录的计算图之外把当作一个常数而不是关于的函数求导的导数为即使构建函数的计算图需要通过控制流例如条件循环或任意函数调用我们仍然可以计算得到的变量的梯度是求欧几里得范数欧几里得范数指得就是通常意义上的距离范数例如在欧式空间里它表示两点间的距离向量的模长即使函数经过了流程控制会产生不同的函数公式每次计算的时候都会把计算图存起来空表示是一个标量',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-29 15:31:46',
  postMainColor: '#393e46',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.2.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="/ye_source/avatar1.webp"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">枫落繁花</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 类别</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 解压</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=3778678&amp;server=netease"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/apps/"><i class="fa-brands fa-canadian-maple-leaf faa-tada"></i><span> 修仙室</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> About</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/ArchLinux/" style="font-size: 1.05rem; color: rgb(200, 140, 42);">ArchLinux<sup>1</sup></a><a href="/tags/GitHub/" style="font-size: 1.05rem; color: rgb(122, 163, 142);">GitHub<sup>2</sup></a><a href="/tags/Linux/" style="font-size: 1.05rem; color: rgb(20, 92, 172);">Linux<sup>5</sup></a><a href="/tags/Numpy/" style="font-size: 1.05rem; color: rgb(141, 96, 84);">Numpy<sup>1</sup></a><a href="/tags/Oral-Practice/" style="font-size: 1.05rem; color: rgb(44, 172, 191);">Oral_Practice<sup>2</sup></a><a href="/tags/Python/" style="font-size: 1.05rem; color: rgb(102, 113, 22);">Python<sup>1</sup></a><a href="/tags/Pytorch/" style="font-size: 1.05rem; color: rgb(42, 102, 38);">Pytorch<sup>4</sup></a><a href="/tags/Skills/" style="font-size: 1.05rem; color: rgb(132, 103, 198);">Skills<sup>1</sup></a><a href="/tags/Sublime-Text/" style="font-size: 1.05rem; color: rgb(31, 173, 19);">Sublime_Text<sup>1</sup></a><a href="/tags/Ubuntu/" style="font-size: 1.05rem; color: rgb(197, 93, 146);">Ubuntu<sup>2</sup></a><a href="/tags/Xfce/" style="font-size: 1.05rem; color: rgb(31, 91, 162);">Xfce<sup>1</sup></a><a href="/tags/conda%E5%91%BD%E4%BB%A4/" style="font-size: 1.05rem; color: rgb(173, 169, 161);">conda命令<sup>1</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/06/"><span class="card-archive-list-date">六月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">8</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/04/"><span class="card-archive-list-date">四月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="post-meta-categories"><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-inbox post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url">深度学习</a></span><span class="article-meta tags"><a class="article-meta__tags" href="/tags/Pytorch/" tabindex="-1" itemprop="url"> <span> <i class="anzhiyufont anzhiyu-icon-hashtag"></i>Pytorch</span></a></span></div></div><h1 class="post-title" itemprop="name headline">动手学深度学习1--预备知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="post-meta-icon anzhiyufont anzhiyu-icon-history"></i><span class="post-meta-label">更新于</span><time itemprop="dateCreated datePublished" datetime="2025-06-29T07:31:46.483Z" title="更新于 2025-06-29 15:31:46">2025-06-29</time></span></div><div class="meta-secondline"><span class="post-meta-separator"></span><span class="post-meta-wordcount"><i class="anzhiyufont anzhiyu-icon-file-word post-meta-icon" title="文章字数"></i><span class="post-meta-label" title="文章字数">字数总计:</span><span class="word-count" title="文章字数">6.5k</span><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-clock post-meta-icon" title="阅读时长"></i><span class="post-meta-label" title="阅读时长">阅读时长:</span><span>28分钟</span></span><span class="post-meta-separator"></span><span class="post-meta-pv-cv" id="" data-flag-title="动手学深度学习1--预备知识"><i class="anzhiyufont anzhiyu-icon-fw-eye post-meta-icon"></i><span class="post-meta-label" title="阅读量">阅读量:</span><span id="busuanzi_value_page_pv"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src="https://www.dyeddie.top/20250628/wallhaven-zy1wrg_compressed.jpg"></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://www.dyeddie.top/20250628/"><header><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url">深度学习</a><a href="/tags/Pytorch/" tabindex="-1" itemprop="url">Pytorch</a><h1 id="CrawlerTitle" itemprop="name headline">动手学深度学习1--预备知识</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">山河忽晚</span><time itemprop="dateCreated datePublished" datetime="2025-06-29T07:31:46.483Z" title="undefined 2025-06-29 15:31:46">2025-06-29</time></header><p><strong>学习内容</strong></p>
<ul>
<li>深度学习基础——线性神经网络，多层感知机</li>
<li>卷积神经网络——LeNet，AlexNet，VGG，Inception，ResNet</li>
<li>循环神经网络——RNN，GRU，LSTM，seq2seq</li>
<li>注意力机制——Attention，Transformer</li>
<li>优化算法——SGD，Momentum，Adam</li>
<li>高性能计算——并行，多GPU，分布式</li>
<li>计算机视觉——目标检测，语义分割</li>
<li>自然语言处理——词嵌入，BERT</li>
</ul>
<h1 id="数据操作-预处理">数据操作 + 预处理</h1>
<p><strong>N 维数组</strong>是机器学习和神经网络的主要数据结构。</p>
<ul>
<li>0-d（标量）——<code>1.0</code>
<ul>
<li>表示一个类别</li>
</ul></li>
<li>1-d（向量）——<code>[1.0, 2.7, 3.4]</code>
<ul>
<li>一个特征向量</li>
</ul></li>
<li>2-d（矩阵）——<code>[[1.0，2.7，3.4], [5.0，0.2，4.6], [4.3，8.5，0.2]]</code>
<ul>
<li>一个样本的特征矩阵，每一行表示一个样本，每一列表示样本的一个特征</li>
</ul></li>
<li>3-d
——<code>[[[0.1，2.7，3.4], [5.0，0.2，4.6], [4.3，8.5，(0.2]], [[3.2，5.7，3.4], [5.4，6.2，3.2], [4.1，3.5，6.2]]]</code>
<ul>
<li>RGB图片（宽 x 高 x 通道）</li>
<li>宽是列的个数（有多少列），高是行的个数（有多少行），通道channel包括RGB三通道</li>
</ul></li>
<li>4-d —— <code>[[[[....]]]]</code>
<ul>
<li>一个RGB图片批量（批量大小 x 宽 x 高 x 通道）</li>
<li>批量大小：batch_size</li>
</ul></li>
<li>5-d ——<code>[[[[[....]]]]]</code>
<ul>
<li>一个视频批量（批量大小 x 时间 x 宽 x 高 x 通道）</li>
</ul></li>
</ul>
<h2 id="数组操作">数组操作</h2>
<h3 id="操作原理">操作原理</h3>
<p>创建数组需要：</p>
<ul>
<li>形状：例如 3 x 4 矩阵</li>
<li>每个元素的数据类型：例如32位浮点数</li>
<li>每个元素的值，例如全是0，或者随机数</li>
</ul>
<p><strong>访问元素：</strong><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="./image-20250621151410304.png"
alt="image-20250621151410304" /></p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="./image-20250621151422645.png"
alt="image-20250621151422645" />
<figcaption aria-hidden="true">image-20250621151422645</figcaption>
</figure>
<h3 id="创建数组">创建数组</h3>
<p>导入<code>torch</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<p>张量表示一个由数值组成的数组，这个数组可能有多个维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])</p>
</blockquote>
<p>可以通过张量的<code>shape</code>属性来访问张量（沿每个轴的长度）的<strong>形状</strong>和张量中元素的<strong>总数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>
<blockquote>
<p>torch.Size([12]) # 只有一个维度，这个维度的长度是12</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.numel()	<span class="comment"># x中元素的种类数</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>12 # 结果为一个标量数据值</p>
</blockquote>
<p>要想改变一个张量的形状而不改变元素数量和元素值，可以调用<code>reshape</code>函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>, <span class="number">4</span>)	<span class="comment"># 改为3行4列赋值给X</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])</p>
</blockquote>
<p>使用全0、全1、其他常量，或者从特定分布中随机采样的数字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))	<span class="comment"># 生成全0元素</span></span><br><span class="line"></span><br><span class="line">torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))	<span class="comment"># 生成全1元素</span></span><br><span class="line"></span><br><span class="line">torch.randn(<span class="number">3</span>, <span class="number">4</span>)		<span class="comment"># 生成随机数元素</span></span><br></pre></td></tr></table></figure>
<p>通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</p>
</blockquote>
<h3 id="数组运算">数组运算</h3>
<p>常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为<strong>按元素</strong>运算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([ 3., 4., 6., 10.]),</p>
<p>tensor([-1., 0., 2., 6.]),</p>
<p>tensor([ 2., 4., 8., 16.]),</p>
<p>tensor([0.5000, 1.0000, 2.0000, 4.0000]),</p>
<p>tensor([ 1., 4., 16., 64.]))</p>
</blockquote>
<p>​ “按元素”方式可以应用更多的计算</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</p>
</blockquote>
<p>我们也可以把多个张量<strong>连结</strong>（concatenate）在一起</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># cat把X、Y合并在一起，dim等于几，就会改变哪个维度，相当于numpy里的axis参数</span></span><br><span class="line"><span class="comment"># dim=0表示在第0维（行）</span></span><br><span class="line"><span class="comment"># dim=1表示在第1维（列）合并</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [
2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]), tensor([[ 0.,
1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8.,
9., 10., 11., 4., 3., 2., 1.]]))</p>
</blockquote>
<p>通过<strong>逻辑运算符</strong>构建二元张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X == Y</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[False, True, False, True], [False, False, False, False],
[False, False, False, False]])</p>
</blockquote>
<p>对张量中的所有元素进行求和，会产生一个单元素张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(66.)</p>
</blockquote>
<h3 id="数组广播">数组广播</h3>
<p>即使形状不同，我们仍然可以通过调用<strong>广播机制</strong>（broadcasting
mechanism）来执行按元素操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([[0], [1], [2]]),</p>
<p>tensor([[0, 1]]))</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b	<span class="comment"># 广播机制自动复制自身</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>[[0, 0], [[0, 1], [[0, 1], [1, 1], + [0, 1], = [1, 2], [2, 2]] [0,
1]] [2, 3],</p>
</blockquote>
<p>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([ 8., 9., 10., 11.]), tensor([[ 4., 5., 6., 7.], [ 8., 9.,
10., 11.]]))</p>
</blockquote>
<p>除读取外，我们还可以通过指定索引来将元素写入矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 0., 1., 2., 3.], [ 4., 5., 9., 7.], [ 8., 9., 10.,
11.]])</p>
</blockquote>
<p>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[12., 12., 12., 12.], [12., 12., 12., 12.], [ 8., 9., 10.,
11.]])</p>
</blockquote>
<h3 id="内存分配">内存分配</h3>
<p>运行一些操作可能会导致为新结果分配内存</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line"><span class="built_in">id</span>(Y) == before</span><br></pre></td></tr></table></figure>
<blockquote>
<p>False</p>
</blockquote>
<p>执行原地操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>id(Z): 140327634811696 id(Z): 140327634811696</p>
</blockquote>
<p>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before</span><br></pre></td></tr></table></figure>
<blockquote>
<p>True</p>
</blockquote>
<p>转换为NumPy张量（<code>ndarray</code>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(numpy.ndarray, torch.Tensor)</p>
</blockquote>
<p>将大小为1的张量转换为Python标量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([3.5000]), 3.5, 3.5, 3)</p>
</blockquote>
<h2 id="数据预处理">数据预处理</h2>
<p>是指如果我有一个原始数据，我怎么把它读取进来，使得我们通过机器学习的方法能够处理。</p>
<h3 id="创建数据集">创建数据集</h3>
<p>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)	<span class="comment"># 表头：房间数量，房间路线名称，房间价格</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)			<span class="comment"># NA表示未知的数</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="读取数据">读取数据</h3>
<p>从创建的CSV文件中加载原始数据集</p>
<p>一般读取CSV文件使用<strong>pandas库</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>NumRooms Alley Price 0 NaN Pave 127500 1 2.0 NaN 106000 2 4.0 NaN
178100 3 NaN NaN 140000</p>
</blockquote>
<h3 id="处理数据">处理数据</h3>
<p>为了处理缺失的数据，典型的方法包括<em>插值法</em>和<em>删除法</em>，</p>
<p>这里，我们将考虑插值法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())	<span class="comment"># mean()指input的均值</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>NumRooms Alley 0 3.0 Pave 1 2.0 NaN 2 4.0 NaN 3 3.0 NaN</p>
</blockquote>
<p>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别（Not
a Number）</p>
<p>对于字符型数值，最好转变为数值类型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把列里面所有出现的不同种类的值都变成一个特征</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)	<span class="comment"># 给不同的类分别赋值为一个单独的特征</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>NumRooms Alley_Pave Alley_nan 0 3.0 1 0 1 2.0 0 1 2 4.0 0 1 3 3.0 0
1</p>
</blockquote>
<p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([[3., 1., 0.], [2., 0., 1.], [4., 0., 1.], [3., 0., 1.]],
dtype=torch.float64), tensor([127500., 106000., 178100., 140000.],
dtype=torch.float64))</p>
</blockquote>
<p>传统python计算会用32位或64位浮点数，但是64位浮点数对深度学习来讲计算比较慢，一般指定数据类型为32位浮点数。</p>
<h1 id="线性代数">线性代数</h1>
<h2 id="理论基础">理论基础</h2>
<p><strong>标量</strong>—-<strong>一个值</strong></p>
<ul>
<li><p>简单操作</p>
<ul>
<li><span
class="math inline"><em>c</em> = <em>a</em> + <em>b</em></span></li>
<li><span
class="math inline"><em>c</em> = <em>a</em> ⋅ <em>b</em></span><br />
</li>
<li><span class="math inline"><em>c</em> = sin <em>a</em></span></li>
</ul></li>
<li><p>长度</p>
<ul>
<li><p><span class="math display">$$
|a| =
\begin{cases}
a &amp; \text{if } a &gt; 0 \\
-a &amp; \text{otherwise}
\end{cases}
$$</span></p></li>
<li><p><span
class="math display">|<em>a</em> + <em>b</em>| ≤ |<em>a</em>|+|<em>b</em>|</span></p></li>
<li><p><span
class="math display">|<em>a</em> ⋅ <em>b</em>| = |<em>a</em>|⋅|<em>b</em>|</span></p></li>
</ul></li>
</ul>
<p><strong>向量</strong> —-<strong>一行值</strong></p>
<ul>
<li><p>向量的简单操作</p>
<ul>
<li>$ c = a + b c_i = a_i + b_i $<br />
</li>
<li>$ c = b c_i = b_i $<br />
</li>
<li>$ c = a c_i = a_i $</li>
</ul></li>
<li><p>长度（<strong>范数</strong>）—-向量的长度就是向量的每个元素的平方求和再开根号</p>
<ul>
<li><p>范数（以 <span class="math inline"><em>L</em><sub>2</sub></span>
范数为例） <span class="math display">$$
\lVert a \rVert_2 = \left[ \sum_{i=1}^m a_i^2 \right]^{\frac{1}{2}}
$$</span></p></li>
<li><p>$ a   a $</p></li>
<li><p>$ a + b a + b $</p></li>
<li><p>$ a b = |a| b $</p></li>
</ul></li>
<li><p>点乘</p>
<ul>
<li><span
class="math inline"><em>a</em><sup>⊤</sup><em>b</em> = ∑<sub><em>i</em></sub><em>a</em><sub><em>i</em></sub><em>b</em><sub><em>i</em></sub></span></li>
</ul></li>
<li><p>正交</p>
<ul>
<li><span
class="math inline"><em>a</em><sup>⊤</sup><em>b</em> = ∑<sub><em>i</em></sub><em>a</em><sub><em>i</em></sub><em>b</em><sub><em>i</em></sub> = 0</span></li>
</ul></li>
</ul>
<p><strong>矩阵</strong></p>
<ul>
<li>简单操作
<ul>
<li>矩阵加法：$ C = A + B C_{ij} = A_{ij} + B_{ij} $<br />
</li>
<li>矩阵数乘：$ C = B C_{ij} = B_{ij} $<br />
</li>
<li>矩阵逐元素正弦运算：$ C = A C_{ij} = A_{ij} $</li>
</ul></li>
<li>特征向量和特征值
<ul>
<li>不被矩阵改变方向的向量</li>
<li>对称矩阵总是可以找到特征向量</li>
</ul></li>
<li>乘法（矩阵乘以向量）
<ul>
<li><span
class="math display"><strong>c</strong> = <strong>A</strong><strong>b</strong> where <em>c</em><sub><em>i</em></sub> = ∑<sub><em>j</em></sub><em>A</em><sub><em>i</em><em>j</em></sub><em>b</em><sub><em>j</em></sub></span><br />
</li>
</ul></li>
<li>乘法（矩阵乘以矩阵）
<ul>
<li><span
class="math display"><strong>C</strong> = <strong>A</strong><strong>B</strong> where <em>C</em><sub><em>i</em><em>k</em></sub> = ∑<sub><em>j</em></sub><em>A</em><sub><em>i</em><em>j</em></sub><em>B</em><sub><em>j</em><em>k</em></sub></span></li>
</ul></li>
<li>范数
<ul>
<li><span
class="math display"><em>c</em> = <em>A</em> ⋅ <em>b</em> hence ‖<em>c</em>‖ ≤ ‖<em>A</em>‖ ⋅ ‖<em>b</em>‖</span><br />
</li>
<li><span
class="math display"><em>取</em><em>决</em><em>于</em><em>如</em><em>何</em><em>衡</em><em>量</em>(<em>b</em>)<em>和</em>(<em>c</em>)<em>的</em><em>长</em><em>度</em></span><br />
</li>
</ul></li>
<li>常见范数
<ul>
<li><strong>矩阵范数</strong>：最小的满足上面公式的值<br />
</li>
<li><strong>Frobenius 范数</strong>：<br />
<span class="math display">$$ \lVert A \rVert_{\text{Frob}} = \left[
\sum_{ij} A_{ij}^2 \right]^{\frac{1}{2}} $$</span></li>
</ul></li>
</ul>
<p><strong>特殊矩阵</strong></p>
<ul>
<li>对称和反对称
<ul>
<li>$ A_{ij} = A_{ji} $（对称矩阵，元素关于主对角线对称）<br />
</li>
<li>$ A_{ij} = -A_{ji} $（反对称矩阵，主对角线元素为 0，( (i,j) ) 与 (
(j,i) ) 元素互反 ）<br />
</li>
</ul></li>
<li>正定—-如果一个矩阵是正定的，那么它乘以任何一个列向量或行向量都大于等于0
<ul>
<li>向量范数平方： <span
class="math display">‖<strong>x</strong>‖<sup>2</sup> = <strong>x</strong><sup>⊤</sup><strong>x</strong> ≥ 0</span><br />
（欧几里得范数平方，内积定义，结果非负 ）<br />
</li>
<li>二次型推广： <span
class="math display"><strong>x</strong><sup>⊤</sup><strong>A</strong><strong>x</strong> ≥ 0</span><br />
（刻画半正定/正定矩阵，( <span
class="math inline"><strong>A</strong></span> ) 对称时，非零 ( <span
class="math inline"><strong>x</strong></span> ) 代入结果非负则 ( <span
class="math display"><strong>A</strong></span> ) 半正定 ）<br />
</li>
</ul></li>
<li>正交矩阵
<ul>
<li>所有行都相互正交<br />
</li>
<li>所有行都有单位长度 <span class="math display"><em>U</em> with
∑<sub><em>j</em></sub><em>U</em><sub><em>i</em><em>j</em></sub><em>U</em><sub><em>k</em><em>j</em></sub> = <em>δ</em><sub><em>i</em><em>k</em></sub></span></li>
<li>可以写成 <span
class="math display"><em>U</em><em>U</em><sup>⊤</sup> = <strong>1</strong></span></li>
</ul></li>
<li>置换矩阵
<ul>
<li><span class="math display"><em>P</em> where
<em>P</em><sub><em>i</em><em>j</em></sub> = 1 if and only if
<em>j</em> = <em>π</em>(<em>i</em>)</span><br />
</li>
<li>置换矩阵是正交矩阵</li>
</ul></li>
</ul>
<h2 id="代码实现">代码实现</h2>
<h3 id="标量计算">标量计算</h3>
<p>标量由只有一个元素的张量表示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x**y</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</p>
</blockquote>
<h3 id="向量基础">向量基础</h3>
<p>向量可以被视为标量值组成的列表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([0, 1, 2, 3])</p>
</blockquote>
<p>通过张量的索引来访问任一元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(3)</p>
</blockquote>
<p>访问张量的长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>4</p>
</blockquote>
<p>只有一个轴的张量，形状只有一个元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure>
<blockquote>
<p>torch.Size([4])</p>
</blockquote>
<h3 id="矩阵基础">矩阵基础</h3>
<p>通过指定两个分量<span class="math inline"><em>m</em></span>和<span
class="math inline"><em>n</em></span>来创建一个形状为<span
class="math inline"><em>m</em> × <em>n</em></span>的矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">A</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14,
15], [16, 17, 18, 19]])</p>
</blockquote>
<p>矩阵的转置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 0, 4, 8, 12, 16], [ 1, 5, 9, 13, 17], [ 2, 6, 10, 14, 18],
[ 3, 7, 11, 15, 19]])</p>
</blockquote>
<p><em>对称矩阵</em>（symmetric matrix）<span
class="math inline"><strong>A</strong></span>等于其转置：<span
class="math inline"><strong>A</strong> = <strong>A</strong><sup>⊤</sup></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">B</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">B == B.T</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[True, True, True], [True, True, True], [True, True,
True]])</p>
</blockquote>
<h3 id="矩阵形状改变">矩阵形状改变</h3>
<p>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</p>
<p>只要元素个数不发生变化，就可以根据个数改变矩阵的形状</p>
<p><strong>行是最后一维（有几列）</strong>，<strong>列是倒数第二维（有几行）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]],</p>
<p>​ [[12, 13, 14, 15], ​ [16, 17, 18, 19], ​ [20, 21, 22, 23]]])</p>
</blockquote>
<h3 id="复制矩阵">复制矩阵</h3>
<p>给定具有相同形状的任意两个张量，任何按元素<strong>二元运算</strong>的结果都将是<strong>相同形状</strong>的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone()</span><br><span class="line">A, A + B</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.],
[12., 13., 14., 15.], [16., 17., 18., 19.]]),</p>
<p>tensor([[ 0., 2., 4., 6.], [ 8., 10., 12., 14.], [16., 18., 20.,
22.], [24., 26., 28., 30.], [32., 34., 36., 38.]]))</p>
</blockquote>
<h3 id="矩阵元素积">矩阵元素积</h3>
<p>两个矩阵的按元素乘法称为<strong>Hadamard</strong>积（Hadamard
product）（数学符号<span class="math inline">⊙</span>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 0., 1., 4., 9.], [ 16., 25., 36., 49.], [ 64., 81., 100.,
121.], [144., 169., 196., 225.], [256., 289., 324., 361.]])</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a + X, (a * X).shape</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([[[ 2, 3, 4, 5], [ 6, 7, 8, 9], [10, 11, 12, 13]],</p>
<p>​ [[14, 15, 16, 17], ​ [18, 19, 20, 21], ​ [22, 23, 24, 25]]]),</p>
<p>torch.Size([2, 3, 4]))</p>
</blockquote>
<h3 id="求和运算">求和运算</h3>
<p>计算向量元素的和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">x, x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([0., 1., 2., 3.]), tensor(6.))</p>
</blockquote>
<p>表示任意形状张量的元素和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, A.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(torch.Size([5, 4]), tensor(190.))</p>
</blockquote>
<h3 id="指定轴求和">指定轴求和</h3>
<p>指定张量沿哪一个轴来通过求和降低维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)		<span class="comment"># 对A.shape的第0维进行求和运算，剩下的矩阵是A.shape去掉第0维后的形状</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([40., 45., 50., 55.]), torch.Size([4]))</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)		<span class="comment"># 对A.shape的第1维进行求和运算，剩下的矩阵是A.shape去掉第1维后的形状</span></span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])		<span class="comment"># 对A.shape的第0维和第1维都进行求和运算，剩下的矩阵是A.shape去掉第0维和第1维后的形状</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(190.)</p>
</blockquote>
<h3 id="平均值计算">平均值计算</h3>
<p>一个与求和相关的量是<strong>平均值</strong>（mean或average）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.mean(), A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor(9.5000), tensor(9.5000))</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]	<span class="comment"># 按某维度来计算</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([ 8., 9., 10., 11.]), tensor([ 8., 9., 10., 11.]))</p>
</blockquote>
<h3 id="沿轴求和时维度不变">沿轴求和时维度不变</h3>
<p>计算总和或均值时保持轴数不变</p>
<p>keepdims=True时，指不会把要求和的维度去掉而是把<strong>shape中对应的那个维度</strong>变成1</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">sum_A</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 6.], [22.], [38.], [54.], [70.]])</p>
</blockquote>
<p>通过广播将<code>A</code>除以<code>sum_A</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A / sum_A</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[0.0000, 0.1667, 0.3333, 0.5000], [0.1818, 0.2273, 0.2727,
0.3182], [0.2105, 0.2368, 0.2632, 0.2895], [0.2222, 0.2407, 0.2593,
0.2778], [0.2286, 0.2429, 0.2571, 0.2714]])</p>
</blockquote>
<h3 id="沿轴累加求和">沿轴累加求和</h3>
<p>某个轴计算<code>A</code>元素的累积总和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 0., 1., 2., 3.], [ 4., 6., 8., 10.], [12., 15., 18., 21.],
[24., 28., 32., 36.], [40., 45., 50., 55.]])</p>
</blockquote>
<p>点积是相同位置的按元素乘积的和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.ones(<span class="number">4</span>, dtype = torch.float32)</span><br><span class="line">x, y, torch.dot(x, y)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</p>
</blockquote>
<h3 id="向量点积计算">向量点积计算</h3>
<p>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(6.)</p>
</blockquote>
<h3 id="矩阵与向量积计算">矩阵与向量积计算</h3>
<p>矩阵向量积<span
class="math inline"><strong>A</strong><strong>x</strong></span>是一个长度为<span
class="math inline"><em>m</em></span>的列向量，其第<span
class="math inline"><em>i</em></span>个元素是点积<span
class="math inline"><strong>a</strong><sub><em>i</em></sub><sup>⊤</sup><strong>x</strong></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>(torch.Size([5, 4]), torch.Size([4]), tensor([ 14., 38., 62., 86.,
110.]))</p>
</blockquote>
<h3 id="矩阵x矩阵计算">矩阵x矩阵计算</h3>
<p>我们可以将矩阵-矩阵乘法<span
class="math inline"><strong>A</strong><strong>B</strong></span>看作简单地执行<span
class="math inline"><em>m</em></span>次矩阵-向量积，并将结果拼接在一起，形成一个<span
class="math inline"><em>n</em> × <em>m</em></span>矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = torch.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([[ 6., 6., 6.], [22., 22., 22.], [38., 38., 38.], [54., 54.,
54.], [70., 70., 70.]])</p>
</blockquote>
<h3 id="向量范数">向量范数</h3>
<p><span
class="math inline"><em>L</em><sub>2</sub></span><em>范数</em>是向量元素平方和的平方根：</p>
<p><span class="math display">$$\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n
x_i^2}$$</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(5.)</p>
</blockquote>
<p><span
class="math inline"><em>L</em><sub>1</sub></span>范数，它表示为向量元素的绝对值之和：</p>
<p><span class="math display">$$\|\mathbf{x}\|_1 = \sum_{i=1}^n
\left|x_i \right|$$</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(7.)</p>
</blockquote>
<h3 id="矩阵范数">矩阵范数</h3>
<p>矩阵的<em>Frobenius范数</em>（Frobenius
norm）是矩阵元素平方和的平方根：</p>
<p><span class="math display">$$\|\mathbf{X}\|_F = \sqrt{\sum_{i=1}^m
\sum_{j=1}^n x_{ij}^2}$$</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>)))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(6.)</p>
</blockquote>
<h1 id="微积分">微积分</h1>
<p>矩阵的计算部分要清楚如何求导数！</p>
<h2 id="理论基础-1">理论基础</h2>
<h3 id="标量导数">标量导数</h3>
<ul>
<li>导数是切线的斜率</li>
</ul>
<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 6%" />
<col style="width: 17%" />
<col style="width: 14%" />
<col style="width: 22%" />
<col style="width: 14%" />
</colgroup>
<thead>
<tr>
<th><span class="math inline"><em>y</em></span></th>
<th><span class="math inline"><em>a</em></span></th>
<th><span
class="math inline"><em>x</em><sup><em>n</em></sup></span></th>
<th><span class="math inline">exp (<em>x</em>)</span></th>
<th><span class="math inline">log (<em>x</em>)</span></th>
<th><span class="math inline">sin (<em>x</em>)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math inline">$\dfrac{dy}{dx}$</span></td>
<td><span class="math inline">0</span></td>
<td><span
class="math inline"><em>n</em><em>x</em><sup><em>n</em> − 1</sup></span></td>
<td><span class="math inline">exp (<em>x</em>)</span></td>
<td><span class="math inline">$\dfrac{1}{x}$</span></td>
<td><span class="math inline">cos (<em>x</em>)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>注：<span class="math inline"><em>a</em></span> 不是 <span
class="math inline"><em>x</em></span> 的函数</p>
</blockquote>
<table>
<colgroup>
<col style="width: 13%" />
<col style="width: 28%" />
<col style="width: 31%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th><span class="math inline"><em>y</em></span></th>
<th><span class="math inline"><em>u</em> + <em>v</em></span></th>
<th><span class="math inline"><em>u</em><em>v</em></span></th>
<th><span
class="math inline"><em>y</em> = <em>f</em>(<em>u</em>), <em>u</em> = <em>g</em>(<em>x</em>)</span></th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math inline">$\dfrac{dy}{dx}$</span></td>
<td><span class="math inline">$\dfrac{du}{dx} +
\dfrac{dv}{dx}$</span></td>
<td><span class="math inline">$\dfrac{du}{dx} v + \dfrac{dv}{dx}
u$</span></td>
<td><span class="math inline">$\dfrac{dy}{du}
\dfrac{du}{dx}$</span></td>
</tr>
</tbody>
</table>
<h3 id="亚导数">亚导数</h3>
<blockquote>
<p>当不存在导数时怎么办？</p>
</blockquote>
<ul>
<li><p>将导数拓展到不可微的函数</p>
<ul>
<li><p>$ y = |x| $</p></li>
<li><p>绝对值函数的导数</p></li>
<li><p><span class="math display">$$
\frac{\partial |x|}{\partial x} =
\begin{cases}
1 &amp; \text{if } x &gt; 0 \\
-1 &amp; \text{if } x &lt; 0 \\
a &amp; \text{if } x = 0, \ a \in [-1,1]
\end{cases}
$$</span></p></li>
<li><p>ReLU 函数的导数（分段形式）：<br />
<span class="math display">$$
\frac{\partial}{\partial x} \text{max}(x, 0) =
\begin{cases}
1 &amp; \text{if } x &gt; 0 \\
0 &amp; \text{if } x &lt; 0 \\
a &amp; \text{if } x = 0, \ a \in [0,1]
\end{cases}
$$</span></p></li>
</ul></li>
</ul>
<h3 id="向量梯度">向量梯度</h3>
<ul>
<li>将导数拓展到向量，需要注意向量或矩阵的形状！！
<ul>
<li><figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="./image-20250622164809141.png"
alt="image-20250622164809141" />
<figcaption aria-hidden="true">image-20250622164809141</figcaption>
</figure></li>
</ul></li>
<li>求导后梯度的形状，按照<strong>分子布局符号</strong>，如上图
<ul>
<li>y是列向量，x是标量，求导后的结果是y的形状（列向量）</li>
<li>x是列向量，y是标量，求导后的结果<strong>行向量</strong></li>
<li>x是向量，y是向量，求导后的结果是矩阵</li>
<li>如果是反过来的情况，则称为<strong>分母布局符号</strong></li>
</ul></li>
</ul>
<h4 id="单向量梯度样例">单向量梯度样例</h4>
<p><strong>x是列向量</strong>，<strong>y是标量</strong></p>
<table>
<colgroup>
<col style="width: 19%" />
<col style="width: 20%" />
<col style="width: 25%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr>
<th><span class="math inline"><em>y</em></span></th>
<th>表达式</th>
<th><span class="math inline">$\dfrac{\partial y}{\partial
\boldsymbol{x}}$</span></th>
<th>补充说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="math inline"><em>y</em> = <em>a</em></span></td>
<td>标量（与 <span class="math inline"><strong>x</strong></span>
无关）</td>
<td><span class="math inline"><strong>0</strong><sup>⊤</sup></span></td>
<td><span class="math inline"><em>a</em></span> 不是 <span
class="math inline"><strong>x</strong></span> 的函数</td>
</tr>
<tr>
<td><span
class="math inline"><em>y</em> = <em>a</em><em>u</em></span></td>
<td>标量（<span class="math inline"><em>u</em></span> 是 <span
class="math inline"><strong>x</strong></span> 的函数）</td>
<td><span class="math inline">$a \dfrac{\partial u}{\partial
\boldsymbol{x}}$</span></td>
<td>数乘求导法则</td>
</tr>
<tr>
<td><span
class="math inline"><em>y</em> = sum(<strong>x</strong>)</span></td>
<td>向量元素和</td>
<td><span class="math inline"><strong>1</strong><sup>⊤</sup></span></td>
<td>对向量各元素求和后求导</td>
</tr>
<tr>
<td><span
class="math inline"><em>y</em> = ‖<strong>x</strong>‖<sup>2</sup></span></td>
<td>向量 <span class="math inline"><em>L</em><sub>2</sub></span>
范数平方</td>
<td><span
class="math inline">2<strong>x</strong><sup>⊤</sup></span></td>
<td>由 <span
class="math inline">‖<strong>x</strong>‖<sup>2</sup> = <strong>x</strong><sup>⊤</sup><strong>x</strong></span>
求导</td>
</tr>
</tbody>
</table>
<p><strong>符号说明</strong></p>
<ul>
<li><span class="math inline"><strong>0</strong></span>：零向量（元素全
0 ），<span class="math inline"><strong>0</strong><sup>⊤</sup></span>
是其转置（行向量）；<br />
</li>
<li><span class="math inline"><strong>1</strong></span>：全 1
向量（元素全 1 ），<span
class="math inline"><strong>1</strong><sup>⊤</sup></span>
是其转置（行向量）；<br />
</li>
<li>用途：向量微积分、机器学习梯度推导基础规则。</li>
</ul>
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 28%" />
<col style="width: 34%" />
<col style="width: 6%" />
</colgroup>
<thead>
<tr>
<th><span class="math inline"><em>y</em></span></th>
<th>表达式</th>
<th><span class="math inline">$\dfrac{\partial y}{\partial
\boldsymbol{x}}$</span></th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><span
class="math inline"><em>y</em> = <em>u</em> + <em>v</em></span></td>
<td>和（<span class="math inline"><em>u</em>, <em>v</em></span>
是标量/向量函数）</td>
<td><span class="math inline">$\dfrac{\partial u}{\partial
\boldsymbol{x}} + \dfrac{\partial v}{\partial
\boldsymbol{x}}$</span></td>
<td>加法求导法则</td>
</tr>
<tr>
<td><span
class="math inline"><em>y</em> = <em>u</em><em>v</em></span></td>
<td>积（<span class="math inline"><em>u</em>, <em>v</em></span>
是标量/向量函数）</td>
<td><span class="math inline">$\dfrac{\partial u}{\partial
\boldsymbol{x}} v + \dfrac{\partial v}{\partial \boldsymbol{x}}
u$</span></td>
<td>乘积求导法则</td>
</tr>
<tr>
<td><span
class="math inline"><em>y</em> = ⟨<strong>u</strong>, <strong>v</strong>⟩</span></td>
<td>内积（<span
class="math inline"><strong>u</strong>, <strong>v</strong></span>
是向量函数）</td>
<td><span class="math inline">$\boldsymbol{u}^\top \dfrac{\partial
\boldsymbol{v}}{\partial \boldsymbol{x}} + \boldsymbol{v}^\top
\dfrac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}}$</span></td>
<td>内积求导法则</td>
</tr>
</tbody>
</table>
<p><strong>用途说明</strong>：向量微积分基础规则，用于机器学习（如反向传播）、优化理论中推导梯度，计算复杂函数对参数向量的导数。</p>
<h3 id="向量对向量的导数">向量对向量的导数</h3>
<p>（雅可比矩阵，Jacobian Matrix）</p>
<p>给定： <span class="math display">$$ \boldsymbol{x} = \begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, \quad \boldsymbol{y} =
\begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix} $$</span></p>
<p>雅可比矩阵定义：<br />
<span class="math display">$$
\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} =
\begin{bmatrix}
\frac{\partial y_1}{\partial \boldsymbol{x}} \\
\frac{\partial y_2}{\partial \boldsymbol{x}} \\
\vdots \\
\frac{\partial y_m}{\partial \boldsymbol{x}}
\end{bmatrix} = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1}, \frac{\partial y_1}{\partial x_2},
\dots, \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1}, \frac{\partial y_2}{\partial x_2},
\dots, \frac{\partial y_2}{\partial x_n} \\
\vdots \\
\frac{\partial y_m}{\partial x_1}, \frac{\partial y_m}{\partial x_2},
\dots, \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
$$</span></p>
<p>首先把y拆分后对向量x求导，再计算单独求导后的结果。</p>
<h4 id="雅可比矩阵">雅可比矩阵</h4>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 36%" />
<col style="width: 26%" />
</colgroup>
<thead>
<tr>
<th><span class="math inline"><strong>y</strong></span></th>
<th><span class="math inline">$\dfrac{\partial \boldsymbol{y}}{\partial
\boldsymbol{x}}$</span></th>
<th>维度与说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><span
class="math inline"><strong>y</strong> = <strong>a</strong></span>（常向量，与
<span class="math inline"><strong>x</strong></span> 无关）</td>
<td><span class="math inline"><strong>0</strong></span>（零矩阵）</td>
<td><span class="math inline"><strong>a</strong></span> 不随 <span
class="math inline"><strong>x</strong></span> 变化</td>
</tr>
<tr>
<td><span
class="math inline"><strong>y</strong> = <strong>x</strong></span>（恒等映射）</td>
<td><span class="math inline"><strong>I</strong></span>（单位矩阵）</td>
<td>自身对自身求导为单位矩阵</td>
</tr>
<tr>
<td><span
class="math inline"><strong>y</strong> = <strong>A</strong><strong>x</strong></span>（矩阵-向量乘法）</td>
<td><span class="math inline"><strong>A</strong></span>（原矩阵）</td>
<td>线性变换导数是变换矩阵本身</td>
</tr>
<tr>
<td><span
class="math inline"><strong>y</strong> = <strong>x</strong><sup>⊤</sup><strong>A</strong></span>（向量-矩阵乘法）</td>
<td><span
class="math inline"><strong>A</strong><sup>⊤</sup></span>（矩阵转置）</td>
<td>转置保证导数维度 $^{m n} $</td>
</tr>
</tbody>
</table>
<p>补充条件</p>
<ul>
<li>维度： <span
class="math inline"><strong>x</strong> ∈ ℝ<sup><em>n</em></sup></span>
， $ ^m $，故 $ ^{m n} $；<br />
</li>
<li>常量： $a, , $均与 $ $无关；<br />
</li>
<li>符号：<span class="math inline"><strong>0</strong></span>
（零矩阵）、 <span class="math inline"><strong>I</strong></span>
（单位矩阵） 。</li>
</ul>
<p><strong>用途</strong>：向量微积分基础，机器学习（如反向传播）、控制理论中线性变换梯度推导的核心规则。</p>
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 40%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr>
<th><span class="math inline"><strong>y</strong></span></th>
<th><span class="math inline">$\dfrac{\partial \boldsymbol{y}}{\partial
\boldsymbol{x}}$</span></th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><span
class="math inline"><strong>y</strong> = <em>a</em><strong>u</strong></span>（标量-向量数乘）</td>
<td><span class="math inline">$a \dfrac{\partial
\boldsymbol{u}}{\partial \boldsymbol{x}}$</span></td>
<td>数乘法则（标量提至导数外）</td>
</tr>
<tr>
<td><span
class="math inline"><strong>y</strong> = <strong>A</strong><strong>u</strong></span>（矩阵-向量乘法）</td>
<td><span class="math inline">$\boldsymbol{A} \dfrac{\partial
\boldsymbol{u}}{\partial \boldsymbol{x}}$</span></td>
<td>线性变换法则（矩阵提至导数外）</td>
</tr>
<tr>
<td><span
class="math inline"><strong>y</strong> = <strong>u</strong> + <strong>v</strong></span>（向量和）</td>
<td><span class="math inline">$\dfrac{\partial \boldsymbol{u}}{\partial
\boldsymbol{x}} + \dfrac{\partial \boldsymbol{v}}{\partial
\boldsymbol{x}}$</span></td>
<td>加法法则（和的导数拆为导数和）</td>
</tr>
</tbody>
</table>
<p>用途说明</p>
<p>向量微积分基础规则，用于机器学习（如神经网络反向传播）、优化理论中，推导线性/仿射变换的梯度，计算损失函数对参数的导数。</p>
<h3 id="向量链式法制">向量链式法制</h3>
<ul>
<li>标量链式法则</li>
</ul>
<p>若 $y = f(u) <span class="math inline">，</span>u = g(x)
$（复合函数， y 经 u 依赖 x ），则：</p>
<p><span class="math display">$$ \frac{dy}{dx} = \frac{dy}{du} \cdot
\frac{du}{dx} $$</span></p>
<p><strong>说明</strong>：微积分核心求导法则，是神经网络反向传播的数学基础（分解复杂梯度为多层简单梯度乘积）。</p>
<ul>
<li><p>标量中间变量 u</p>
<ul>
<li><p><span
class="math inline"><em>若</em><em>y</em> = <em>f</em>(<em>u</em>)（<em>y</em><em>标</em><em>量</em>），<em>u</em> = <em>g</em>(<strong>x</strong>)（<em>u</em><em>标</em><em>量</em>，<strong>x</strong> ∈ ℝ<sup><em>n</em></sup>）</span><span
class="math inline">：</span>$ = $</p></li>
<li><p>维度：<span class="math display">$$ \frac{dy}{\partial
\boldsymbol{x}} (1,n) = \frac{dy}{du} (1,) \cdot \frac{\partial
u}{\partial \boldsymbol{x}} (1,n)  $$</span></p></li>
</ul></li>
<li><p>拓展到向量，向量中间变量 <span
class="math display"><strong>u</strong></span></p>
<ul>
<li>若 <span
class="math inline"><em>y</em> = <em>f</em>(<strong>u</strong>)（<em>y</em><em>标</em><em>量</em>），<strong>u</strong> = <em>g</em>(<strong>x</strong>)（<strong>u</strong> ∈ ℝ<sup><em>k</em></sup>，<strong>x</strong> ∈ ℝ<sup><em>n</em></sup>）</span>：
<span class="math display">$$ \frac{dy}{\partial \boldsymbol{x}} =
\frac{dy}{\partial \boldsymbol{u}} \cdot \frac{\partial
\boldsymbol{u}}{\partial \boldsymbol{x}} $$</span><br />
</li>
<li>维度：<span class="math display">$$ \frac{dy}{\partial
\boldsymbol{x}} (1,n) = \frac{dy}{\partial \boldsymbol{u}} (1,k) \cdot
\frac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}} (k,n)
$$</span></li>
</ul></li>
<li><p>向量对向量的链式求导法则</p>
<ul>
<li>若 $ 经向量 依赖于 $，则： <span class="math display">$$
\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} = \frac{\partial
\boldsymbol{y}}{\partial \boldsymbol{u}} \cdot \frac{\partial
\boldsymbol{u}}{\partial \boldsymbol{x}} $$</span></li>
</ul></li>
<li><p>维度说明</p>
<ul>
<li><p><span class="math inline">$\frac{\partial
\boldsymbol{y}}{\partial \boldsymbol{x}} ：(m, n) 矩阵（ \boldsymbol{y}
\in \mathbb{R}^m ， \boldsymbol{x} \in \mathbb{R}^n  ）$</span></p></li>
<li><p><span class="math display">$$ \frac{\partial
\boldsymbol{y}}{\partial \boldsymbol{u}} ： (m, k)  矩阵（
\boldsymbol{u} \in \mathbb{R}^k ）  $$</span></p></li>
<li><p>$ ： (k, n) 矩阵 $</p></li>
</ul></li>
<li><p>合法性：需满足 ( k ) 维度匹配（前导矩阵列数 =
后续矩阵行数）。</p></li>
</ul>
<h2 id="代码实现-1">代码实现</h2>
<p>如果<span
class="math inline"><em>f</em></span>的<em>导数</em>存在，这个极限被定义为：</p>
<p><span class="math display">$$f'(x) = \lim_{h \rightarrow 0}
\frac{f(x+h) - f(x)}{h}$$</span></p>
<p>定义<span
class="math inline"><em>u</em> = <em>f</em>(<em>x</em>) = 3<em>x</em><sup>2</sup> − 4<em>x</em></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib_inline <span class="keyword">import</span> backend_inline</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span> * x ** <span class="number">2</span> - <span class="number">4</span> * x</span><br></pre></td></tr></table></figure>
<p>通过令<span class="math inline"><em>x</em> = 1</span>并让<span
class="math inline"><em>h</em></span>接近<span
class="math inline">0</span>，<span
class="math inline">$\frac{f(x+h)-f(x)}{h}$</span>的数值结果接近<span
class="math inline">2</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_lim</span>(<span class="params">f, x, h</span>):</span><br><span class="line">    <span class="keyword">return</span> (f(x + h) - f(x)) / h</span><br><span class="line"></span><br><span class="line">h = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;h=<span class="subst">&#123;h:<span class="number">.5</span>f&#125;</span>, numerical limit=<span class="subst">&#123;numerical_lim(f, <span class="number">1</span>, h):<span class="number">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    h *= <span class="number">0.1</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>h=0.10000, numerical limit=2.30000 h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300 h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003</p>
</blockquote>
<p>为了对导数的这种解释进行可视化，我们将使用<code>matplotlib</code>定义几个函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_svg_display</span>():  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class="line">    backend_inline.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    d2l.plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X, Y=<span class="literal">None</span>, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">         ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">         fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>), axes=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        legend = []</span><br><span class="line"></span><br><span class="line">    set_figsize(figsize)</span><br><span class="line">    axes = axes <span class="keyword">if</span> axes <span class="keyword">else</span> d2l.plt.gca()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">hasattr</span>(X, <span class="string">&quot;ndim&quot;</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>)</span><br><span class="line">                <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>], <span class="string">&quot;__len__&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> has_one_axis(X):</span><br><span class="line">        X = [X]</span><br><span class="line">    <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X, Y = [[]] * <span class="built_in">len</span>(X), X</span><br><span class="line">    <span class="keyword">elif</span> has_one_axis(Y):</span><br><span class="line">        Y = [Y]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(X) != <span class="built_in">len</span>(Y):</span><br><span class="line">        X = X * <span class="built_in">len</span>(Y)</span><br><span class="line">    axes.cla()</span><br><span class="line">    <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(x):</span><br><span class="line">            axes.plot(x, y, fmt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            axes.plot(y, fmt)</span><br><span class="line">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure>
<p>绘制函数<span
class="math inline"><em>u</em> = <em>f</em>(<em>x</em>)</span>及其在<span
class="math inline"><em>x</em> = 1</span>处的切线<span
class="math inline"><em>y</em> = 2<em>x</em> − 3</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">plot(x, [f(x), <span class="number">2</span> * x - <span class="number">3</span>], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, legend=[<span class="string">&#x27;f(x)&#x27;</span>, <span class="string">&#x27;Tangent line (x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure>
<h1 id="自动求导">自动求导</h1>
<p>自动求导计算一个函数在指定值上的导数</p>
<p>它有别于<strong>符号求导</strong>和<strong>数值求导</strong></p>
<h2 id="理论基础-2">理论基础</h2>
<h3 id="计算图">计算图</h3>
<p>将代码分解成操作子</p>
<p>将计算表示成一个<strong>无环图</strong></p>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="./image-20250626171942683.png"
alt="image-20250626171942683" />
<figcaption aria-hidden="true">image-20250626171942683</figcaption>
</figure>
<p>显示构造，可实现框架：Tensorflow、Theano、MXNet</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> sym</span><br><span class="line"></span><br><span class="line">a = sym.var()</span><br><span class="line">b = sym.var()</span><br><span class="line">c = <span class="number">2</span> * a + b</span><br><span class="line"><span class="comment"># bind data into a and b later</span></span><br></pre></td></tr></table></figure>
<p>隐式构造，可实现框架：PyTorch、MXNet</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    a = nd.ones((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    b = nd.ones((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    c = <span class="number">2</span> * a + b</span><br></pre></td></tr></table></figure>
<h3 id="两种模式">两种模式</h3>
<p>核心原理<strong>链式法则</strong>：<span
class="math display">$$\frac{\partial y}{\partial x} = \frac{\partial
y}{\partial u_n} \frac{\partial u_n}{\partial u_{n - 1}} \dots
\frac{\partial u_2}{\partial u_1} \frac{\partial u_1}{\partial
x}$$</span></p>
<ul>
<li><p>正向积累：<span class="math display">$$\frac{\partial y}{\partial
x} = \frac{\partial y}{\partial u_n} \left( \frac{\partial u_n}{\partial
u_{n - 1}} \left( \dots \left( \frac{\partial u_2}{\partial u_1}
\frac{\partial u_1}{\partial x} \right) \right)
\right)$$</span></p></li>
<li><p>反向累积、又称<strong>反向传递</strong> <span
class="math display">$$\frac{\partial y}{\partial x} = \left( \left(
\left( \frac{\partial y}{\partial u_n} \frac{\partial u_n}{\partial u_{n
- 1}} \right) \dots \right) \frac{\partial u_2}{\partial u_1} \right)
\frac{\partial u_1}{\partial x}$$</span></p></li>
</ul>
<h3 id="反向累积总结">反向累积总结</h3>
<ul>
<li>构造计算图</li>
<li>前向：执行图，存储中间结果</li>
<li>反向：从相反方向执行图
<ul>
<li>去除不需要的枝</li>
</ul></li>
</ul>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="./image-20250626173435627.png"
alt="image-20250626173435627" />
<figcaption aria-hidden="true">image-20250626173435627</figcaption>
</figure>
<h3 id="复杂度">复杂度</h3>
<ul>
<li>计算复杂度：<span class="math inline"><em>O</em>(<em>n</em>)</span>,
<span class="math inline"><em>n</em></span> 是操作子个数
<ul>
<li>通常正向和反向的代价类似<br />
</li>
</ul></li>
<li>内存复杂度: <span
class="math inline"><em>O</em>(<em>n</em>)</span>，因为需要存储正向的所有中间结果
。（耗费计算资源的主要原因）</li>
<li>跟正向累积对比：
<ul>
<li><span class="math inline"><em>O</em>(<em>n</em>)</span>
计算复杂度用来计算一个变量的梯度<br />
</li>
<li><span class="math inline"><em>O</em>(1)</span> 内存复杂度</li>
</ul></li>
</ul>
<h2 id="代码实现-2">代码实现</h2>
<p>假设我们想对函数<span
class="math inline"><em>y</em> = 2<strong>x</strong><sup>⊤</sup><strong>x</strong></span>关于列向量<span
class="math inline"><strong>x</strong></span>求导</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br></pre></td></tr></table></figure>
<p>在我们计算<span class="math inline"><em>y</em></span>关于<span
class="math inline"><strong>x</strong></span>的梯度之前，需要一个地方来存储梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad_(<span class="literal">True</span>)	<span class="comment"># 等价于x = torch.arange(4.0, requires_grad=True)，默认值是None</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<p>现在计算<span class="math inline"><em>y</em></span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(28., grad_fn=<MulBackward0>)</p>
</blockquote>
<p>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward()	<span class="comment"># 求导</span></span><br><span class="line">x.grad			<span class="comment"># 访问导数</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([ 0., 4., 8., 12.])</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad == <span class="number">4</span> * x</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([True, True, True, True])</p>
</blockquote>
<p>现在计算<code>x</code>的另一个函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([1., 1., 1., 1.])</p>
</blockquote>
<p>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([0., 2., 4., 6.])</p>
</blockquote>
<p>将某些计算移动到记录的计算图之外</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()	<span class="comment"># 把y当作一个常数，而不是关于x的函数</span></span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()		<span class="comment"># z求导</span></span><br><span class="line">x.grad == u				<span class="comment"># u*x的导数为u</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([True, True, True, True])</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span> * x</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor([True, True, True, True])</p>
</blockquote>
<p>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:  </span><br><span class="line">        <span class="comment"># norm()是求欧几里得范数，欧几里得范数指得就是通常意义上的距离范数。</span></span><br><span class="line">        <span class="comment"># 例如在欧式空间里,它表示两点间的距离(向量x的模长)。</span></span><br><span class="line">        <span class="comment"># 即使函数经过了流程控制会产生不同的函数公式，每次计算的时候，pytorch都会把计算图存起来。</span></span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)	<span class="comment"># size=空，表示是一个标量</span></span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br><span class="line"></span><br><span class="line">a.grad == d / a</span><br></pre></td></tr></table></figure>
<blockquote>
<p>tensor(True)</p>
</blockquote>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/avatar1.webp" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/avatar1.webp" title="头像" alt="头像"></a><div class="post-copyright__author_name">山河忽晚</div><div class="post-copyright__author_desc">落落冰川流转着千年古忆</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://www.dyeddie.top/20250628/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://www.dyeddie.top/20250628/')">动手学深度学习1--预备知识</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://www.dyeddie.top/20250628/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.dyeddie.top" target="_blank">枫落繁花</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"><a class="post-meta__box__tags" href="/tags/Pytorch/"><span class="tags-punctuation"> <i class="anzhiyufont anzhiyu-icon-tag"></i></span>Pytorch<span class="tagsPageCount">4</span></a></div></div></div><div class="post_share"><div class="social-share" data-image="https://www.dyeddie.top/20250628/wallhaven-zy1wrg_compressed.jpg" data-sites="facebook,twitter,wechat,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/20250603/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://www.dyeddie.top/20250603/105204nCGuS.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">初识Pytorch(三) -- 完整的模型训练流程 + GPU调用训练</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="anzhiyufont anzhiyu-icon-thumbs-up fa-fw" style="font-size: 1.5rem; margin-right: 4px"></i><span>喜欢这篇文章的人也看了</span></div><div class="relatedPosts-list"><div><a href="/20250601/" title="初识Pytorch(一) -- Transforms笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://www.dyeddie.top/20250601/1800483AlZN.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-06-03</div><div class="title">初识Pytorch(一) -- Transforms笔记</div></div></a></div><div><a href="/20250602/" title="初识Pytorch(二) -- 神经网络搭建"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://www.dyeddie.top/20250602/110119t32pt.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-06-03</div><div class="title">初识Pytorch(二) -- 神经网络搭建</div></div></a></div><div><a href="/20250603/" title="初识Pytorch(三) -- 完整的模型训练流程 + GPU调用训练"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://www.dyeddie.top/20250603/105204nCGuS.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="anzhiyufont anzhiyu-icon-calendar-days fa-fw"></i> 2025-06-03</div><div class="title">初识Pytorch(三) -- 完整的模型训练流程 + GPU调用训练</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C-%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">数据操作 + 预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E7%BB%84%E6%93%8D%E4%BD%9C"><span class="toc-text">数组操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%93%8D%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">操作原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E7%BB%84"><span class="toc-text">创建数组</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E7%BB%84%E8%BF%90%E7%AE%97"><span class="toc-text">数组运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E7%BB%84%E5%B9%BF%E6%92%AD"><span class="toc-text">数组广播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="toc-text">内存分配</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-text">数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">创建数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">读取数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-text">处理数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-text">线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-text">理论基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="toc-text">标量计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%9F%BA%E7%A1%80"><span class="toc-text">向量基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%9F%BA%E7%A1%80"><span class="toc-text">矩阵基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%BD%A2%E7%8A%B6%E6%94%B9%E5%8F%98"><span class="toc-text">矩阵形状改变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E5%88%B6%E7%9F%A9%E9%98%B5"><span class="toc-text">复制矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%85%83%E7%B4%A0%E7%A7%AF"><span class="toc-text">矩阵元素积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B1%82%E5%92%8C%E8%BF%90%E7%AE%97"><span class="toc-text">求和运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E8%BD%B4%E6%B1%82%E5%92%8C"><span class="toc-text">指定轴求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%9D%87%E5%80%BC%E8%AE%A1%E7%AE%97"><span class="toc-text">平均值计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B2%BF%E8%BD%B4%E6%B1%82%E5%92%8C%E6%97%B6%E7%BB%B4%E5%BA%A6%E4%B8%8D%E5%8F%98"><span class="toc-text">沿轴求和时维度不变</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B2%BF%E8%BD%B4%E7%B4%AF%E5%8A%A0%E6%B1%82%E5%92%8C"><span class="toc-text">沿轴累加求和</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E7%82%B9%E7%A7%AF%E8%AE%A1%E7%AE%97"><span class="toc-text">向量点积计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E4%B8%8E%E5%90%91%E9%87%8F%E7%A7%AF%E8%AE%A1%E7%AE%97"><span class="toc-text">矩阵与向量积计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5x%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97"><span class="toc-text">矩阵x矩阵计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E8%8C%83%E6%95%B0"><span class="toc-text">向量范数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E8%8C%83%E6%95%B0"><span class="toc-text">矩阵范数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BE%AE%E7%A7%AF%E5%88%86"><span class="toc-text">微积分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-1"><span class="toc-text">理论基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%87%E9%87%8F%E5%AF%BC%E6%95%B0"><span class="toc-text">标量导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%9A%E5%AF%BC%E6%95%B0"><span class="toc-text">亚导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E6%A2%AF%E5%BA%A6"><span class="toc-text">向量梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E5%90%91%E9%87%8F%E6%A2%AF%E5%BA%A6%E6%A0%B7%E4%BE%8B"><span class="toc-text">单向量梯度样例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%AF%B9%E5%90%91%E9%87%8F%E7%9A%84%E5%AF%BC%E6%95%B0"><span class="toc-text">向量对向量的导数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%85%E5%8F%AF%E6%AF%94%E7%9F%A9%E9%98%B5"><span class="toc-text">雅可比矩阵</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%B6"><span class="toc-text">向量链式法制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC"><span class="toc-text">自动求导</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80-2"><span class="toc-text">理论基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-text">计算图</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F"><span class="toc-text">两种模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E7%B4%AF%E7%A7%AF%E6%80%BB%E7%BB%93"><span class="toc-text">反向累积总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">复杂度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-2"><span class="toc-text">代码实现</span></a></li></ol></li></ol></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v5.4.0" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/bottom_footer/Frame-Hexo.svg" alt="博客框架为Hexo_v5.4.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/bottom_footer/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://www.dogecloud.com/" style="margin-inline:5px" data-title="本站使用多吉云为静态资源提供CDN加速" title="本站使用多吉云为静态资源提供CDN加速"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/bottom_footer/CDN-多吉云-3693F3.svg" alt="本站使用多吉云为静态资源提供CDN加速"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/bottom_footer/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/bottom_footer/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2024 - 2025 By <a class="footer-bar-link" href="/" title="山河忽晚" target="_blank">山河忽晚</a></div></div><div id="footer-type-tips"></div><div class="js-pjax"><script>function subtitleType () {
  fetch('https://v1.hitokoto.cn')
    .then(response => response.json())
    .then(data => {
      if (true) {
        const from = '出自 ' + data.from
        const sub = []
        sub.unshift(data.hitokoto, from)
        window.typed = new Typed('#footer-type-tips', {
          strings: sub,
          startDelay: 300,
          typeSpeed: 150,
          loop: true,
          backSpeed: 50,
        })
      } else {
        document.getElementById('footer-type-tips').innerHTML = data.hitokoto
      }
    })
}

if (true) {
  if (typeof Typed === 'function') {
    subtitleType()
  } else {
    getScript('https://cdn.cbd.int/typed.js@2.1.0/dist/typed.umd.js').then(subtitleType)
  }
} else {
  subtitleType()
}
</script></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://beian.miit.gov.cn/" title="冀ICP备-2024076436号">冀ICP备-2024076436号</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">18</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">6</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">其他</div><div class="back-menu-list"><a class="back-menu-item" href="/archives/" title="文章归档"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/archieves.jpg" alt="文章归档"/><span class="back-menu-item-text">文章归档</span></a><a class="back-menu-item" href="/music/?id=3778678&amp;server=netease" title="听力练习"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/ye_source/de36c853a5ea9.jpg" alt="听力练习"/><span class="back-menu-item-text">听力练习</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 类别</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 解压</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/?id=3778678&amp;server=netease"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/apps/"><i class="fa-brands fa-canadian-maple-leaf faa-tada"></i><span> 修仙室</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> About</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/ArchLinux/" style="font-size: 0.88rem; color: rgb(155, 27, 194);">ArchLinux<sup>1</sup></a><a href="/tags/GitHub/" style="font-size: 0.88rem; color: rgb(110, 129, 126);">GitHub<sup>2</sup></a><a href="/tags/Linux/" style="font-size: 0.88rem; color: rgb(47, 13, 194);">Linux<sup>5</sup></a><a href="/tags/Numpy/" style="font-size: 0.88rem; color: rgb(25, 138, 17);">Numpy<sup>1</sup></a><a href="/tags/Oral-Practice/" style="font-size: 0.88rem; color: rgb(47, 53, 104);">Oral_Practice<sup>2</sup></a><a href="/tags/Python/" style="font-size: 0.88rem; color: rgb(134, 21, 131);">Python<sup>1</sup></a><a href="/tags/Pytorch/" style="font-size: 0.88rem; color: rgb(88, 121, 2);">Pytorch<sup>4</sup></a><a href="/tags/Skills/" style="font-size: 0.88rem; color: rgb(53, 29, 160);">Skills<sup>1</sup></a><a href="/tags/Sublime-Text/" style="font-size: 0.88rem; color: rgb(151, 110, 39);">Sublime_Text<sup>1</sup></a><a href="/tags/Ubuntu/" style="font-size: 0.88rem; color: rgb(139, 14, 169);">Ubuntu<sup>2</sup></a><a href="/tags/Xfce/" style="font-size: 0.88rem; color: rgb(36, 117, 153);">Xfce<sup>1</sup></a><a href="/tags/conda%E5%91%BD%E4%BB%A4/" style="font-size: 0.88rem; color: rgb(172, 101, 100);">conda命令<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("05/01/2024 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2024 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 山河忽晚 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script async data-pjax src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.1/bubble/bubble.js"></script><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: true,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', 'G-3RZJMX45BS', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>