<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>动手学深度学习1--预备知识</title>
      <link href="/20250628/"/>
      <url>/20250628/</url>
      
        <content type="html"><![CDATA[<p><strong>学习内容</strong></p><ul><li>深度学习基础——线性神经网络，多层感知机</li><li>卷积神经网络——LeNet，AlexNet，VGG，Inception，ResNet</li><li>循环神经网络——RNN，GRU，LSTM，seq2seq</li><li>注意力机制——Attention，Transformer</li><li>优化算法——SGD，Momentum，Adam</li><li>高性能计算——并行，多GPU，分布式</li><li>计算机视觉——目标检测，语义分割</li><li>自然语言处理——词嵌入，BERT</li></ul><h1 id="数据操作-预处理"><a href="#数据操作-预处理" class="headerlink" title="数据操作 + 预处理"></a>数据操作 + 预处理</h1><p><strong>N 维数组</strong>是机器学习和神经网络的主要数据结构。</p><ul><li>0-d（标量）——<code>1.0</code><ul><li>表示一个类别</li></ul></li><li>1-d（向量）——<code>[1.0, 2.7, 3.4]</code><ul><li>一个特征向量</li></ul></li><li>2-d（矩阵）——<code>[[1.0，2.7，3.4], [5.0，0.2，4.6], [4.3，8.5，0.2]]</code><ul><li>一个样本的特征矩阵，每一行表示一个样本，每一列表示样本的一个特征</li></ul></li><li>3-d  ——<code>[[[0.1，2.7，3.4], [5.0，0.2，4.6], [4.3，8.5，(0.2]], [[3.2，5.7，3.4], [5.4，6.2，3.2], [4.1，3.5，6.2]]]</code><ul><li>RGB图片（宽 x 高 x 通道）</li><li>宽是列的个数（有多少列），高是行的个数（有多少行），通道channel包括RGB三通道</li></ul></li><li>4-d  —— <code>[[[[....]]]]</code><ul><li>一个RGB图片批量（批量大小 x 宽 x 高 x 通道）</li><li>批量大小：batch_size</li></ul></li><li>5-d  ——<code>[[[[[....]]]]]</code><ul><li>一个视频批量（批量大小 x 时间 x 宽 x 高 x 通道）</li></ul></li></ul><h2 id="数组操作"><a href="#数组操作" class="headerlink" title="数组操作"></a>数组操作</h2><h3 id="操作原理"><a href="#操作原理" class="headerlink" title="操作原理"></a>操作原理</h3><p>创建数组需要：</p><ul><li>形状：例如 3 x 4 矩阵</li><li>每个元素的数据类型：例如32位浮点数</li><li>每个元素的值，例如全是0，或者随机数</li></ul><p><strong>访问元素：</strong><img src="/./20250628/image-20250621151410304.png" alt="image-20250621151410304"></p><p><img src="/./20250628/image-20250621151422645.png" alt="image-20250621151422645"></p><h3 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h3><p>导入<code>torch</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p>张量表示一个由数值组成的数组，这个数组可能有多个维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">12</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><blockquote><p>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</p></blockquote><p>可以通过张量的<code>shape</code>属性来访问张量（沿每个轴的长度）的<strong>形状</strong>和张量中元素的<strong>总数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><blockquote><p>torch.Size([12])# 只有一个维度，这个维度的长度是12</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.numel()<span class="comment"># x中元素的种类数</span></span><br></pre></td></tr></table></figure><blockquote><p>12# 结果为一个标量数据值</p></blockquote><p>要想改变一个张量的形状而不改变元素数量和元素值，可以调用<code>reshape</code>函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = x.reshape(<span class="number">3</span>, <span class="number">4</span>)<span class="comment"># 改为3行4列赋值给X</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 0,  1,  2,  3],<br>               [ 4,  5,  6,  7],<br>               [ 8,  9, 10, 11]])</p></blockquote><p>使用全0、全1、其他常量，或者从特定分布中随机采样的数字      </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))<span class="comment"># 生成全0元素</span></span><br><span class="line"></span><br><span class="line">torch.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))<span class="comment"># 生成全1元素</span></span><br><span class="line"></span><br><span class="line">torch.randn(<span class="number">3</span>, <span class="number">4</span>)<span class="comment"># 生成随机数元素</span></span><br></pre></td></tr></table></figure><p>通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[2, 1, 4, 3],<br>               [1, 2, 3, 4],<br>               [4, 3, 2, 1]])</p></blockquote><h3 id="数组运算"><a href="#数组运算" class="headerlink" title="数组运算"></a>数组运算</h3><p>常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为<strong>按元素</strong>运算</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span>])</span><br><span class="line">y = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">x + y, x - y, x * y, x / y, x ** y</span><br></pre></td></tr></table></figure><blockquote><p>tensor([ 3.,  4.,  6., 10.]),</p><p>tensor([-1.,  0.,  2.,  6.]),</p><p>tensor([ 2.,  4.,  8., 16.]),</p><p>tensor([0.5000, 1.0000, 2.0000, 4.0000]),</p><p>tensor([ 1.,  4., 16., 64.]))</p></blockquote><p>​        “按元素”方式可以应用更多的计算      </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.exp(x)</span><br></pre></td></tr></table></figure><blockquote><p>tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])</p></blockquote><p>我们也可以把多个张量<strong>连结</strong>（concatenate）在一起</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">12</span>, dtype=torch.float32).reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">Y = torch.tensor([[<span class="number">2.0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">torch.cat((X, Y), dim=<span class="number">0</span>), torch.cat((X, Y), dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># cat把X、Y合并在一起，dim等于几，就会改变哪个维度，相当于numpy里的axis参数</span></span><br><span class="line"><span class="comment"># dim=0表示在第0维（行）</span></span><br><span class="line"><span class="comment"># dim=1表示在第1维（列）合并</span></span><br></pre></td></tr></table></figure><blockquote><p>(tensor([[ 0.,  1.,  2.,  3.],<br>      [ 4.,  5.,  6.,  7.],<br>      [ 8.,  9., 10., 11.],<br>      [ 2.,  1.,  4.,  3.],<br>      [ 1.,  2.,  3.,  4.],<br>      [ 4.,  3.,  2.,  1.]]),<br>tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],<br>      [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],<br>      [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</p></blockquote><p>通过<strong>逻辑运算符</strong>构建二元张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X == Y</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[False,  True, False,  True],<br>     [False, False, False, False],<br>     [False, False, False, False]])</p></blockquote><p>对张量中的所有元素进行求和，会产生一个单元素张量      </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><blockquote><p>tensor(66.)</p></blockquote><h3 id="数组广播"><a href="#数组广播" class="headerlink" title="数组广播"></a>数组广播</h3><p>即使形状不同，我们仍然可以通过调用<strong>广播机制</strong>（broadcasting mechanism）来执行按元素操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">3</span>).reshape((<span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line">b = torch.arange(<span class="number">2</span>).reshape((<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">a, b</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([[0],<br>                [1],<br>                [2]]),</p><p>tensor([[0, 1]]))</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a + b<span class="comment"># 广播机制自动复制自身</span></span><br></pre></td></tr></table></figure><blockquote><p>[[0, 0], [[0, 1],[[0, 1],<br>     [1, 1],+[0, 1],&#x3D;       [1, 2],<br>     [2, 2]]  [0, 1]] [2, 3],</p></blockquote><p>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X[-<span class="number">1</span>], X[<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([ 8.,  9., 10., 11.]),<br>    tensor([[ 4.,  5.,  6.,  7.],<br>                [ 8.,  9., 10., 11.]]))</p></blockquote><p>除读取外，我们还可以通过指定索引来将元素写入矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">1</span>, <span class="number">2</span>] = <span class="number">9</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 0.,  1.,  2.,  3.],<br>               [ 4.,  5.,  9.,  7.],<br>               [ 8.,  9., 10., 11.]])</p></blockquote><p>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X[<span class="number">0</span>:<span class="number">2</span>, :] = <span class="number">12</span></span><br><span class="line">X</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[12., 12., 12., 12.],<br>               [12., 12., 12., 12.],<br>               [ 8.,  9., 10., 11.]])</p></blockquote><h3 id="内存分配"><a href="#内存分配" class="headerlink" title="内存分配"></a>内存分配</h3><p>运行一些操作可能会导致为新结果分配内存</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = <span class="built_in">id</span>(Y)</span><br><span class="line">Y = Y + X</span><br><span class="line"><span class="built_in">id</span>(Y) == before</span><br></pre></td></tr></table></figure><blockquote><p>False</p></blockquote><p>执行原地操作      </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Z = torch.zeros_like(Y)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br><span class="line">Z[:] = X + Y</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;id(Z):&#x27;</span>, <span class="built_in">id</span>(Z))</span><br></pre></td></tr></table></figure><blockquote><p>id(Z): 140327634811696<br>    id(Z): 140327634811696</p></blockquote><p>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">before = id(X)</span><br><span class="line">X += Y</span><br><span class="line">id(X) == before</span><br></pre></td></tr></table></figure><blockquote><p>True</p></blockquote><p>转换为NumPy张量（<code>ndarray</code>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = X.numpy()</span><br><span class="line">B = torch.tensor(A)</span><br><span class="line"><span class="built_in">type</span>(A), <span class="built_in">type</span>(B)</span><br></pre></td></tr></table></figure><blockquote><p>(numpy.ndarray, torch.Tensor)</p></blockquote><p>将大小为1的张量转换为Python标量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">3.5</span>])</span><br><span class="line">a, a.item(), <span class="built_in">float</span>(a), <span class="built_in">int</span>(a)</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([3.5000]), 3.5, 3.5, 3)</p></blockquote><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>是指如果我有一个原始数据，我怎么把它读取进来，使得我们通过机器学习的方法能够处理。</p><h3 id="创建数据集"><a href="#创建数据集" class="headerlink" title="创建数据集"></a>创建数据集</h3><p>创建一个人工数据集，并存储在CSV（逗号分隔值）文件      </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.makedirs(os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>), exist_ok=<span class="literal">True</span>)</span><br><span class="line">data_file = os.path.join(<span class="string">&#x27;..&#x27;</span>, <span class="string">&#x27;data&#x27;</span>, <span class="string">&#x27;house_tiny.csv&#x27;</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(data_file, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">&#x27;NumRooms,Alley,Price\n&#x27;</span>)<span class="comment"># 表头：房间数量，房间路线名称，房间价格</span></span><br><span class="line">    f.write(<span class="string">&#x27;NA,Pave,127500\n&#x27;</span>)<span class="comment"># NA表示未知的数</span></span><br><span class="line">    f.write(<span class="string">&#x27;2,NA,106000\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;4,NA,178100\n&#x27;</span>)</span><br><span class="line">    f.write(<span class="string">&#x27;NA,NA,140000\n&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><p>从创建的CSV文件中加载原始数据集</p><p>一般读取CSV文件使用<strong>pandas库</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(data_file)</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></table></figure><blockquote><p>   NumRooms   Alley   Price<br>   0       NaN  Pave  127500<br>   1        2.0   NaN   106000<br>   2        4.0   NaN   178100<br>   3       NaN  NaN  140000</p></blockquote><h3 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h3><p>为了处理缺失的数据，典型的方法包括<em>插值法</em>和<em>删除法</em>，</p><p>这里，我们将考虑插值法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inputs, outputs = data.iloc[:, <span class="number">0</span>:<span class="number">2</span>], data.iloc[:, <span class="number">2</span>]</span><br><span class="line">inputs = inputs.fillna(inputs.mean())<span class="comment"># mean()指input的均值</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><blockquote><p>   NumRooms Alley<br>   0       3.0  Pave<br>   1       2.0   NaN<br>   2       4.0   NaN<br>   3       3.0   NaN</p></blockquote><p>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别（Not a Number）</p><p>对于字符型数值，最好转变为数值类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把列里面所有出现的不同种类的值都变成一个特征</span></span><br><span class="line">inputs = pd.get_dummies(inputs, dummy_na=<span class="literal">True</span>)<span class="comment"># 给不同的类分别赋值为一个单独的特征</span></span><br><span class="line"><span class="built_in">print</span>(inputs)</span><br></pre></td></tr></table></figure><blockquote><p>   NumRooms  Alley_Pave  Alley_nan<br>   0       3.0           1          0<br>   1       2.0           0          1<br>   2       4.0           0          1<br>   3       3.0           0          1</p></blockquote><p>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">X = torch.tensor(inputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">y = torch.tensor(outputs.to_numpy(dtype=<span class="built_in">float</span>))</span><br><span class="line">X, y</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([[3., 1., 0.],<br>      [2., 0., 1.],<br>      [4., 0., 1.],<br>      [3., 0., 1.]], dtype&#x3D;torch.float64),<br>tensor([127500., 106000., 178100., 140000.], dtype&#x3D;torch.float64))</p></blockquote><p>传统python计算会用32位或64位浮点数，但是64位浮点数对深度学习来讲计算比较慢，一般指定数据类型为32位浮点数。</p><h1 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h1><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><p><strong>标量</strong>—-<strong>一个值</strong></p><ul><li><p>简单操作</p><ul><li>$ c &#x3D; a + b $ </li><li>$ c &#x3D; a \cdot b $  </li><li>$ c &#x3D; \sin a $</li></ul></li><li><p>长度</p><ul><li><p>$$<br>|a| &#x3D;<br>\begin{cases}<br>a &amp; \text{if } a &gt; 0 \<br>-a &amp; \text{otherwise}<br>\end{cases}<br>$$</p></li><li><p>$$ |a + b| \leq |a| + |b| $$  </p></li><li><p>$$ |a \cdot b| &#x3D; |a| \cdot |b| $$</p></li></ul></li></ul><p><strong>向量</strong> —-<strong>一行值</strong></p><ul><li><p>向量的简单操作</p><ul><li>$ c &#x3D; a + b \quad \text{where } c_i &#x3D; a_i + b_i $  </li><li>$ c &#x3D; \alpha \cdot b \quad \text{where } c_i &#x3D; \alpha b_i $  </li><li>$ c &#x3D; \sin a \quad \text{where } c_i &#x3D; \sin a_i $</li></ul></li><li><p>长度（<strong>范数</strong>）—-向量的长度就是向量的每个元素的平方求和再开根号</p><ul><li><p>范数（以 $L_2$ 范数为例）<br>$$<br>\lVert a \rVert_2 &#x3D; \left[ \sum_{i&#x3D;1}^m a_i^2 \right]^{\frac{1}{2}}<br>$$</p></li><li><p>$ \lVert a \rVert \geq 0 \ \text{for all } a $  </p></li><li><p>$ \lVert a + b \rVert \leq \lVert a \rVert + \lVert b \rVert $ </p></li><li><p>$ \lVert a \cdot b \rVert &#x3D; |a| \cdot \lVert b \rVert $</p></li></ul></li><li><p>点乘</p><ul><li>$a^\top b &#x3D; \sum_{i} a_i b_i$</li></ul></li><li><p>正交</p><ul><li>$a^\top b &#x3D; \sum_{i} a_i b_i &#x3D; 0$</li></ul></li></ul><p><strong>矩阵</strong></p><ul><li>简单操作<ul><li>矩阵加法：$ C &#x3D; A + B \quad \text{where } C_{ij} &#x3D; A_{ij} + B_{ij} $  </li><li>矩阵数乘：$ C &#x3D; \alpha \cdot B \quad \text{where } C_{ij} &#x3D; \alpha B_{ij} $  </li><li>矩阵逐元素正弦运算：$ C &#x3D; \sin A \quad \text{where } C_{ij} &#x3D; \sin A_{ij} $</li></ul></li><li>特征向量和特征值<ul><li>不被矩阵改变方向的向量</li><li>对称矩阵总是可以找到特征向量</li></ul></li><li>乘法（矩阵乘以向量）<ul><li>$$ \boldsymbol{c} &#x3D; \boldsymbol{A}\boldsymbol{b} \ \text{where} \ c_i &#x3D; \sum_j A_{ij} b_j $$</li></ul></li><li>乘法（矩阵乘以矩阵）  <ul><li>$$ \boldsymbol{C} &#x3D; \boldsymbol{A}\boldsymbol{B} \ \text{where} \ C_{ik} &#x3D; \sum_j A_{ij} B_{jk} $$</li></ul></li><li>范数<ul><li>$$ c &#x3D; A \cdot b \ \text{hence} \ \lVert c \rVert \leq \lVert A \rVert \cdot \lVert b \rVert $$  </li><li>$$取决于如何衡量 ( b ) 和 ( c ) 的长度$$</li></ul></li><li>常见范数<ul><li><strong>矩阵范数</strong>：最小的满足上面公式的值  </li><li><strong>Frobenius 范数</strong>：<br>$$ \lVert A \rVert_{\text{Frob}} &#x3D; \left[ \sum_{ij} A_{ij}^2 \right]^{\frac{1}{2}} $$</li></ul></li></ul><p><strong>特殊矩阵</strong></p><ul><li>对称和反对称<ul><li>$ A_{ij} &#x3D; A_{ji} $（对称矩阵，元素关于主对角线对称）  </li><li>$ A_{ij} &#x3D; -A_{ji} $（反对称矩阵，主对角线元素为 0，( (i,j) ) 与 ( (j,i) ) 元素互反 ）</li></ul></li><li>正定—-如果一个矩阵是正定的，那么它乘以任何一个列向量或行向量都大于等于0<ul><li>向量范数平方：    $$ \lVert \boldsymbol{x} \rVert^2 &#x3D; \boldsymbol{x}^\top \boldsymbol{x} \geq 0 $$<br>（欧几里得范数平方，内积定义，结果非负 ）  </li><li>二次型推广：    $$ \boldsymbol{x}^\top \boldsymbol{A}\boldsymbol{x} \geq 0 $$<br>（刻画半正定&#x2F;正定矩阵，( $\boldsymbol{A}$ ) 对称时，非零 ( $\boldsymbol{x}$ ) 代入结果非负则 ( $$\boldsymbol{A}$$ ) 半正定 ）</li></ul></li><li>正交矩阵  <ul><li>所有行都相互正交  </li><li>所有行都有单位长度 $$  U  \text{ with }  \sum_{j} U_{ij}U_{kj} &#x3D; \delta_{ik}   $$</li><li>可以写成 $$UU^\top &#x3D; \boldsymbol{1} $$</li></ul></li><li>置换矩阵  <ul><li>$$ P \text{ where } P_{ij} &#x3D; 1 \text{ if and only if }  j &#x3D; \pi(i) $$  </li><li>置换矩阵是正交矩阵</li></ul></li></ul><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="标量计算"><a href="#标量计算" class="headerlink" title="标量计算"></a>标量计算</h3><p>标量由只有一个元素的张量表示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line">y = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">x + y, x * y, x / y, x**y</span><br></pre></td></tr></table></figure><blockquote><p>(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</p></blockquote><h3 id="向量基础"><a href="#向量基础" class="headerlink" title="向量基础"></a>向量基础</h3><p>向量可以被视为标量值组成的列表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><blockquote><p>tensor([0, 1, 2, 3])</p></blockquote><p>通过张量的索引来访问任一元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[<span class="number">3</span>]</span><br></pre></td></tr></table></figure><blockquote><p>tensor(3)</p></blockquote><p>访问张量的长度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(x)</span><br></pre></td></tr></table></figure><blockquote><p>4</p></blockquote><p>只有一个轴的张量，形状只有一个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><blockquote><p>torch.Size([4])</p></blockquote><h3 id="矩阵基础"><a href="#矩阵基础" class="headerlink" title="矩阵基础"></a>矩阵基础</h3><p>通过指定两个分量$m$和$n$来创建一个形状为$m \times n$的矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">A</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 0,  1,  2,  3],<br>               [ 4,  5,  6,  7],<br>               [ 8,  9, 10, 11],<br>               [12, 13, 14, 15],<br>               [16, 17, 18, 19]])</p></blockquote><p>矩阵的转置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.T</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 0,  4,  8, 12, 16],<br>               [ 1,  5,  9, 13, 17],<br>               [ 2,  6, 10, 14, 18],<br>               [ 3,  7, 11, 15, 19]])</p></blockquote><p><em>对称矩阵</em>（symmetric matrix）$\mathbf{A}$等于其转置：$\mathbf{A} &#x3D; \mathbf{A}^\top$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">4</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]])</span><br><span class="line">B</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[1, 2, 3],<br>               [2, 0, 4],<br>               [3, 4, 5]])</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">B == B.T</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[True, True, True],<br>               [True, True, True],<br>               [True, True, True]])</p></blockquote><h3 id="矩阵形状改变"><a href="#矩阵形状改变" class="headerlink" title="矩阵形状改变"></a>矩阵形状改变</h3><p>就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构</p><p>只要元素个数不发生变化，就可以根据个数改变矩阵的形状</p><p><strong>行是最后一维（有几列）</strong>，<strong>列是倒数第二维（有几行）</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[[ 0,  1,  2,  3],<br>                [ 4,  5,  6,  7],<br>                [ 8,  9, 10, 11]],</p><p>​      [[12, 13, 14, 15],<br>​               [16, 17, 18, 19],<br>​               [20, 21, 22, 23]]])</p></blockquote><h3 id="复制矩阵"><a href="#复制矩阵" class="headerlink" title="复制矩阵"></a>复制矩阵</h3><p>给定具有相同形状的任意两个张量，任何按元素<strong>二元运算</strong>的结果都将是<strong>相同形状</strong>的张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">A = torch.arange(<span class="number">20</span>, dtype=torch.float32).reshape(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">B = A.clone()</span><br><span class="line">A, A + B</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([[ 0.,  1.,  2.,  3.],<br>                [ 4.,  5.,  6.,  7.],<br>                [ 8.,  9., 10., 11.],<br>                [12., 13., 14., 15.],<br>                [16., 17., 18., 19.]]),</p><p>tensor([[ 0.,  2.,  4.,  6.],<br>                [ 8., 10., 12., 14.],<br>                [16., 18., 20., 22.],<br>                [24., 26., 28., 30.],<br>                [32., 34., 36., 38.]]))</p></blockquote><h3 id="矩阵元素积"><a href="#矩阵元素积" class="headerlink" title="矩阵元素积"></a>矩阵元素积</h3><p>两个矩阵的按元素乘法称为<strong>Hadamard</strong>积（Hadamard product）（数学符号$\odot$）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A * B</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[  0.,   1.,   4.,   9.],<br>               [ 16.,  25.,  36.,  49.],<br>               [ 64.,  81., 100., 121.],<br>               [144., 169., 196., 225.],<br>               [256., 289., 324., 361.]])</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="number">2</span></span><br><span class="line">X = torch.arange(<span class="number">24</span>).reshape(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a + X, (a * X).shape</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([[[ 2,  3,  4,  5],<br>               [ 6,  7,  8,  9],<br>               [10, 11, 12, 13]],</p><p>​     [[14, 15, 16, 17],<br>​              [18, 19, 20, 21],<br>​              [22, 23, 24, 25]]]),</p><p>torch.Size([2, 3, 4]))</p></blockquote><h3 id="求和运算"><a href="#求和运算" class="headerlink" title="求和运算"></a>求和运算</h3><p>计算向量元素的和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line">x, x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([0., 1., 2., 3.]), tensor(6.))</p></blockquote><p>表示任意形状张量的元素和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, A.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><blockquote><p>(torch.Size([5, 4]), tensor(190.))</p></blockquote><h3 id="指定轴求和"><a href="#指定轴求和" class="headerlink" title="指定轴求和"></a>指定轴求和</h3><p>指定张量沿哪一个轴来通过求和降低维度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis0 = A.<span class="built_in">sum</span>(axis=<span class="number">0</span>)<span class="comment"># 对A.shape的第0维进行求和运算，剩下的矩阵是A.shape去掉第0维后的形状</span></span><br><span class="line">A_sum_axis0, A_sum_axis0.shape</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([40., 45., 50., 55.]), torch.Size([4]))</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A_sum_axis1 = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>)<span class="comment"># 对A.shape的第1维进行求和运算，剩下的矩阵是A.shape去掉第1维后的形状</span></span><br><span class="line">A_sum_axis1, A_sum_axis1.shape</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.<span class="built_in">sum</span>(axis=[<span class="number">0</span>, <span class="number">1</span>])<span class="comment"># 对A.shape的第0维和第1维都进行求和运算，剩下的矩阵是A.shape去掉第0维和第1维后的形状</span></span><br></pre></td></tr></table></figure><blockquote><p>tensor(190.)</p></blockquote><h3 id="平均值计算"><a href="#平均值计算" class="headerlink" title="平均值计算"></a>平均值计算</h3><p>一个与求和相关的量是<strong>平均值</strong>（mean或average）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.mean(), A.<span class="built_in">sum</span>() / A.numel()</span><br></pre></td></tr></table></figure><blockquote><p>(tensor(9.5000), tensor(9.5000))</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.mean(axis=<span class="number">0</span>), A.<span class="built_in">sum</span>(axis=<span class="number">0</span>) / A.shape[<span class="number">0</span>]<span class="comment"># 按某维度来计算</span></span><br></pre></td></tr></table></figure><blockquote><p>(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))</p></blockquote><h3 id="沿轴求和时维度不变"><a href="#沿轴求和时维度不变" class="headerlink" title="沿轴求和时维度不变"></a>沿轴求和时维度不变</h3><p>计算总和或均值时保持轴数不变</p><p>keepdims&#x3D;True时，指不会把要求和的维度去掉而是把<strong>shape中对应的那个维度</strong>变成1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum_A = A.<span class="built_in">sum</span>(axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">sum_A</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 6.],<br>               [22.],<br>               [38.],<br>               [54.],<br>               [70.]])</p></blockquote><p>通过广播将<code>A</code>除以<code>sum_A</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A / sum_A</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[0.0000, 0.1667, 0.3333, 0.5000],<br>               [0.1818, 0.2273, 0.2727, 0.3182],<br>               [0.2105, 0.2368, 0.2632, 0.2895],<br>               [0.2222, 0.2407, 0.2593, 0.2778],<br>               [0.2286, 0.2429, 0.2571, 0.2714]])</p></blockquote><h3 id="沿轴累加求和"><a href="#沿轴累加求和" class="headerlink" title="沿轴累加求和"></a>沿轴累加求和</h3><p>某个轴计算<code>A</code>元素的累积总和      </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.cumsum(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 0.,  1.,  2.,  3.],<br>               [ 4.,  6.,  8., 10.],<br>               [12., 15., 18., 21.],<br>               [24., 28., 32., 36.],<br>               [40., 45., 50., 55.]])</p></blockquote><p>点积是相同位置的按元素乘积的和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = torch.ones(<span class="number">4</span>, dtype = torch.float32)</span><br><span class="line">x, y, torch.dot(x, y)</span><br></pre></td></tr></table></figure><blockquote><p>(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))</p></blockquote><h3 id="向量点积计算"><a href="#向量点积计算" class="headerlink" title="向量点积计算"></a>向量点积计算</h3><p>我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(x * y)</span><br></pre></td></tr></table></figure><blockquote><p>tensor(6.)</p></blockquote><h3 id="矩阵与向量积计算"><a href="#矩阵与向量积计算" class="headerlink" title="矩阵与向量积计算"></a>矩阵与向量积计算</h3><p>矩阵向量积$\mathbf{A}\mathbf{x}$是一个长度为$m$的列向量，其第$i$个元素是点积$\mathbf{a}^\top_i \mathbf{x}$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A.shape, x.shape, torch.mv(A, x)</span><br></pre></td></tr></table></figure><blockquote><p>(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))</p></blockquote><h3 id="矩阵x矩阵计算"><a href="#矩阵x矩阵计算" class="headerlink" title="矩阵x矩阵计算"></a>矩阵x矩阵计算</h3><p>我们可以将矩阵-矩阵乘法$\mathbf{AB}$看作简单地执行$m$次矩阵-向量积，并将结果拼接在一起，形成一个$n \times m$矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">B = torch.ones(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">torch.mm(A, B)</span><br></pre></td></tr></table></figure><blockquote><p>tensor([[ 6.,  6.,  6.],<br>               [22., 22., 22.],<br>               [38., 38., 38.],<br>               [54., 54., 54.],<br>               [70., 70., 70.]])</p></blockquote><h3 id="向量范数"><a href="#向量范数" class="headerlink" title="向量范数"></a>向量范数</h3><p>$L_2$<em>范数</em>是向量元素平方和的平方根：</p><p>$$|\mathbf{x}|<em>2 &#x3D; \sqrt{\sum</em>{i&#x3D;1}^n x_i^2}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">u = torch.tensor([<span class="number">3.0</span>, -<span class="number">4.0</span>])</span><br><span class="line">torch.norm(u)</span><br></pre></td></tr></table></figure><blockquote><p>tensor(5.)</p></blockquote><p>$L_1$范数，它表示为向量元素的绝对值之和：</p><p>$$|\mathbf{x}|<em>1 &#x3D; \sum</em>{i&#x3D;1}^n \left|x_i \right|$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(u).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><blockquote><p>tensor(7.)</p></blockquote><h3 id="矩阵范数"><a href="#矩阵范数" class="headerlink" title="矩阵范数"></a>矩阵范数</h3><p>矩阵的<em>Frobenius范数</em>（Frobenius norm）是矩阵元素平方和的平方根：</p><p>$$|\mathbf{X}|<em>F &#x3D; \sqrt{\sum</em>{i&#x3D;1}^m \sum_{j&#x3D;1}^n x_{ij}^2}$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(torch.ones((<span class="number">4</span>, <span class="number">9</span>)))</span><br></pre></td></tr></table></figure><blockquote><p>tensor(6.)</p></blockquote><h1 id="微积分"><a href="#微积分" class="headerlink" title="微积分"></a>微积分</h1><p>矩阵的计算部分要清楚如何求导数！</p><h2 id="理论基础-1"><a href="#理论基础-1" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="标量导数"><a href="#标量导数" class="headerlink" title="标量导数"></a>标量导数</h3><ul><li>导数是切线的斜率</li></ul><table><thead><tr><th>$y$</th><th>$a$</th><th>$x^n$</th><th>$\exp(x)$</th><th>$\log(x)$</th><th>$\sin(x)$</th></tr></thead><tbody><tr><td>$\dfrac{dy}{dx}$</td><td>$0$</td><td>$n x^{n-1}$</td><td>$\exp(x)$</td><td>$\dfrac{1}{x}$</td><td>$\cos(x)$</td></tr></tbody></table><blockquote><p>注：$a$ 不是 $x$ 的函数  </p></blockquote><table><thead><tr><th>$y$</th><th>$u + v$</th><th>$uv$</th><th>$y &#x3D; f(u), , u &#x3D; g(x)$</th></tr></thead><tbody><tr><td>$\dfrac{dy}{dx}$</td><td>$\dfrac{du}{dx} + \dfrac{dv}{dx}$</td><td>$\dfrac{du}{dx} v + \dfrac{dv}{dx} u$</td><td>$\dfrac{dy}{du} \dfrac{du}{dx}$</td></tr></tbody></table><h3 id="亚导数"><a href="#亚导数" class="headerlink" title="亚导数"></a>亚导数</h3><blockquote><p>当不存在导数时怎么办？</p></blockquote><ul><li><p>将导数拓展到不可微的函数</p><ul><li><p>$ y &#x3D; |x| $</p></li><li><p>绝对值函数的导数</p></li><li><p>$$<br>\frac{\partial |x|}{\partial x} &#x3D;<br>\begin{cases}<br>1 &amp; \text{if } x &gt; 0 \<br>-1 &amp; \text{if } x &lt; 0 \<br>a &amp; \text{if } x &#x3D; 0, \ a \in [-1,1]<br>\end{cases}<br>$$</p></li><li><p>ReLU 函数的导数（分段形式）：<br>$$<br>\frac{\partial}{\partial x} \text{max}(x, 0) &#x3D;<br>\begin{cases}<br>1 &amp; \text{if } x &gt; 0 \<br>0 &amp; \text{if } x &lt; 0 \<br>a &amp; \text{if } x &#x3D; 0, \ a \in [0,1]<br>\end{cases}<br>$$</p></li></ul></li></ul><h3 id="向量梯度"><a href="#向量梯度" class="headerlink" title="向量梯度"></a>向量梯度</h3><ul><li>将导数拓展到向量，需要注意向量或矩阵的形状！！<ul><li><img src="/./20250628/image-20250622164809141.png" alt="image-20250622164809141"></li></ul></li><li>求导后梯度的形状，按照<strong>分子布局符号</strong>，如上图<ul><li>y是列向量，x是标量，求导后的结果是y的形状（列向量）</li><li>x是列向量，y是标量，求导后的结果<strong>行向量</strong></li><li>x是向量，y是向量，求导后的结果是矩阵</li><li>如果是反过来的情况，则称为<strong>分母布局符号</strong></li></ul></li></ul><h4 id="单向量梯度样例"><a href="#单向量梯度样例" class="headerlink" title="单向量梯度样例"></a>单向量梯度样例</h4><p><strong>x是列向量</strong>，<strong>y是标量</strong></p><table><thead><tr><th>$y$</th><th>表达式</th><th>$\dfrac{\partial y}{\partial \boldsymbol{x}}$</th><th>补充说明</th></tr></thead><tbody><tr><td>$y &#x3D; a$</td><td>标量（与 $\boldsymbol{x}$ 无关）</td><td>$\boldsymbol{0}^\top$</td><td>$a$ 不是 $\boldsymbol{x}$ 的函数</td></tr><tr><td>$y &#x3D; a u$</td><td>标量（$u$ 是 $\boldsymbol{x}$ 的函数）</td><td>$a \dfrac{\partial u}{\partial \boldsymbol{x}}$</td><td>数乘求导法则</td></tr><tr><td>$y &#x3D; \text{sum}(\boldsymbol{x})$</td><td>向量元素和</td><td>$\boldsymbol{1}^\top$</td><td>对向量各元素求和后求导</td></tr><tr><td>$y &#x3D; \lVert \boldsymbol{x} \rVert^2$</td><td>向量 $L_2$ 范数平方</td><td>$2\boldsymbol{x}^\top$</td><td>由 $\lVert \boldsymbol{x} \rVert^2 &#x3D; \boldsymbol{x}^\top \boldsymbol{x}$ 求导</td></tr></tbody></table><p><strong>符号说明</strong>  </p><ul><li>$\boldsymbol{0}$：零向量（元素全 0 ），$\boldsymbol{0}^\top$ 是其转置（行向量）；  </li><li>$\boldsymbol{1}$：全 1 向量（元素全 1 ），$\boldsymbol{1}^\top$ 是其转置（行向量）；  </li><li>用途：向量微积分、机器学习梯度推导基础规则。</li></ul><table><thead><tr><th>$y$</th><th>表达式</th><th>$\dfrac{\partial y}{\partial \boldsymbol{x}}$</th><th>说明</th></tr></thead><tbody><tr><td>$y &#x3D; u + v$</td><td>和（$u,v$ 是标量&#x2F;向量函数）</td><td>$\dfrac{\partial u}{\partial \boldsymbol{x}} + \dfrac{\partial v}{\partial \boldsymbol{x}}$</td><td>加法求导法则</td></tr><tr><td>$y &#x3D; u v$</td><td>积（$u,v$ 是标量&#x2F;向量函数）</td><td>$\dfrac{\partial u}{\partial \boldsymbol{x}} v + \dfrac{\partial v}{\partial \boldsymbol{x}} u$</td><td>乘积求导法则</td></tr><tr><td>$y &#x3D; \langle \boldsymbol{u}, \boldsymbol{v} \rangle$</td><td>内积（$\boldsymbol{u},\boldsymbol{v}$ 是向量函数）</td><td>$\boldsymbol{u}^\top \dfrac{\partial \boldsymbol{v}}{\partial \boldsymbol{x}} + \boldsymbol{v}^\top \dfrac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}}$</td><td>内积求导法则</td></tr></tbody></table><p><strong>用途说明</strong>：向量微积分基础规则，用于机器学习（如反向传播）、优化理论中推导梯度，计算复杂函数对参数向量的导数。 </p><h3 id="向量对向量的导数"><a href="#向量对向量的导数" class="headerlink" title="向量对向量的导数"></a>向量对向量的导数</h3><p>（雅可比矩阵，Jacobian Matrix）  </p><p>给定：  $$ \boldsymbol{x} &#x3D; \begin{bmatrix} x_1 \ x_2 \ \vdots \ x_n \end{bmatrix}, \quad \boldsymbol{y} &#x3D; \begin{bmatrix} y_1 \ y_2 \ \vdots \ y_m \end{bmatrix} $$  </p><p>雅可比矩阵定义：<br>$$<br>\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} &#x3D; \begin{bmatrix}<br>\frac{\partial y_1}{\partial \boldsymbol{x}} \<br>\frac{\partial y_2}{\partial \boldsymbol{x}} \<br>\vdots \<br>\frac{\partial y_m}{\partial \boldsymbol{x}}<br>\end{bmatrix} &#x3D; \begin{bmatrix}<br>\frac{\partial y_1}{\partial x_1}, \frac{\partial y_1}{\partial x_2}, \dots, \frac{\partial y_1}{\partial x_n} \<br>\frac{\partial y_2}{\partial x_1}, \frac{\partial y_2}{\partial x_2}, \dots, \frac{\partial y_2}{\partial x_n} \<br>\vdots \<br>\frac{\partial y_m}{\partial x_1}, \frac{\partial y_m}{\partial x_2}, \dots, \frac{\partial y_m}{\partial x_n}<br>\end{bmatrix}<br>$$</p><p>首先把y拆分后对向量x求导，再计算单独求导后的结果。</p><h4 id="雅可比矩阵"><a href="#雅可比矩阵" class="headerlink" title="雅可比矩阵"></a>雅可比矩阵</h4><table><thead><tr><th>$\boldsymbol{y}$</th><th>$\dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$</th><th>维度与说明</th></tr></thead><tbody><tr><td>$\boldsymbol{y} &#x3D; \boldsymbol{a}$（常向量，与 $\boldsymbol{x}$ 无关）</td><td>$\boldsymbol{0}$（零矩阵）</td><td>$\boldsymbol{a}$ 不随 $\boldsymbol{x}$ 变化</td></tr><tr><td>$\boldsymbol{y} &#x3D; \boldsymbol{x}$（恒等映射）</td><td>$\boldsymbol{I}$（单位矩阵）</td><td>自身对自身求导为单位矩阵</td></tr><tr><td>$\boldsymbol{y} &#x3D; \boldsymbol{A}\boldsymbol{x}$（矩阵-向量乘法）</td><td>$\boldsymbol{A}$（原矩阵）</td><td>线性变换导数是变换矩阵本身</td></tr><tr><td>$\boldsymbol{y} &#x3D; \boldsymbol{x}^\top \boldsymbol{A}$（向量-矩阵乘法）</td><td>$\boldsymbol{A}^\top$（矩阵转置）</td><td>转置保证导数维度 $\mathbb{R}^{m \times n} $</td></tr></tbody></table><p>补充条件  </p><ul><li>维度： $\boldsymbol{x} \in \mathbb{R}^n$ ， $\boldsymbol{y} \in \mathbb{R}^m $，故  $\dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} \in \mathbb{R}^{m \times n} $；  </li><li>常量： $a, \boldsymbol{a}, \boldsymbol{A} $均与 $\boldsymbol{x} $无关；  </li><li>符号：$\boldsymbol{0}$ （零矩阵）、 $\boldsymbol{I}$ （单位矩阵） 。</li></ul><p><strong>用途</strong>：向量微积分基础，机器学习（如反向传播）、控制理论中线性变换梯度推导的核心规则。  </p><table><thead><tr><th>$\boldsymbol{y}$</th><th>$\dfrac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$</th><th>说明</th></tr></thead><tbody><tr><td>$\boldsymbol{y} &#x3D; a \boldsymbol{u}$（标量-向量数乘）</td><td>$a \dfrac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}}$</td><td>数乘法则（标量提至导数外）</td></tr><tr><td>$\boldsymbol{y} &#x3D; \boldsymbol{A}\boldsymbol{u}$（矩阵-向量乘法）</td><td>$\boldsymbol{A} \dfrac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}}$</td><td>线性变换法则（矩阵提至导数外）</td></tr><tr><td>$\boldsymbol{y} &#x3D; \boldsymbol{u} + \boldsymbol{v}$（向量和）</td><td>$\dfrac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}} + \dfrac{\partial \boldsymbol{v}}{\partial \boldsymbol{x}}$</td><td>加法法则（和的导数拆为导数和）</td></tr></tbody></table><p>用途说明  </p><p>向量微积分基础规则，用于机器学习（如神经网络反向传播）、优化理论中，推导线性&#x2F;仿射变换的梯度，计算损失函数对参数的导数。  </p><h3 id="向量链式法制"><a href="#向量链式法制" class="headerlink" title="向量链式法制"></a>向量链式法制</h3><ul><li>标量链式法则</li></ul><p>若  $y &#x3D; f(u) $，$u &#x3D; g(x) $（复合函数， y 经  u 依赖 x  ），则： </p><p>$$ \frac{dy}{dx} &#x3D; \frac{dy}{du} \cdot \frac{du}{dx} $$  </p><p><strong>说明</strong>：微积分核心求导法则，是神经网络反向传播的数学基础（分解复杂梯度为多层简单梯度乘积）。  </p><ul><li><p>标量中间变量 u </p><ul><li><p>$若  y &#x3D; f(u) （ y  标量）， u &#x3D; g(\boldsymbol{x}) （ u 标量， \boldsymbol{x} \in \mathbb{R}^n ）$$：<br>$$ \frac{dy}{\partial \boldsymbol{x}} &#x3D; \frac{dy}{du} \cdot \frac{\partial u}{\partial \boldsymbol{x}} $</p></li><li><p>维度：$$ \frac{dy}{\partial \boldsymbol{x}} (1,n) &#x3D; \frac{dy}{du} (1,) \cdot \frac{\partial u}{\partial \boldsymbol{x}} (1,n)  $$</p></li></ul></li><li><p>拓展到向量，向量中间变量 $$ \boldsymbol{u} $$ </p><ul><li>若 $y &#x3D; f(\boldsymbol{u}) （y  标量）， \boldsymbol{u} &#x3D; g(\boldsymbol{x}) （ \boldsymbol{u} \in \mathbb{R}^k ，\boldsymbol{x} \in \mathbb{R}^n  ）$：  $$ \frac{dy}{\partial \boldsymbol{x}} &#x3D; \frac{dy}{\partial \boldsymbol{u}} \cdot \frac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}} $$  </li><li>维度：$$ \frac{dy}{\partial \boldsymbol{x}} (1,n) &#x3D; \frac{dy}{\partial \boldsymbol{u}} (1,k) \cdot \frac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}} (k,n) $$</li></ul></li><li><p>向量对向量的链式求导法则  </p><ul><li>若 $\boldsymbol{y}  经向量  \boldsymbol{u}  依赖于 \boldsymbol{x} $，则：  $$ \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} &#x3D; \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{u}} \cdot \frac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}} $$</li></ul></li><li><p>维度说明  </p><ul><li><p>$\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} ：(m, n) 矩阵（ \boldsymbol{y} \in \mathbb{R}^m ， \boldsymbol{x} \in \mathbb{R}^n  ）$</p></li><li><p>$$ \frac{\partial \boldsymbol{y}}{\partial \boldsymbol{u}} ： (m, k)  矩阵（ \boldsymbol{u} \in \mathbb{R}^k ）  $$</p></li><li><p>$\frac{\partial \boldsymbol{u}}{\partial \boldsymbol{x}} ： (k, n)  矩阵  $</p></li></ul></li><li><p>合法性：需满足 ( k ) 维度匹配（前导矩阵列数 &#x3D; 后续矩阵行数）。</p></li></ul><h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><p>如果$f$的<em>导数</em>存在，这个极限被定义为：</p><p>$$f’(x) &#x3D; \lim_{h \rightarrow 0} \frac{f(x+h) - f(x)}{h}$$</p><p>定义$u&#x3D;f(x)&#x3D;3x^2-4x$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib_inline <span class="keyword">import</span> backend_inline</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3</span> * x ** <span class="number">2</span> - <span class="number">4</span> * x</span><br></pre></td></tr></table></figure><p>通过令$x&#x3D;1$并让$h$接近$0$，$\frac{f(x+h)-f(x)}{h}$的数值结果接近$2$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numerical_lim</span>(<span class="params">f, x, h</span>):</span><br><span class="line">    <span class="keyword">return</span> (f(x + h) - f(x)) / h</span><br><span class="line"></span><br><span class="line">h = <span class="number">0.1</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;h=<span class="subst">&#123;h:<span class="number">.5</span>f&#125;</span>, numerical limit=<span class="subst">&#123;numerical_lim(f, <span class="number">1</span>, h):<span class="number">.5</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    h *= <span class="number">0.1</span></span><br></pre></td></tr></table></figure><blockquote><p>h&#x3D;0.10000, numerical limit&#x3D;2.30000<br>    h&#x3D;0.01000, numerical limit&#x3D;2.03000<br>    h&#x3D;0.00100, numerical limit&#x3D;2.00300<br>    h&#x3D;0.00010, numerical limit&#x3D;2.00030<br>    h&#x3D;0.00001, numerical limit&#x3D;2.00003</p></blockquote><p>为了对导数的这种解释进行可视化，我们将使用<code>matplotlib</code>定义几个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">use_svg_display</span>():  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;使用svg格式在Jupyter中显示绘图&quot;&quot;&quot;</span></span><br><span class="line">    backend_inline.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的图表大小&quot;&quot;&quot;</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    d2l.plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_axes</span>(<span class="params">axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;设置matplotlib的轴&quot;&quot;&quot;</span></span><br><span class="line">    axes.set_xlabel(xlabel)</span><br><span class="line">    axes.set_ylabel(ylabel)</span><br><span class="line">    axes.set_xscale(xscale)</span><br><span class="line">    axes.set_yscale(yscale)</span><br><span class="line">    axes.set_xlim(xlim)</span><br><span class="line">    axes.set_ylim(ylim)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        axes.legend(legend)</span><br><span class="line">    axes.grid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot</span>(<span class="params">X, Y=<span class="literal">None</span>, xlabel=<span class="literal">None</span>, ylabel=<span class="literal">None</span>, legend=<span class="literal">None</span>, xlim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">         ylim=<span class="literal">None</span>, xscale=<span class="string">&#x27;linear&#x27;</span>, yscale=<span class="string">&#x27;linear&#x27;</span>,</span></span><br><span class="line"><span class="params">         fmts=(<span class="params"><span class="string">&#x27;-&#x27;</span>, <span class="string">&#x27;m--&#x27;</span>, <span class="string">&#x27;g-.&#x27;</span>, <span class="string">&#x27;r:&#x27;</span></span>), figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>), axes=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;绘制数据点&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> legend <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        legend = []</span><br><span class="line"></span><br><span class="line">    set_figsize(figsize)</span><br><span class="line">    axes = axes <span class="keyword">if</span> axes <span class="keyword">else</span> d2l.plt.gca()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">has_one_axis</span>(<span class="params">X</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="built_in">hasattr</span>(X, <span class="string">&quot;ndim&quot;</span>) <span class="keyword">and</span> X.ndim == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">isinstance</span>(X, <span class="built_in">list</span>)</span><br><span class="line">                <span class="keyword">and</span> <span class="keyword">not</span> <span class="built_in">hasattr</span>(X[<span class="number">0</span>], <span class="string">&quot;__len__&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> has_one_axis(X):</span><br><span class="line">        X = [X]</span><br><span class="line">    <span class="keyword">if</span> Y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        X, Y = [[]] * <span class="built_in">len</span>(X), X</span><br><span class="line">    <span class="keyword">elif</span> has_one_axis(Y):</span><br><span class="line">        Y = [Y]</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(X) != <span class="built_in">len</span>(Y):</span><br><span class="line">        X = X * <span class="built_in">len</span>(Y)</span><br><span class="line">    axes.cla()</span><br><span class="line">    <span class="keyword">for</span> x, y, fmt <span class="keyword">in</span> <span class="built_in">zip</span>(X, Y, fmts):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(x):</span><br><span class="line">            axes.plot(x, y, fmt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            axes.plot(y, fmt)</span><br><span class="line">    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)</span><br></pre></td></tr></table></figure><p>绘制函数$u&#x3D;f(x)$及其在$x&#x3D;1$处的切线$y&#x3D;2x-3$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span>, <span class="number">0.1</span>)</span><br><span class="line">plot(x, [f(x), <span class="number">2</span> * x - <span class="number">3</span>], <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;f(x)&#x27;</span>, legend=[<span class="string">&#x27;f(x)&#x27;</span>, <span class="string">&#x27;Tangent line (x=1)&#x27;</span>])</span><br></pre></td></tr></table></figure><h1 id="自动求导"><a href="#自动求导" class="headerlink" title="自动求导"></a>自动求导</h1><p>自动求导计算一个函数在指定值上的导数</p><p>它有别于<strong>符号求导</strong>和<strong>数值求导</strong></p><h2 id="理论基础-2"><a href="#理论基础-2" class="headerlink" title="理论基础"></a>理论基础</h2><h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>将代码分解成操作子</p><p>将计算表示成一个<strong>无环图</strong></p><p><img src="/./20250628/image-20250626171942683.png" alt="image-20250626171942683"></p><p>显示构造，可实现框架：Tensorflow、Theano、MXNet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> sym</span><br><span class="line"></span><br><span class="line">a = sym.var()</span><br><span class="line">b = sym.var()</span><br><span class="line">c = <span class="number">2</span> * a + b</span><br><span class="line"><span class="comment"># bind data into a and b later</span></span><br></pre></td></tr></table></figure><p>隐式构造，可实现框架：PyTorch、MXNet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    a = nd.ones((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    b = nd.ones((<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line">    c = <span class="number">2</span> * a + b</span><br></pre></td></tr></table></figure><h3 id="两种模式"><a href="#两种模式" class="headerlink" title="两种模式"></a>两种模式</h3><p>核心原理<strong>链式法则</strong>：$$\frac{\partial y}{\partial x} &#x3D; \frac{\partial y}{\partial u_n} \frac{\partial u_n}{\partial u_{n - 1}} \dots \frac{\partial u_2}{\partial u_1} \frac{\partial u_1}{\partial x}$$ </p><ul><li><p>正向积累：$$\frac{\partial y}{\partial x} &#x3D; \frac{\partial y}{\partial u_n} \left( \frac{\partial u_n}{\partial u_{n - 1}} \left( \dots \left( \frac{\partial u_2}{\partial u_1} \frac{\partial u_1}{\partial x} \right) \right) \right)$$  </p></li><li><p>反向累积、又称<strong>反向传递</strong>  $$\frac{\partial y}{\partial x} &#x3D; \left( \left( \left( \frac{\partial y}{\partial u_n} \frac{\partial u_n}{\partial u_{n - 1}} \right) \dots \right) \frac{\partial u_2}{\partial u_1} \right) \frac{\partial u_1}{\partial x}$$</p></li></ul><h3 id="反向累积总结"><a href="#反向累积总结" class="headerlink" title="反向累积总结"></a>反向累积总结</h3><ul><li>构造计算图</li><li>前向：执行图，存储中间结果</li><li>反向：从相反方向执行图<ul><li>去除不需要的枝</li></ul></li></ul><p><img src="/./20250628/image-20250626173435627.png" alt="image-20250626173435627"></p><h3 id="复杂度"><a href="#复杂度" class="headerlink" title="复杂度"></a>复杂度</h3><ul><li>计算复杂度：$O(n)$, $n$ 是操作子个数  <ul><li>通常正向和反向的代价类似</li></ul></li><li>内存复杂度: $O(n)$，因为需要存储正向的所有中间结果 。（耗费计算资源的主要原因）</li><li>跟正向累积对比：  <ul><li>$O(n)$ 计算复杂度用来计算一个变量的梯度  </li><li>$O(1)$ 内存复杂度</li></ul></li></ul><h2 id="代码实现-2"><a href="#代码实现-2" class="headerlink" title="代码实现"></a>代码实现</h2><p>假设我们想对函数$y&#x3D;2\mathbf{x}^{\top}\mathbf{x}$关于列向量$\mathbf{x}$求导</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">4.0</span>)</span><br></pre></td></tr></table></figure><p>在我们计算$y$关于$\mathbf{x}$的梯度之前，需要一个地方来存储梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x.requires_grad_(<span class="literal">True</span>)<span class="comment"># 等价于x = torch.arange(4.0, requires_grad=True)，默认值是None</span></span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><p>现在计算$y$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = <span class="number">2</span> * torch.dot(x, x)</span><br></pre></td></tr></table></figure><blockquote><p>tensor(28., grad_fn&#x3D;<MulBackward0>)</p></blockquote><p>通过调用反向传播函数来自动计算<code>y</code>关于<code>x</code>每个分量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.backward()<span class="comment"># 求导</span></span><br><span class="line">x.grad<span class="comment"># 访问导数</span></span><br></pre></td></tr></table></figure><blockquote><p>tensor([ 0.,  4.,  8., 12.])</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad == <span class="number">4</span> * x</span><br></pre></td></tr></table></figure><blockquote><p>tensor([True, True, True, True])</p></blockquote><p>现在计算<code>x</code>的另一个函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x.<span class="built_in">sum</span>()</span><br><span class="line">y.backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><blockquote><p>tensor([1., 1., 1., 1.])</p></blockquote><p>深度学习中，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad</span><br></pre></td></tr></table></figure><blockquote><p>tensor([0., 2., 4., 6.])</p></blockquote><p>将某些计算移动到记录的计算图之外</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y = x * x</span><br><span class="line">u = y.detach()<span class="comment"># 把y当作一个常数，而不是关于x的函数</span></span><br><span class="line">z = u * x</span><br><span class="line"></span><br><span class="line">z.<span class="built_in">sum</span>().backward()<span class="comment"># z求导</span></span><br><span class="line">x.grad == u<span class="comment"># u*x的导数为u</span></span><br></pre></td></tr></table></figure><blockquote><p>tensor([True, True, True, True])</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.<span class="built_in">sum</span>().backward()</span><br><span class="line">x.grad == <span class="number">2</span> * x</span><br></pre></td></tr></table></figure><blockquote><p>tensor([True, True, True, True])</p></blockquote><p>即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">a</span>):</span><br><span class="line">    b = a * <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> b.norm() &lt; <span class="number">1000</span>:  </span><br><span class="line">        <span class="comment"># norm()是求欧几里得范数，欧几里得范数指得就是通常意义上的距离范数。</span></span><br><span class="line">        <span class="comment"># 例如在欧式空间里,它表示两点间的距离(向量x的模长)。</span></span><br><span class="line">        <span class="comment"># 即使函数经过了流程控制会产生不同的函数公式，每次计算的时候，pytorch都会把计算图存起来。</span></span><br><span class="line">        b = b * <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> b.<span class="built_in">sum</span>() &gt; <span class="number">0</span>:</span><br><span class="line">        c = b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        c = <span class="number">100</span> * b</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">a = torch.randn(size=(), requires_grad=<span class="literal">True</span>)<span class="comment"># size=空，表示是一个标量</span></span><br><span class="line">d = f(a)</span><br><span class="line">d.backward()</span><br><span class="line"></span><br><span class="line">a.grad == d / a</span><br></pre></td></tr></table></figure><blockquote><p>tensor(True)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识Pytorch(三) -- 完整的模型训练流程 + GPU调用训练</title>
      <link href="/20250603/"/>
      <url>/20250603/</url>
      
        <content type="html"><![CDATA[<h1 id="1-构建训练模型"><a href="#1-构建训练模型" class="headerlink" title="1 构建训练模型"></a>1 构建训练模型</h1><p>以CIFAR10数据集为例</p><h2 id="1-1-导入torch模块"><a href="#1-1-导入torch模块" class="headerlink" title="1.1 导入torch模块"></a>1.1 导入torch模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure><h2 id="1-2-准备数据集"><a href="#1-2-准备数据集" class="headerlink" title="1.2 准备数据集"></a>1.2 准备数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="1-3-查看数据集的大小"><a href="#1-3-查看数据集的大小" class="headerlink" title="1.3 查看数据集的大小"></a>1.3 查看数据集的大小</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据集的长度为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据集的长度为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(test_data_size))</span><br></pre></td></tr></table></figure><h2 id="1-4-加载数据集"><a href="#1-4-加载数据集" class="headerlink" title="1.4 加载数据集"></a>1.4 加载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用dataloader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><h2 id="1-5-搭建模型"><a href="#1-5-搭建模型" class="headerlink" title="1.5 搭建模型"></a>1.5 搭建模型</h2><p>以 CIFAR 10 的结构为例，神经网络模型结构如下</p><p><img src="/./20250603/image-20250604122341166.png" alt="image-20250604122341166"></p><p>创建model.py文件存放自定义模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># model.py文件</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModel, self).__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span> * <span class="number">4</span> * <span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.model(x)</span><br></pre></td></tr></table></figure><h2 id="1-6-训练参数设置"><a href="#1-6-训练参数设置" class="headerlink" title="1.6 训练参数设置"></a>1.6 训练参数设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">td = TdModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">lose_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line"><span class="comment"># learning_rate = 0.01    # 学习率</span></span><br><span class="line"><span class="comment"># 1e-2 = 1*(10)^(-2) = 1/100 = 0.01</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span>    <span class="comment"># 学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(td.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练的轮次</span></span><br><span class="line">epoch = <span class="number">10</span></span><br></pre></td></tr></table></figure><h2 id="1-7-模型训练与测试"><a href="#1-7-模型训练与测试" class="headerlink" title="1.7 模型训练与测试"></a>1.7 模型训练与测试</h2><p>使用<strong>tensorboard</strong>记录训练过程，查看模型是否训练达到自己的需求：</p><p>每次训练完一轮后，在测试数据集上跑一遍，用测试数据集的<strong>损失或正确率</strong>来评估模型有没有训练好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练内容</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------------------第&#123;&#125;轮训练开始--------------------&#x27;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = td(imgs)</span><br><span class="line">        loss = lose_fn(outputs, targets)    <span class="comment"># 损失值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 优化器模型</span></span><br><span class="line">        optimizer.zero_grad()               <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward()                     <span class="comment"># 计算每个权重的梯度</span></span><br><span class="line">        optimizer.step()                    <span class="comment"># 进行权重优化</span></span><br><span class="line">        </span><br><span class="line">        total_train_step += <span class="number">1</span>               <span class="comment"># 训练次数统计</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:     <span class="comment"># 训练步骤逢百打印</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;0&#125;，Loss：&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试步骤开始</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():    <span class="comment"># 禁用梯度计算，不去track梯度</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = td(imgs)        <span class="comment"># 使用已训练的模型进行推理</span></span><br><span class="line">            loss = lose_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss.item()    <span class="comment"># loss为tensor数据类型，loss.item()为普通数字类型</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;整体测试集上的Loss：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step, )</span><br><span class="line">        total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        torch.save(td, <span class="string">&#x27;./td_model&#123;&#125;.pt&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;模型已保存&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="1-8-正确率-分类问题"><a href="#1-8-正确率-分类问题" class="headerlink" title="1.8 正确率(分类问题)"></a>1.8 正确率(分类问题)</h2><p>即便得到整体数据集上的Loss，也不能很好地说明模型在测试集上的效果</p><p>在分类问题中可以用<strong>正确率</strong>来表示模型是否优秀</p><p>对于目标检测、语义分割等操作，可以直接把得到的输出在tensorboard中显示</p><p><strong>正确率测试说明</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">outputs = torch.tensor(         <span class="comment"># 自定义三种类别，两个样本（一行为一个样本的数据）</span></span><br><span class="line">    [[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.8</span>],</span><br><span class="line">     [<span class="number">0.5</span>,<span class="number">0.05</span>,<span class="number">0.4</span>]]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(outputs.argmax(<span class="number">0</span>))    <span class="comment"># tensor([1, 0, 0])</span></span><br><span class="line"><span class="built_in">print</span>(outputs.argmax(<span class="number">1</span>))    <span class="comment"># tensor([2, 0])</span></span><br><span class="line"><span class="comment"># argmax(1)水平方向，对每一行操作，返回该行中最大值的索引</span></span><br><span class="line"><span class="comment"># argmax(0) 垂直方向，对每一列操作，返回该列中最大值的索引</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本验证</span></span><br><span class="line">preds = outputs.argmax(<span class="number">1</span>)           <span class="comment"># 找到每个样本中概率最大的索引</span></span><br><span class="line">targets = torch.tensor([<span class="number">2</span>,<span class="number">1</span>])       <span class="comment"># 只有两个样本，样本的标签对应只有两个！！！</span></span><br><span class="line">true_number = (preds == targets)    <span class="comment"># tensor([ True, False])</span></span><br><span class="line"><span class="built_in">print</span>(true_number.<span class="built_in">sum</span>())            <span class="comment"># 计算对应位置相等的个数</span></span><br><span class="line"><span class="comment"># tensor(1)</span></span><br></pre></td></tr></table></figure><p>将**[1.7代码优化](#1.7 模型训练与测试)**，计算测试集中整体的正确个数</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># 测试步骤开始</span><br><span class="line">total_test_loss = 0</span><br><span class="line">total_test_accuracy = 0</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in test_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = td(imgs)</span><br><span class="line">        loss = lose_fn(outputs, targets)</span><br><span class="line">        total_test_loss += loss.item()</span><br><span class="line">        </span><br><span class="line">    # 计算测试集上的正确率    </span><br><span class="line">        accuracy = (outputs.argmax(dim=1) == targets).sum()</span><br><span class="line">        total_test_accuracy += accuracy.item()</span><br><span class="line">    print(&#x27;整体测试集上的正确率：&#123;&#125;&#x27;.format(total_test_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(&#x27;test_accuracy&#x27;, total_test_accuracy/test_data_size, total_test_step)</span><br><span class="line">        </span><br><span class="line">    print(&#x27;整体测试集上的Loss：&#123;&#125;&#x27;.format(total_test_loss))</span><br><span class="line">    writer.add_scalar(&#x27;test_loss&#x27;, total_test_loss, total_test_step, )</span><br><span class="line">    total_test_step += 1</span><br><span class="line">    </span><br><span class="line">    # 保存每轮训练的结果</span><br><span class="line">    torch.save(td, &#x27;./td_model_&#123;&#125;.pth&#x27;.format(i))</span><br><span class="line">    print(&#x27;第&#123;&#125;次训练模型已保存&#x27;.format(i))</span><br></pre></td></tr></table></figure><h2 id="1-9-完整代码"><a href="#1-9-完整代码" class="headerlink" title="1.9 完整代码"></a>1.9 完整代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 数据集 length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;训练数据集的长度为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试数据集的长度为：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.准备dataloader加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.创建网络模型</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *    <span class="comment"># 网络模型TdModel()在model.py文件中</span></span><br><span class="line">td = TdModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.损失函数</span></span><br><span class="line">lose_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.优化器</span></span><br><span class="line"><span class="comment"># learning_rate = 0.01    # 学习率</span></span><br><span class="line"><span class="comment"># 1e-2 = 1*(10)^(-2) = 1/100 = 0.01</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span>    <span class="comment"># 学习率</span></span><br><span class="line">optimizer = torch.optim.SGD(td.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6.设置训练网络的一些参数</span></span><br><span class="line">total_train_step = <span class="number">0</span>    <span class="comment"># 记录训练的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span>     <span class="comment"># 记录测试的次数</span></span><br><span class="line">epoch = <span class="number">50</span>              <span class="comment"># 训练的轮次 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 7.开始训练内容，以每个epoch为一轮回，实现多次训练</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;--------------------第&#123;&#125;轮训练开始--------------------&#x27;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 8.训练步骤开始，模型网络进入训练状态</span></span><br><span class="line">    <span class="comment"># td.train()    # 这仅对某些模块有效，例如 Dropout、BatchNorm 等</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:    <span class="comment"># 从训练的dataloader中不断的取数据</span></span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = td(imgs)</span><br><span class="line">        loss = lose_fn(outputs, targets)    <span class="comment"># 计算损失值</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 9.优化器模型，计算出误差后放入优化器中进行优化</span></span><br><span class="line">        optimizer.zero_grad()               <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward()                     <span class="comment"># 计算每个权重的梯度</span></span><br><span class="line">        optimizer.step()                    <span class="comment"># 进行权重优化</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 10.展示输出</span></span><br><span class="line">        total_train_step += <span class="number">1</span>               <span class="comment"># 训练次数统计</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;训练次数：&#123;0&#125;，Loss：&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(total_train_step, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&#x27;train_loss&#x27;</span>, loss.item(), total_train_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 11.测试步骤开始，在 一轮结束 或 特定步数 后进行测试</span></span><br><span class="line">    <span class="comment"># td.eval()    # 这仅对某些模块有效。例如 Dropout、BatchNorm 等</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_test_accuracy = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():          <span class="comment"># 禁用梯度计算，不去track梯度</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader:</span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = td(imgs)</span><br><span class="line">            loss = lose_fn(outputs, targets)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># 计算测试集上的正确率    </span></span><br><span class="line">            accuracy = (outputs.argmax(dim=<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_test_accuracy += accuracy.item()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;整体测试集上的正确率：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_accuracy/test_data_size))</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;test_accuracy&#x27;</span>, total_test_accuracy/test_data_size, total_test_step)</span><br><span class="line">            </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;整体测试集上的Loss：&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">        writer.add_scalar(<span class="string">&#x27;test_loss&#x27;</span>, total_test_loss, total_test_step, )</span><br><span class="line">        total_test_step += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 保存每轮训练的结果</span></span><br><span class="line">        torch.save(td, <span class="string">&#x27;./td_model_&#123;&#125;.pth&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;第&#123;&#125;次训练模型已保存&#x27;</span>.<span class="built_in">format</span>(i))</span><br></pre></td></tr></table></figure><h1 id="2-使用-GPU-训练"><a href="#2-使用-GPU-训练" class="headerlink" title="2 使用 GPU 训练"></a>2 使用 GPU 训练</h1><h2 id="2-1-调用方式-1"><a href="#2-1-调用方式-1" class="headerlink" title="2.1 调用方式 1"></a>2.1 调用方式 1</h2><p>只有<strong>网络模型</strong>、<strong>损失函数</strong>、<strong>数据（输入、输出）</strong>可以设置为GPU运算</p><p>使用方式：在对象后添加 <strong>.cuda()</strong></p><p><strong>网络模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.创建网络模型</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *    <span class="comment"># 网络模型TdModel()在model.py文件中</span></span><br><span class="line">td = TdModel()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    td = td.cuda()</span><br></pre></td></tr></table></figure><p><strong>损失函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.损失函数</span></span><br><span class="line">lose_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    lose_fn = lose_fn.cuda()</span><br></pre></td></tr></table></figure><p><strong>输入、输出数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 8.训练步骤开始，模型网络进入训练状态</span></span><br><span class="line"><span class="comment"># td.train()    # 这仅对某些模块有效，例如 Dropout、BatchNorm 等</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:    <span class="comment"># 从训练的dataloader中不断的取数据</span></span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        imgs, targets = imgs.cuda(), targets.cuda()</span><br><span class="line">    outputs = td(imgs)</span><br><span class="line">    loss = lose_fn(outputs, targets)    <span class="comment"># 计算损失值</span></span><br></pre></td></tr></table></figure><h2 id="2-2-调用方式-2"><a href="#2-2-调用方式-2" class="headerlink" title="2.2 调用方式 2"></a>2.2 调用方式 2</h2><p>使用方式：在对象后添加 <code>.to(device)</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)    <span class="comment"># 指定运算设备为cpu</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)      <span class="comment"># 指定运算设备为gpu，默认为第一张显卡</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)    <span class="comment"># 指定运算设备为gpu，使用第一张显卡</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:1&#x27;</span>)    <span class="comment"># 指定运算设备为gpu，使用第二张显卡</span></span><br></pre></td></tr></table></figure><p><strong>指定设备</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定训练的设备</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>调用设备</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.创建网络模型</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *    <span class="comment"># 网络模型TdModel()在model.py文件中</span></span><br><span class="line">td = TdModel()</span><br><span class="line">td = td.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.损失函数</span></span><br><span class="line">lose_fn = nn.CrossEntropyLoss()</span><br><span class="line">lose_fn = lose_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从训练的dataloader中取数据</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:    </span><br><span class="line">    imgs, targets = data</span><br><span class="line">    imgs, targets = imgs.to(device), targets.to(device)</span><br></pre></td></tr></table></figure><h1 id="3-测试模型结果"><a href="#3-测试模型结果" class="headerlink" title="3 测试模型结果"></a>3 测试模型结果</h1><p>利用已经训练好的模型，给模型提供输入</p><p>从网络上下载一个图片，使用PIL读取图片</p><blockquote><p>因为png格式是四个通道，除了RGB三通道外，还有一个透明度通道</p><p>调用 image &#x3D; image.convert(‘RGB’) 保留图片的颜色通道</p><p>如果图片本来就是三个颜色通道，经过此操作后，无变化</p><p>加上这一步后可以适应png、jpg等格式的图片</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用PIL读取图片</span></span><br><span class="line">image_path = <span class="string">&#x27;images/dog.png&#x27;</span></span><br><span class="line">image = Image.<span class="built_in">open</span>(image_path)  </span><br><span class="line">image = image.convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(image)</span><br><span class="line"><span class="comment"># &lt;PIL.Image.Image image mode=RGB size=197x143 at 0x1991212BE20&gt;</span></span><br></pre></td></tr></table></figure><p>调整图像大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transform = torchvision.transforms.Compose([    <span class="comment"># 联立transform</span></span><br><span class="line">    torchvision.transforms.Resize((<span class="number">32</span>,<span class="number">32</span>)),     <span class="comment"># 对输入图像进行尺寸调整。</span></span><br><span class="line">    torchvision.transforms.ToTensor(),</span><br><span class="line">])</span><br><span class="line">image = transform(image)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br></pre></td></tr></table></figure><p>加载网络模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载网络模型</span></span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line">model = torch.load(<span class="string">&quot;td_model_0.pth&quot;</span>,map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>), weights_only=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br><span class="line"><span class="comment"># 注意此时引用的模型文件是用gpu训练的，如果测试时使用的是cpu，则需要指定设备到cpu去运算</span></span><br></pre></td></tr></table></figure><p>输入图片，推理结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">image = torch.reshape(image, (<span class="number">1</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">model.<span class="built_in">eval</span>()            <span class="comment"># 用于将模型设置为评估模式。</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():   <span class="comment"># 禁用梯度计算（节省内存，加速推理）。</span></span><br><span class="line">    output = model(image)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line"><span class="built_in">print</span>(output.argmax(dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识Pytorch(二) -- 神经网络搭建</title>
      <link href="/20250602/"/>
      <url>/20250602/</url>
      
        <content type="html"><![CDATA[<h1 id="1-神经网络基本骨架"><a href="#1-神经网络基本骨架" class="headerlink" title="1 神经网络基本骨架"></a>1 神经网络基本骨架</h1><p>pytorch关于神经网络的工具主要在torch.nn中（Neural Network）</p><p>官网文档：<a href="https://docs.pytorch.org/docs/stable/nn.html">https://docs.pytorch.org/docs/stable/nn.html</a></p><ul><li><p>Containers# 主要给神经网络定义了一些骨架（结构），往结构中添加不同的内容就可以组成神经网络</p></li><li><p>Convolution Layers# 卷积层</p></li><li><p>Pooling layers# 池化层</p></li><li><p>Padding Layers# 填充层</p></li><li><p>Non-linear Activations (weighted sum, nonlinearity)# 非线性激活</p></li><li><p>Non-linear Activations (other)</p></li><li><p>Normalization Layers# 标准化（归一化）层</p></li><li><p>…</p></li></ul><h2 id="1-1-骨架-Containers"><a href="#1-1-骨架-Containers" class="headerlink" title="1.1 骨架 Containers"></a>1.1 骨架 Containers</h2><p>Containers包含6个模块，<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a>是最常用的模块，给<strong>所有</strong>神经网络提供一个基本的骨架</p><table><thead><tr><th><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module">Module</a></th><th>Base class for all neural network modules.</th></tr></thead><tbody><tr><td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential">Sequential</a></td><td>A sequential container.</td></tr><tr><td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleList.html#torch.nn.ModuleList">ModuleList</a></td><td>Holds submodules in a list.</td></tr><tr><td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ModuleDict.html#torch.nn.ModuleDict">ModuleDict</a></td><td>Holds submodules in a dictionary.</td></tr><tr><td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ParameterList.html#torch.nn.ParameterList">ParameterList</a></td><td>Holds parameters in a list.</td></tr><tr><td><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ParameterDict.html#torch.nn.ParameterDict">ParameterDict</a></td><td>Holds parameters in a dictionary.</td></tr></tbody></table><h3 id="1-1-1-Module的使用"><a href="#1-1-1-Module的使用" class="headerlink" title="1.1.1 Module的使用"></a>1.1.1 Module的使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModel</span>(nn.Module):    <span class="comment"># 创建自定义模型，需要继承nn.Module</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()   <span class="comment"># 调用父类的初始化函数</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):       <span class="comment"># 前向传播</span></span><br><span class="line">        output = x + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">td = TdModel()    <span class="comment"># 实例化模型对象</span></span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)     <span class="comment"># 设定输入值</span></span><br><span class="line">output = td(x)            <span class="comment"># 调用模型运算</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="1-1-2-模型搭建-Sequential"><a href="#1-1-2-模型搭建-Sequential" class="headerlink" title="1.1.2 模型搭建 Sequential"></a>1.1.2 模型搭建 Sequential</h3><p>按照顺序执行pytorch模型中的各项功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using Sequential to create a small model. When `model` is run,</span></span><br><span class="line"><span class="comment"># input will first be passed to `Conv2d(1,20,5)`. The output of</span></span><br><span class="line"><span class="comment"># `Conv2d(1,20,5)` will be used as the input to the first</span></span><br><span class="line"><span class="comment"># `ReLU`; the output of the first `ReLU` will become the input</span></span><br><span class="line"><span class="comment"># for `Conv2d(20,64,5)`. Finally, the output of</span></span><br><span class="line"><span class="comment"># `Conv2d(20,64,5)` will be used as input to the second `ReLU`</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using Sequential with OrderedDict. This is functionally the</span></span><br><span class="line"><span class="comment"># same as the above code</span></span><br><span class="line">model = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">          (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">          (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br></pre></td></tr></table></figure><p>以 CIFAR 10 的结构为例</p><p><img src="/./20250602/image-20250604103242875.png" alt="image-20250604103242875"></p><p><strong>不使用 Sequential</strong> ，神经网络模型结构过程</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        self.maxpool3 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.flatten = nn.Flatten()</span><br><span class="line">        self.linear1 = nn.Linear(<span class="number">1024</span>, <span class="number">64</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.maxpool1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.maxpool2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.maxpool3(x)</span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.linear1(x)</span><br><span class="line">        x = self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">td = TdModule()</span><br><span class="line"><span class="built_in">print</span>(td)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出结果查看</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)    <span class="comment"># 测试数据</span></span><br><span class="line">output = td(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p><strong>使用 Sequential</strong> 组建神经网络模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()</span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">td = TdModule()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型输出结果查看</span></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)    <span class="comment"># 测试数据</span></span><br><span class="line">output = td(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output.size())</span><br></pre></td></tr></table></figure><p>使用 <strong>tensotboard 可视化</strong>模型架构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">writer.add_graph(TdModule(), <span class="built_in">input</span>)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="1-2-卷积层-Convolution-Layers"><a href="#1-2-卷积层-Convolution-Layers" class="headerlink" title="1.2 卷积层 Convolution Layers"></a>1.2 卷积层 Convolution Layers</h2><p><strong>torch.nn</strong>是对<strong>torch.nn.functional</strong>的一个封装</p><p><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">nn.Conv1d</a>表示一维，<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">nn.Conv2d</a>表示二维（图片是2D），<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d">nn.Conv3d</a>表示三维</p><p>卷积操作可视化：<a href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">https://github.com/vdumoulin/conv_arithmetic&#x2F;blob&#x2F;master&#x2F;README.md</a></p><h3 id="1-2-1-nn-functional-conv2d"><a href="#1-2-1-nn-functional-conv2d" class="headerlink" title="1.2.1 nn.functional.conv2d"></a>1.2.1 nn.functional.conv2d</h3><p><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d">torch.nn.functional.conv2d</a><strong>用法</strong>，注意数据要使用tensor格式！！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.functional.conv2d(</span><br><span class="line">    <span class="built_in">input</span>, weight, bias=<span class="literal">None</span>, stride=<span class="number">1</span>, </span><br><span class="line">    padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span></span><br><span class="line">) </span><br></pre></td></tr></table></figure><ul><li><strong>input</strong>输入，形状要求 (minibatch,in_channels,iH,iW)</li><li><strong>weight</strong>权重 &#x2F; 卷积核，形状要求 (out_channels,in_channelsgroups,kH,kW)</li><li><strong>bias</strong>偏置，形状要求 (out_channels)(out_channels)，默认值：None</li><li><strong>stride</strong>步长，卷积核每次移动步长，2d卷积中要求长宽，可以是单个数字或一个元组 (sH, sW)，默认是1</li><li><strong>padding</strong>填充，在输入图像的左右两边进行填充，padding决定填充多大范围，可以是单个数字或一个元组 (padH, padW)，默认是0（不填充）<ul><li>如果不填充白边，那么边缘的图像特征就会缺失</li></ul></li></ul><p><strong>示例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor(       <span class="comment"># 输入图像</span></span><br><span class="line">    [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">     [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]]</span><br><span class="line">)</span><br><span class="line">kernel = torch.tensor(      <span class="comment"># 卷积核，就是权重weight</span></span><br><span class="line">    [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>]]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>))     <span class="comment"># minibatch为1，1通道，5H*5W</span></span><br><span class="line">kernel = torch.reshape(kernel, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">1</span>)    <span class="comment"># 步长为1</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br><span class="line">output = F.conv2d(<span class="built_in">input</span>, kernel, stride=<span class="number">2</span>)    <span class="comment"># 步长为2</span></span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="1-2-2-nn-Conv2d"><a href="#1-2-2-nn-Conv2d" class="headerlink" title="1.2.2 nn.Conv2d"></a>1.2.2 nn.Conv2d</h3><p><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">nn.Conv2d</a>用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, <span class="comment"># 常用的五个参数</span></span><br><span class="line">    dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>, padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">    device=<span class="literal">None</span>, dtype=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><strong>in_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – 输入图像中的通道数</li><li><strong>out_channels</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – 卷积后输出的通道数</li><li><strong>kernel_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>) – 卷积核的大小，一个数字时为<strong>n*n</strong>大小的卷积核，不规则核设置为 <strong>(行, 宽)</strong></li><li><strong>stride</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>, optional</em>) – 卷积的步长，2d卷积中要求长宽，可以是单个数字或一个元组 (sH, sW)，默认是1</li><li><strong>padding</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>, optional</em>) – 在输入的四个边上添加填充，padding决定填充多大范围，默认是0（不填充）</li><li><strong>padding_mode</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>, optional</em>) –选择padding填充的时候，按什么模式进行填充 ‘zeros’, ‘reflect’, ‘replicate’ or ‘circular’. 默认: ‘zeros’</li><li><strong>dilation</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>, optional</em>) – 内核元素之间的间距，默认是1</li><li><strong>groups</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>, optional</em>) – 从输入通道到输出通道的阻塞连接数，默认是1</li><li><strong>bias</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>, optional</em>) – 为True时向输出添加可学习的偏差，默认True</li></ul><p><strong>代码实例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):    <span class="comment"># 自定义模型</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()    <span class="comment"># 完成父类初始化</span></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">td = TdModule()</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;./logs&#x27;</span>)    <span class="comment"># 使用tensorboard可视化模型结果</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = td(imgs)</span><br><span class="line">    <span class="comment"># print(imgs.shape)   # torch.Size([64, 3, 32, 32])</span></span><br><span class="line">    <span class="comment"># print(output.shape) # torch.Size([64, 6, 30, 30])</span></span><br><span class="line">    writer.add_image(<span class="string">&#x27;input&#x27;</span>, imgs, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)   <span class="comment"># 输入图像</span></span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))                      <span class="comment"># 输出是6通道，修改为3通道进行显示</span></span><br><span class="line">    writer.add_image(<span class="string">&#x27;output&#x27;</span>, output, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)</span><br><span class="line">    step += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>卷积层说明，如下图所示。</p><p>输入图像是 224 x 224 x 3，经过一个卷积和非线性激活后，变成 224 x 224 x 64</p><p>输出层的高宽计算公式见pytorch官方文档：<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d</a></p><p><img src="/./20250602/image-20250604103633371.png" alt="image-20250604103633371"></p><h2 id="1-3-池化层-Pooling-layers"><a href="#1-3-池化层-Pooling-layers" class="headerlink" title="1.3 池化层 Pooling layers"></a>1.3 池化层 Pooling layers</h2><p>最大池化（MaxPool）也称为下采样，上采样（MaxUnpool），平均池化（AvgPool），自适应的最大池化（AdaptiveMaxPool）</p><h3 id="1-3-1-nn-MaxPool2d"><a href="#1-3-1-nn-MaxPool2d" class="headerlink" title="1.3.1 nn.MaxPool2d"></a>1.3.1 nn.MaxPool2d</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MaxPool2d(</span><br><span class="line">    kernel_size, stride=<span class="literal">None</span>, padding=<span class="number">0</span>, </span><br><span class="line">    dilation=<span class="number">1</span>, return_indices=<span class="literal">False</span>, ceil_mode=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>kernel_size (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><em>Union</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>]]</em>) – 设置用来取最大值的窗口尺寸，类似卷积核，设置为3的时候会生成一个3*3的窗口</li><li>stride (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><em>Union</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>]]</em>) – 窗口的步长，默认为kernel_size</li><li>padding (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><em>Union</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>]]</em>) – 在两侧添加填充</li><li>dilation (<a href="https://docs.python.org/3/library/typing.html#typing.Union"><em>Union</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>[*<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a></em>,* <a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>]]</em>) – 窗口中各元素之间的步长</li><li>return_indices (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a>) – 为True时返回最大索引和输出，用于 <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d">torch.nn.MaxUnpool2d</a> 之后</li><li>ceil_mode (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a>) – 为True时使用 ceil <strong>而不是 floor</strong> 来计算输出形状，ceil对缺失信息部分也进行上采样，floor会对缺失信息部分进行舍弃<ul><li>floor向下取整，ceil向上取整，如下图所示</li><li><img src="./20250602/image-20250604103717552.png" alt="image-20250604103717552" style="zoom:60%;" /></li></ul></li></ul><p><strong>实例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor(</span><br><span class="line">    [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">     [<span class="number">5</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">     [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]], dtype=torch.float32</span><br><span class="line">)</span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))     <span class="comment"># 变形为[minibatchsize，通道数，长，宽]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModel, self).__init__()</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">        </span><br><span class="line">td = TdModel()</span><br><span class="line">output = td(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="1-3-2-最大池化作用"><a href="#1-3-2-最大池化作用" class="headerlink" title="1.3.2 最大池化作用"></a>1.3.2 最大池化作用</h3><p>最大池化的目的是保留输入的特征，同时把数据量减小</p><p>通过tensorboard可视化最大池化效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                                       transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.maxpool1 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.maxpool1(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">td = Net()</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_image(<span class="string">&#x27;input&#x27;</span>, imgs, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)</span><br><span class="line">    outputs = td(imgs)</span><br><span class="line">    writer.add_image(<span class="string">&#x27;outputs&#x27;</span>, outputs, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="1-4-非线性激活-Non-linear-Activations"><a href="#1-4-非线性激活-Non-linear-Activations" class="headerlink" title="1.4 非线性激活 Non-linear Activations"></a>1.4 非线性激活 Non-linear Activations</h2><p>非线性激活主要为了给神经网络引入非线性的特征，常见的有：</p><ul><li><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">ReLU</a>函数，输入大于0时输出原值，小于0 时输出0</li><li><a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid">Sigmoid</a>函数，通过公式计算，输出结果在 <strong>(0,1)</strong> 范围内，常用于二分类问题</li></ul><h3 id="1-4-1-ReLU函数"><a href="#1-4-1-ReLU函数" class="headerlink" title="1.4.1 ReLU函数"></a>1.4.1 ReLU函数</h3><p>pytorch文档地址：<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU">https://docs.pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ReLU(inplace=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"> <span class="built_in">input</span> = -<span class="number">1</span></span><br><span class="line"> ReLU(<span class="built_in">input</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"> <span class="comment"># input = 0</span></span><br><span class="line"> </span><br><span class="line">output = ReLU(<span class="built_in">input</span>, inplace=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># input = -1</span></span><br><span class="line"><span class="comment"># output = 0</span></span><br></pre></td></tr></table></figure><ul><li>inplace为True时，会将输入内存地址的数据直接改变，就是原地操作，无需返回值</li><li>inplace为False时，保留原始数据，需要采用返回值的形式接收改变后的数据</li></ul><p><strong>实例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.tensor(</span><br><span class="line">    [[<span class="number">1</span>, -<span class="number">0.5</span>],</span><br><span class="line">     [-<span class="number">1</span>, <span class="number">3</span>]]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 输入添加上batchsize参数，图像是1维的，高宽都是2</span></span><br><span class="line"><span class="built_in">input</span> = torch.reshape(<span class="built_in">input</span>, (-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>))    </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.relu1(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">Td = TdModule()</span><br><span class="line">output = Td(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(output)</span><br></pre></td></tr></table></figure><h3 id="1-4-2-Sigmoid函数"><a href="#1-4-2-Sigmoid函数" class="headerlink" title="1.4.2 Sigmoid函数"></a>1.4.2 Sigmoid函数</h3><p>pytorch文档地址：<a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid">https://docs.pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Sigmoid(*args, **kwargs)</span><br></pre></td></tr></table></figure><p><strong>使用tensorboard可视化结果</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor())</span><br><span class="line">dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.sigmoid1 = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        output = self.sigmoid1(x)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">Td = TdModule()</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_image(<span class="string">&#x27;input&#x27;</span>, imgs, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)</span><br><span class="line">    outputs = Td(imgs)</span><br><span class="line">    writer.add_image(<span class="string">&#x27;outputs&#x27;</span>, outputs, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="1-5-损失函数-Loss-Functions"><a href="#1-5-损失函数-Loss-Functions" class="headerlink" title="1.5 损失函数 Loss Functions"></a>1.5 损失函数 Loss Functions</h2><ul><li>计算实际输出和目标之间的差距</li><li>为更新输出提供一定的依据（反向传播）</li></ul><p>pytorch文档地址：<a href="https://docs.pytorch.org/docs/stable/nn.html#loss-functions">https://docs.pytorch.org/docs/stable/nn.html#loss-functions</a></p><h3 id="1-5-1-L1Loss函数"><a href="#1-5-1-L1Loss函数" class="headerlink" title="1.5.1 L1Loss函数"></a>1.5.1 <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss"><strong>L1Loss</strong></a>函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.L1Loss(</span><br><span class="line">    size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li>size_average (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>, optional</em>) – 已弃用（请参阅reduction）。默认情况下，损失按批次中的每个损失元素进行平均。请注意，对于某些损失，每个样本都有多个元素。如果字段size_maverage设置为False，则将每个小批量的损失相加。当reduce为False时忽略。默认值：True</li><li>reduce (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>, optional</em>) – 已弃用（参见reduce）。默认情况下，根据size_maverage，对每个小批量的观测值进行平均或求和。当reduce为False时，返回每个批处理元素的损失，并忽略size_maverage。默认值：True</li><li>reduction (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>, optional</em>) – 指定要应用于输出的缩减：’none’|’表示’|’和’。’none’：不应用缩减，’mean’：输出的总和将除以输出中的元素数量，’sum’：输出将被求和。注意：size_maverage和reduce正在被弃用，同时，指定这两个参数中的任何一个都将覆盖reduce。默认值：’mean’</li></ul><p><strong>实例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">inputs = torch.reshape(inputs, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))   <span class="comment"># 变成batchsize，1通道，1行3列</span></span><br><span class="line">targets = torch.reshape(targets, (<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss = nn.L1Loss()</span><br><span class="line">result = loss(inputs, targets)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><h3 id="1-5-2-MSELoss函数"><a href="#1-5-2-MSELoss函数" class="headerlink" title="1.5.2 MSELoss函数"></a>1.5.2 <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss">MSELoss</a>函数</h3><p>计算平方差函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.MSELoss(</span><br><span class="line">    size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>实例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">inputs = torch.reshape(inputs, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))  <span class="comment"># 变成batchsize，1通道，1行3列</span></span><br><span class="line">targets = torch.reshape(targets, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs, targets)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br></pre></td></tr></table></figure><h3 id="1-5-3-CrossEntropyLoss函数"><a href="#1-5-3-CrossEntropyLoss函数" class="headerlink" title="1.5.3 CrossEntropyLoss函数"></a>1.5.3 <a href="https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss">CrossEntropyLoss</a>函数</h3><p>交叉熵函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.CrossEntropyLoss(</span><br><span class="line">    weight=<span class="literal">None</span>, size_average=<span class="literal">None</span>, ignore_index=-<span class="number">100</span>, </span><br><span class="line">    reduce=<span class="literal">None</span>, reduction=<span class="string">&#x27;mean&#x27;</span>, label_smoothing=<span class="number">0.0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>实例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>], dtype=torch.<span class="built_in">float</span>)    <span class="comment"># x有3类</span></span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x, (<span class="number">1</span>,<span class="number">3</span>))     <span class="comment"># 变为 batchsize，类别数</span></span><br><span class="line"></span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x, y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure><h1 id="2-反向传播-backward"><a href="#2-反向传播-backward" class="headerlink" title="2 反向传播 backward"></a>2 反向传播 backward</h1><p>使用 <strong>CrossEntropyLoss函数</strong> 实现反向传播计算</p><p>神经网络中每个节点（要更新的参数）都有一个梯度，根据梯度对参数进行优化，最终实现降低loss的目的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()</span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">td = TdModule()</span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = td(imgs)</span><br><span class="line">    <span class="comment"># print(outputs)      # 输出有10个数值，每一个代表每个类的概率</span></span><br><span class="line">    <span class="comment"># print(targets)      # 图片的类别编号</span></span><br><span class="line">    result_loss = loss(outputs, targets)    <span class="comment"># 交叉熵函数</span></span><br><span class="line">    <span class="built_in">print</span>(result_loss)</span><br><span class="line">    result_loss.backward()     <span class="comment"># 开启计算梯度</span></span><br></pre></td></tr></table></figure><p><code>.backward()</code> 用来计算梯度，需要使用合适的<strong>优化器</strong>去更新参数，以达到整体的误差降低的目的</p><h1 id="3-优化器-optim"><a href="#3-优化器-optim" class="headerlink" title="3 优化器 optim"></a><strong>3 优化器 optim</strong></h1><p>pytorch文档地址：<a href="https://docs.pytorch.org/docs/stable/optim.html">https://docs.pytorch.org/docs/stable/optim.html</a></p><ul><li>params模型的参数</li><li>lr   学习速率 learning rate</li></ul><p><strong>构造优化器</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># optimizer = optim.Adam([var1, var2], lr=0.0001)</span></span><br></pre></td></tr></table></figure><p>首先在优化器中放入模型的参数，优化步骤如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> dataset:</span><br><span class="line">    optimizer.zero_grad()          <span class="comment"># 梯度清零</span></span><br><span class="line">    output = model(<span class="built_in">input</span>)          </span><br><span class="line">    loss = loss_fn(output, target) <span class="comment"># 计算输出与真实值的误差</span></span><br><span class="line">    loss.backward()                <span class="comment"># 得到每个要更新参数的梯度</span></span><br><span class="line">    optimizer.step()               <span class="comment"># 每个参数都根据上步的梯度进行优化</span></span><br></pre></td></tr></table></figure><p><strong>实例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建自定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TdModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(TdModule, self).__init__()</span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入数据集</span></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化模型，.to(&#x27;cuda&#x27;)将数据移至GPU计算，如果没有GPU，则</span></span><br><span class="line">td = TdModule().to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line">loss = nn.CrossEntropyLoss().to(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用随机梯度下降优化器 SGD</span></span><br><span class="line">optim = torch.optim.SGD(td.parameters(), lr=<span class="number">0.01</span>)   <span class="comment"># 定义优化器</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = td(imgs.to(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">        <span class="comment"># print(outputs)      # 输出有10个数值，每一个代表每个类的概率</span></span><br><span class="line">        <span class="comment"># print(targets)      # 图片的类别编号</span></span><br><span class="line">        result_loss = loss(outputs, targets.to(<span class="string">&#x27;cuda&#x27;</span>))  <span class="comment"># 交叉熵函数</span></span><br><span class="line">        optim.zero_grad()       <span class="comment"># 梯度清零，上一次的梯度对这一次的梯度更新没有用</span></span><br><span class="line">        result_loss.backward()  <span class="comment"># 得到每个可以调节参数对应的梯度</span></span><br><span class="line">        optim.step()            <span class="comment"># 对每个参数进行调优</span></span><br><span class="line">        running_loss = running_loss + result_loss   <span class="comment"># 整体误差求和</span></span><br><span class="line">    <span class="built_in">print</span>(running_loss)</span><br></pre></td></tr></table></figure><h1 id="4-现有网络模型"><a href="#4-现有网络模型" class="headerlink" title="4 现有网络模型"></a>4 现有网络模型</h1><p>pytorch文档地址：<a href="https://docs.pytorch.org/vision/stable/models.html">https://docs.pytorch.org/vision/stable/models.html</a></p><p>torchvision是关于<strong>图像</strong>相关的模型，torchaudio是关于<strong>语音</strong>相关的模型，torchtext是关于<strong>文字</strong>相关的模型…</p><h2 id="4-1-VGG分类模型"><a href="#4-1-VGG分类模型" class="headerlink" title="4.1 VGG分类模型"></a>4.1 VGG分类模型</h2><p>常用的有VGG16、VGG19</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torchvision.models.vgg16(</span><br><span class="line">    *, weights: <span class="type">Optional</span>[VGG16_Weights] = <span class="literal">None</span>, </span><br><span class="line">    progress: <span class="built_in">bool</span> = <span class="literal">True</span>, **kwargs: <span class="type">Any</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><code>weights</code> (<a href="https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights">VGG16_Weights</a>, optional) – 使用已经下载好的的预训练权重，参阅 <a href="https://docs.pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.VGG16_Weights">VGG16_Weights</a> 。默认情况下，不使用预先训练的权重。</li><li><code>progress</code> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>, optional</em>) – 如果为True，则显示下载到stderr的进度条。默认值为True。</li><li><code>**kwargs</code> – 传递给 <code>torchvision.models.vgg.VGG</code> 的参数。 更多详细信息参阅 <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py">source code</a> 。</li></ul><h2 id="4-2-查看VGG模型架构"><a href="#4-2-查看VGG模型架构" class="headerlink" title="4.2 查看VGG模型架构"></a>4.2 查看VGG模型架构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"> </span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)    <span class="comment"># 不使用预训练模型</span></span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>)      <span class="comment"># 使用预训练模型</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(vgg16_true)   <span class="comment"># 查看与训练模型的架构</span></span><br></pre></td></tr></table></figure><p><strong>VGG 模型架构</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">VGG(</span><br><span class="line">  (features): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">3</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">5</span>): Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">6</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">7</span>): Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">8</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">9</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">10</span>): Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">11</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">12</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">13</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">14</span>): Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">15</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">16</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">17</span>): Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">18</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">19</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">20</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">21</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">22</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">23</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">24</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">25</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">26</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">27</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">28</span>): Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=(<span class="number">3</span>, <span class="number">3</span>), stride=(<span class="number">1</span>, <span class="number">1</span>), padding=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    (<span class="number">29</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">30</span>): MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, ceil_mode=<span class="literal">False</span>)</span><br><span class="line">  )</span><br><span class="line">  (avgpool): AdaptiveAvgPool2d(output_size=(<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">  (classifier): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">25088</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">4096</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">5</span>): Dropout(p=<span class="number">0.5</span>, inplace=<span class="literal">False</span>)</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">4096</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="4-3-修改网络模型"><a href="#4-3-修改网络模型" class="headerlink" title="4.3 修改网络模型"></a><strong>4.3 修改网络模型</strong></h2><blockquote><p>VGG模型通过ImageNet数据集训练，最终分类类别有1000个</p><p>CIFAR10数据集中只有10各类别的数据</p><p>如何利用现有的网络，去改动它的结构</p></blockquote><p>在网络最后<strong>添加一个层级</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在模型末尾添加层级</span></span><br><span class="line">vgg16_true.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在classifier模块最后添加一个层级</span></span><br><span class="line">vgg16_true.classifier.add_module(<span class="string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="number">1000</span>, <span class="number">10</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure><p>修改某个层级参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整classifier模块中第6层参数</span></span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure><h1 id="5-模型保存与加载"><a href="#5-模型保存与加载" class="headerlink" title="5 模型保存与加载"></a>5 模型保存与加载</h1><h2 id="5-1-保存模型结构-参数"><a href="#5-1-保存模型结构-参数" class="headerlink" title="5.1 保存模型结构(参数)"></a>5.1 保存模型结构(参数)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">vgg16 = torchvision.models.vgg16(weights=<span class="literal">None</span>)      <span class="comment"># 初始化模型</span></span><br></pre></td></tr></table></figure><p><strong>保存方式1，保存模型结构 + 模型参数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(vgg16, <span class="string">&#x27;vgg16_method1.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>保存方式2，把模型的参数保存成字典（官方推荐）</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(vgg16.state_dict(), <span class="string">&#x27;vgg16_method2.pth&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="5-2-模型加载"><a href="#5-2-模型加载" class="headerlink" title="5.2 模型加载"></a>5.2 模型加载</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><p><strong>方式1 –&gt; 保存方式1，加载模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = torch.load(<span class="string">&#x27;vgg16_method1.pth&#x27;</span>, weights_only=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure><p><strong>方式2 –&gt; 保存方式2，加载模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vgg16 = torchvision.models.vgg16(weights=<span class="literal">None</span>)</span><br><span class="line">vgg16.load_state_dict(torch.load(<span class="string">&#x27;vgg16_method2.pth&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br><span class="line">                      </span><br><span class="line"><span class="comment"># model = torch.load(&#x27;vgg16_method2.pth&#x27;)   # 加载模型权重，读取出来字典形式</span></span><br><span class="line"><span class="comment"># print(model)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识Pytorch(一) -- Transforms笔记</title>
      <link href="/20250601/"/>
      <url>/20250601/</url>
      
        <content type="html"><![CDATA[<p>测试torch是否安装成功</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.is_available()</span><br><span class="line"><span class="comment"># True    # 若安装成功，则会打印true</span></span><br></pre></td></tr></table></figure><p>蚂蚁蜜蜂分类数据集和下载连接：<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip">https://download.pytorch.org/tutorial/hymenoptera_data.zip</a></p><h1 id="1-函数功能查看"><a href="#1-函数功能查看" class="headerlink" title="1 函数功能查看"></a>1 函数功能查看</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span>(torch.cuda)打开，看见</span><br><span class="line"><span class="built_in">help</span>(函数名不带括号)说明书</span><br></pre></td></tr></table></figure><p><strong>python中<code>__call__</code>的用法</strong></p><ul><li>在类中双下划线表示是类的内置函数</li><li>有<code>__call__</code>的情况下，调用对象时，可以直接传参到call中</li><li>没有<code>__call__</code>的情况下，调用对象需要加上“<code>.</code>”来调用其中的方法</li></ul><p><strong>python中__getitem__的用法</strong></p><ul><li>在进行索引取值时自动调用，可以查看原对象中<code>__getitem__</code>的<strong>return值</strong></li></ul><p><strong>不知道返回值时</strong></p><ul><li><code>print()</code></li><li><code>print(type())</code></li><li>打断点 debug</li></ul><h1 id="2-torch数据加载（读取）"><a href="#2-torch数据加载（读取）" class="headerlink" title="2 torch数据加载（读取）"></a>2 torch数据加载（读取）</h1><p>pytorch官网文档地址：<a href="https://docs.pytorch.org/">https://docs.pytorch.org</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure><h2 id="2-1-查看Dataset说明"><a href="#2-1-查看Dataset说明" class="headerlink" title="2.1 查看Dataset说明"></a>2.1 查看Dataset说明</h2><p>说明文档查看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># help(Dataset)</span></span><br><span class="line">Dataset??</span><br><span class="line"><span class="comment"># 双?打印的结果更易读</span></span><br><span class="line"><span class="comment"># dataset     # 提供一种方式去获取数据及其label</span></span><br><span class="line">                <span class="comment"># 如何获取每一个数据及其label</span></span><br><span class="line">                <span class="comment"># 告诉我们总共有多少的数据</span></span><br><span class="line"><span class="comment"># dataloader    # 为后面的网络提供不同的数据形式</span></span><br></pre></td></tr></table></figure><h2 id="2-2-初始化类操作"><a href="#2-2-初始化类操作" class="headerlink" title="2.2 初始化类操作"></a>2.2 初始化类操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):     </span><br><span class="line">        <span class="comment"># 初始化，为整个class提供一个全局变量</span></span><br><span class="line">        self.root_dir = root_dir        <span class="comment"># 数据根目录</span></span><br><span class="line">        self.label_dir = label_dir      <span class="comment"># 数据标签目录</span></span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)     <span class="comment"># 拼接目录</span></span><br><span class="line">        self.img_path = os.listdir(self.path)       <span class="comment"># 读取目录中所有文件名</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># index作为一个编号</span></span><br><span class="line">        <span class="comment"># 首先获取图片地址的列表，通过index获取图片的存储地址</span></span><br><span class="line">        img_name = self.img_path[index]     <span class="comment"># 单张图片文件名</span></span><br><span class="line">        img_item_path = os.path.join(self.path, img_name)   <span class="comment"># 拼接文件路径</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)     <span class="comment"># 打开图片，用变量img加载</span></span><br><span class="line">        label = self.label_dir              <span class="comment"># 图片标签</span></span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)   <span class="comment"># 返回整个数据集有多少数据</span></span><br></pre></td></tr></table></figure><h2 id="2-3-查看数据"><a href="#2-3-查看数据" class="headerlink" title="2.3 查看数据"></a>2.3 查看数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">root_dir = <span class="string">&#x27;./dataset/train&#x27;</span></span><br><span class="line">ants_label_dir = <span class="string">&#x27;ants&#x27;</span></span><br><span class="line">bees_label_dir = <span class="string">&#x27;bees&#x27;</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)      <span class="comment"># 蚂蚁数据集</span></span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)     <span class="comment"># 蜜蜂数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 单个数据查看</span></span><br><span class="line">img, label = ants_dataset[<span class="number">10</span>]</span><br><span class="line">img, label = bees_dataset[<span class="number">10</span>]</span><br></pre></td></tr></table></figure><h1 id="3-图片数据读取"><a href="#3-图片数据读取" class="headerlink" title="3 图片数据读取"></a>3 图片数据读取</h1><p>将图片数据读取为其他格式：(<code>torch.Tensor</code>, <code>numpy.ndarray</code>, <code>string/blobname</code>)</p><h2 id="3-1-PIL读取图片"><a href="#3-1-PIL读取图片" class="headerlink" title="3.1 PIL读取图片"></a>3.1 PIL读取图片</h2><p>PIL读取图片后，得到的数据类型为<code>PIL.Image.Image</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)    <span class="comment"># img_path为图片路径</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img))</span><br></pre></td></tr></table></figure><h2 id="3-2-Numpy转换PIL格式"><a href="#3-2-Numpy转换PIL格式" class="headerlink" title="3.2 Numpy转换PIL格式"></a>3.2 Numpy转换PIL格式</h2><p>利用<code>numpy.array()</code>，对<strong>PIL图片</strong>进行转换</p><p>转换后的图片类型为<code>numpy.ndarray</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img_array = np.array(img)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img_array))</span><br></pre></td></tr></table></figure><h2 id="3-3-Opencv打开图片"><a href="#3-3-Opencv打开图片" class="headerlink" title="3.3 Opencv打开图片"></a>3.3 Opencv打开图片</h2><p>利用<strong>Opencv</strong>读取图片，获取numpy型图片数据</p><p>安装Opencv：<code>pip install opencv-python</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">cv_img = cv2.imread(img_path)    <span class="comment"># cv_img是ndarray类型</span></span><br></pre></td></tr></table></figure><h1 id="4-TensorBoard-使用"><a href="#4-TensorBoard-使用" class="headerlink" title="4 TensorBoard 使用"></a>4 TensorBoard 使用</h1><p>TensorBoard的安装：<code>pip install tensorboard</code></p><p>TensorBoard用来显示模型训练到xx步时，模型的output是什么样，如损失函数、训练结果 </p><p>启动命令：<code>tensorboard --logdir=logs --port=6007</code> </p><p>logdir相对路径logs，port手动设置为6007</p><h2 id="4-1-add-scalar-用法"><a href="#4-1-add-scalar-用法" class="headerlink" title="4.1 .add_scalar() 用法"></a>4.1 .add_scalar() 用法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)    <span class="comment"># 存储路径为当前目录下创建logs文件夹</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># .add_scalar()用法示例</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    writer.add_scalar(</span><br><span class="line">        tag=<span class="string">&#x27;Y=2x&#x27;</span>,         <span class="comment"># 图表的标题</span></span><br><span class="line">        scalar_value=<span class="number">2</span>*i,  <span class="comment"># 图表的纵轴，需要保存的数值</span></span><br><span class="line">        global_step=i,   <span class="comment"># 图表的横轴，训练的步数</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>在终端输入：<code>tensorboard --logdir=logs --port=6007</code> </p><p>打开网页查看训练数据</p><h2 id="4-2-add-image-用法"><a href="#4-2-add-image-用法" class="headerlink" title="4.2 .add_image() 用法"></a>4.2 .add_image() 用法</h2><p>从<strong>PIL</strong>到<strong>numpy</strong>，需要在<code>add_image()</code>中指定<strong>shape</strong>中每一个数字&#x2F;维表示的含义</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">image_path = <span class="string">&#x27;dataset/train/ants/0013035.jpg&#x27;</span></span><br><span class="line">img_PIL = Image.<span class="built_in">open</span>(image_path)</span><br><span class="line">img_array = np.array(img_PIL)    <span class="comment"># numpy.ndarray格式图片数据</span></span><br><span class="line"><span class="built_in">print</span>(img_array.shape)  <span class="comment"># 查看图像格式，通道数3在最后一位</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># .add_image()用法示例</span></span><br><span class="line">writer.add_image(</span><br><span class="line">    tag=<span class="string">&#x27;test&#x27;</span>,                 <span class="comment"># 图像标题</span></span><br><span class="line">    img_tensor=img_array,       <span class="comment"># 图像数据如 (torch.Tensor, numpy.ndarray, or string/blobname)</span></span><br><span class="line">    global_step=<span class="number">1</span>,              <span class="comment"># 训练步数</span></span><br><span class="line">    dataformats=<span class="string">&#x27;HWC&#x27;</span>           <span class="comment"># 图像格式（高、宽、通道数）</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h1 id="5-Transforms-使用"><a href="#5-Transforms-使用" class="headerlink" title="5 Transforms 使用"></a>5 Transforms 使用</h1><p>transform基本上是对图片进行变化，使用方式是把一些特定格式的图片，通过<strong>Transform</strong>工具输出为我们想要的结果。</p><p>具体使用方式直接查看<strong>transforms.py</strong>文件中相关方法的使用说明。</p><ol><li>使用某方法时，首先<strong>关注它的输入和输出</strong>类型</li><li>多看<strong>transforms.py</strong>文件内容</li><li>关注方法需要什么参数</li></ol><h2 id="5-1-Transform-的结构和用法"><a href="#5-1-Transform-的结构和用法" class="headerlink" title="5.1 Transform 的结构和用法"></a>5.1 Transform 的结构和用法</h2><p>使用pycharm左侧工具栏的<strong>结构</strong>功能查看<strong>transforms.py</strong>的结构</p><p>可以把<strong>transforms.py</strong>看成一个工具箱，里面有各种工具如<strong>totensor</strong>（把一些数据类型转化为tensor类型）、<strong>resize</strong>、、、</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br></pre></td></tr></table></figure><p>首先要搞懂tensor数据类型，通过Transforms.ToTensor去解决两个问题</p><ol><li>transforms该如何使用（python）</li><li>为什么需要Tensor数据类型<ul><li>a.tensor数据类型是包装了神经网络理论基础的参数</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">&#x27;dataset/train/ants/6240338_93729615ec.jpg&#x27;</span></span><br><span class="line"><span class="comment"># img = Image.open(img_path)    # 使用PIL读取图片</span></span><br><span class="line">img = cv2.imread(img_path)      <span class="comment"># 使用opencv读取图片</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转为tensor格式</span></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">tensor_img = tensor_trans(img)</span><br></pre></td></tr></table></figure><p><strong>tensor_img</strong>是一个<strong>tensor</strong>的数据类型，它的参数：</p><ul><li><code>_backward_hooks</code>：神经网络中的反向传播，根据结果对参数进行调整</li><li><code>_grad</code>：梯度</li><li><code>_grad_fn</code>：梯度的方法</li><li><code>data</code>：图片的具体数据</li></ul><h2 id="5-2-使用tensorboard记录"><a href="#5-2-使用tensorboard记录" class="headerlink" title="5.2 使用tensorboard记录"></a>5.2 使用tensorboard记录</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(</span><br><span class="line">    log_dir=<span class="string">&#x27;logs&#x27;</span>,      <span class="comment"># 日志存放的文件夹</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.add_image(</span><br><span class="line">    tag=<span class="string">&#x27;Tensor_img&#x27;</span>,      <span class="comment"># 日志显示的图片名称</span></span><br><span class="line">    img_tensor=tensor_img,  <span class="comment">#  (torch.Tensor, numpy.ndarray, or string/blobname)  </span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="5-3-常见的Transforms"><a href="#5-3-常见的Transforms" class="headerlink" title="5.3 常见的Transforms"></a>5.3 常见的Transforms</h2><p> 需关注<strong>transforms.py</strong>中各函数的</p><ol><li>输入——PIL  ——<code>Image.open()</code></li><li>输出——tensor    ——<code>ToTensor()</code></li><li>作用——narrays  ——<code>cv.imread()</code></li></ol><h3 id="5-3-1-Compose类"><a href="#5-3-1-Compose类" class="headerlink" title="5.3.1 Compose类"></a>5.3.1 Compose类</h3><p>把不同的<strong>transform</strong>结合在一起，比如有张图片要处理，经过compose类时，首先进行一个中心的裁剪，再转为tensor数据类型…</p><p><code>Compose()</code>中的参数是一个列表，列表内容是<strong>transforms类型</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Example:</span><br><span class="line">    &gt;&gt;&gt; transforms.Compose([</span><br><span class="line">    &gt;&gt;&gt;     transforms.CenterCrop(10),</span><br><span class="line">    &gt;&gt;&gt;     transforms.PILToTensor(),</span><br><span class="line">    &gt;&gt;&gt;     transforms.ConvertImageDtype(torch.float),</span><br><span class="line">    &gt;&gt;&gt; ])</span><br></pre></td></tr></table></figure><h3 id="5-3-2-ToTensor类"><a href="#5-3-2-ToTensor类" class="headerlink" title="5.3.2 ToTensor类"></a>5.3.2 ToTensor类</h3><p>输入必须时<strong>PIL Image</strong> 或 <strong>ndarray</strong> 三通道图片</p><p>输出为<strong>Tensor</strong>数据类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&#x27;images/IMG_202408121501_3840x2161.png&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ToTensor</span></span><br><span class="line">trans_totensor = transforms.ToTensor()</span><br><span class="line">img_tensor = trans_totensor(img)        <span class="comment"># 转为tensor格式</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;ToTensor&#x27;</span>, img_tensor)    <span class="comment"># 调用tensorboard保存日志</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h3 id="5-3-3-Normalize类"><a href="#5-3-3-Normalize类" class="headerlink" title="5.3.3 Normalize类"></a>5.3.3 Normalize类</h3><p>归一化一个 tensor 类型的 image，根据它的<strong>均值</strong>和<strong>标准差</strong></p><p><code>数据 = (输入 - 均值) / 标准差</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(img_tensor[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">trans_norm = transforms.Normalize(mean=[<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], std=[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])    </span><br><span class="line"><span class="comment"># mean是均值，std是标准差</span></span><br><span class="line"></span><br><span class="line">img_norm = trans_norm(img_tensor)</span><br><span class="line"><span class="built_in">print</span>(img_norm[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">writer.add_image(<span class="string">&#x27;Normalize&#x27;</span>, img_norm,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="5-3-4-Resize类"><a href="#5-3-4-Resize类" class="headerlink" title="5.3.4 Resize类"></a>5.3.4 Resize类</h3><p>给定尺寸进行缩放，如果只给定了一个数，Resize就会根据最小的边去等比缩放</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Resize</span></span><br><span class="line"><span class="built_in">print</span>(img.size)</span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># img PIL -&gt; resize -&gt; img_resize PIL</span></span><br><span class="line">img_resize = trans_resize(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># img_resize PIL -&gt; totensor -&gt; img_resize tensor</span></span><br><span class="line">img_resize = trans_totensor(img_resize)     <span class="comment"># PIL转为tensor格式</span></span><br><span class="line"><span class="comment"># print(img_resize)</span></span><br><span class="line">writer.add_image(<span class="string">&#x27;Resize&#x27;</span>, img_resize, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="5-3-5-Compose-Resize"><a href="#5-3-5-Compose-Resize" class="headerlink" title="5.3.5 Compose - Resize"></a>5.3.5 Compose - Resize</h3><p>应用Compose 将Resize步骤整合，并使用tensorboard记录日志</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compose - Resize</span></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 trans_resize_2 与 trans_totensor 合并</span></span><br><span class="line"><span class="comment"># PIL -&gt; PIL -&gt; tensor</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2, trans_totensor])</span><br><span class="line"></span><br><span class="line">img_resize_2 = trans_compose(img)</span><br><span class="line">writer.add_image(<span class="string">&#x27;Resize&#x27;</span>, img_resize_2, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="5-3-6-RandomCrop类"><a href="#5-3-6-RandomCrop类" class="headerlink" title="5.3.6 RandomCrop类"></a>5.3.6 RandomCrop类</h3><p>随机裁剪，输入一个序列如<code>(h, w)</code>或一个<code>整数值size</code>，会按输入裁剪为一个<code>(h,w)</code>或一个正方形<code>(size, size)</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># RandomCrop</span><br><span class="line"># trans_random = transforms.RandomCrop(512)</span><br><span class="line">trans_random = transforms.RandomCrop((512, 1024))</span><br><span class="line">trans_compose_2 = transforms.Compose([trans_random, trans_totensor])</span><br><span class="line">for i in range(10):</span><br><span class="line">    img_crop = trans_compose_2(img)</span><br><span class="line">    writer.add_image(&#x27;RandomCropHW&#x27;, img_crop, i)</span><br></pre></td></tr></table></figure><h2 id="5-4-结合-Datasets-使用"><a href="#5-4-结合-Datasets-使用" class="headerlink" title="5.4 结合 Datasets 使用"></a>5.4 结合 Datasets 使用</h2><p>pytorch官方提供的数据集：<a href="https://docs.pytorch.org/vision/stable/datasets.html">https://docs.pytorch.org/vision/stable/datasets.html</a></p><p>如<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a>，需要设置参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">torchvision.datasets.CIFAR10(</span><br><span class="line">    root: <span class="type">Union</span>[<span class="built_in">str</span>, Path],     <span class="comment"># 数据集的位置</span></span><br><span class="line">    train: <span class="built_in">bool</span> = <span class="literal">True</span>,         <span class="comment"># 为True是一个训练集，为False则是一个测试集</span></span><br><span class="line">    transform: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>,            <span class="comment"># 需要对数据集进行什么transform变化</span></span><br><span class="line">    target_transform: <span class="type">Optional</span>[<span class="type">Callable</span>] = <span class="literal">None</span>,     <span class="comment"># 对target进行一个transform变化</span></span><br><span class="line">    download: <span class="built_in">bool</span> = <span class="literal">False</span>        <span class="comment"># 设置为True时自动从网上下载数据集，为False时不会下载</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Parameters:</p><ul><li>root (str or pathlib.Path) – Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True.</li><li>train (bool, optional) – If True, creates dataset from training set, otherwise creates from test set.</li><li>transform (callable, optional) – A function&#x2F;transform that takes in a PIL image and returns a transformed version. E.g, transforms.RandomCrop</li><li>target_transform (callable, optional) – A function&#x2F;transform that takes in the target and transforms it.</li><li>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</li></ul><p><strong>示例代码</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置用到的transform方法，通过compose进行组合</span></span><br><span class="line">dataset_transform = torchvision.transforms.Compose([</span><br><span class="line">    torchvision.transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集和测试集设置</span></span><br><span class="line">train_set = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;dataset&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=dataset_transform,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line">test_set = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;dataset&#x27;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=dataset_transform,</span><br><span class="line">    download=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入tensorboard</span></span><br><span class="line">writer = SummaryWriter(log_dir=<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    img, target = test_set[i]   <span class="comment"># 此时的img是tensor数据类型</span></span><br><span class="line">    writer.add_image(<span class="string">&#x27;test_set&#x27;</span>, img, i)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h1 id="6-Dataloader"><a href="#6-Dataloader" class="headerlink" title="6 Dataloader"></a>6 Dataloader</h1><p>dataloader是一个数据加载器，把数据加载到神经网络中，通过参数设置如何去dataset中取数据。</p><p>官网说明：<a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">None</span>, sampler=<span class="literal">None</span>, </span><br><span class="line">    batch_sampler=<span class="literal">None</span>, num_workers=<span class="number">0</span>, collate_fn=<span class="literal">None</span>, </span><br><span class="line">    pin_memory=<span class="literal">False</span>, drop_last=<span class="literal">False</span>, timeout=<span class="number">0</span>, </span><br><span class="line">    worker_init_fn=<span class="literal">None</span>, multiprocessing_context=<span class="literal">None</span>, </span><br><span class="line">    generator=<span class="literal">None</span>, *, prefetch_factor=<span class="literal">None</span>, </span><br><span class="line">    persistent_workers=<span class="literal">False</span>, pin_memory_device=<span class="string">&#x27;&#x27;</span>, </span><br><span class="line">    in_order=<span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure><ul><li><strong>dataset</strong> (<a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"><em>Dataset</em></a>) ：自定义的dataset，告诉我们数据集在什么位置，以及具体数据的索引等等。</li><li><strong>batch_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>, optional</em>) ：每次读取数据的量</li><li><strong>shuffle</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>, optional</em>) ：是否打乱数据顺序，为True则进行打乱操作，默认为False</li><li><strong>num_workers</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>, optional</em>)：加载数据时使用单个进程或多个进程，多个进程更快，默认情况是0。不为0时在Windows环境中可能会报错</li><li><strong>drop_last</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>, optional</em>) ：当数据量总数除不尽<strong>batch_size</strong>时，为True则最后的数据舍去，默认False</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;./dataset&#x27;</span>, </span><br><span class="line">    train=<span class="literal">False</span>, </span><br><span class="line">    download=<span class="literal">True</span>, </span><br><span class="line">    transform=torchvision.transforms.ToTensor()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="comment"># 定义数据加载对象</span></span><br><span class="line">test_loader = DataLoader(</span><br><span class="line">    dataset=test_data,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    num_workers=<span class="number">0</span>,</span><br><span class="line">    drop_last=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集中第一张图片及target</span></span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>查看dataloader的数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape, targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个数据：4张图片，3通道，32*32像素</span></span><br><span class="line"><span class="comment"># 第二个数据：4张图片对应的编号</span></span><br><span class="line"><span class="comment"># torch.Size([4, 3, 32, 32]) tensor([9, 6, 3, 7])</span></span><br><span class="line"><span class="comment"># torch.Size([4, 3, 32, 32]) tensor([7, 1, 7, 0])</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><p><strong>使用tensorboard记录日志</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):    <span class="comment"># epoch是指完整遍历数据集一次（完成总样本数量的计算，随机取样时会有重复）</span></span><br><span class="line">    step = <span class="number">0</span>    <span class="comment"># 迭代次数，使用一个批次（batch）数据进行一次参数更新的过程。</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        <span class="comment"># print(imgs.shape, targets)</span></span><br><span class="line">        writer.add_image(<span class="string">&#x27;Epoch:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch), imgs, step, dataformats=<span class="string">&#x27;NCHW&#x27;</span>)</span><br><span class="line">        step = step + <span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numpy 常用的几种命令</title>
      <link href="/20240902/"/>
      <url>/20240902/</url>
      
        <content type="html"><![CDATA[<h1 id="一、创建数组"><a href="#一、创建数组" class="headerlink" title="一、创建数组"></a>一、创建数组</h1><h2 id="一-Ndarray-对象及其创建"><a href="#一-Ndarray-对象及其创建" class="headerlink" title="(一) Ndarray 对象及其创建"></a>(一) Ndarray 对象及其创建</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入Numpy库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h3 id="1、创建Numpy数组"><a href="#1、创建Numpy数组" class="headerlink" title="1、创建Numpy数组"></a>1、创建Numpy数组</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一维数组（向量）</span></span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二维数组（矩阵）</span></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br></pre></td></tr></table></figure><h3 id="2、Numpy数组的属性"><a href="#2、Numpy数组的属性" class="headerlink" title="2、Numpy数组的属性"></a>2、Numpy数组的属性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看维度数量</span></span><br><span class="line">a.ndim  <span class="comment"># 结果为1</span></span><br><span class="line">A.ndim  <span class="comment"># 结果为2</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 查看数组形状</span></span><br><span class="line">a.shape  <span class="comment"># 结果为(6,)  表示1行6列</span></span><br><span class="line">A.shape  <span class="comment"># 结果为(2,3)  表示2行3列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数组中元素总个数</span></span><br><span class="line">a.size  <span class="comment"># 结果为6</span></span><br><span class="line">A.size  <span class="comment"># 结果为6</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 数组中元素类型</span></span><br><span class="line">a.dtype  <span class="comment"># 结果为dtype(&#x27;int32&#x27;)</span></span><br><span class="line">A.dtype  <span class="comment"># 结果为dtype(&#x27;int32&#x27;)</span></span><br></pre></td></tr></table></figure><h3 id="3、创建数组的便捷函数"><a href="#3、创建数组的便捷函数" class="headerlink" title="3、创建数组的便捷函数"></a>3、创建数组的便捷函数</h3><p><strong>ones 创建全是<code>1.</code>的数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认数据类型是浮点型</span></span><br><span class="line">np.ones(<span class="number">4</span>)  <span class="comment"># 结果为array([1.,1.,1.,1.])</span></span><br><span class="line">np.ones(<span class="number">4</span>).dtype  <span class="comment"># 结果为dtype(&#x27;float64&#x27;)</span></span><br><span class="line">np.ones(<span class="number">4</span>, dtype=<span class="string">&#x27;int64&#x27;</span>)  <span class="comment"># 结果为array([1, 1, 1, 1], dtype=int64)</span></span><br><span class="line">np.ones(shape=(<span class="number">2</span>,<span class="number">4</span>))  <span class="comment"># 创建2行4列的数组（矩阵） &#x27;shape&#x27;可省</span></span><br><span class="line">np.ones_like(A)  <span class="comment"># 创建与A数组形状相同的数组</span></span><br></pre></td></tr></table></figure><p><strong>zeros 创建全为<code>0.</code>的数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认数据类型是浮点型</span></span><br><span class="line">np.zeros(<span class="number">4</span>)  <span class="comment"># array([0., 0., 0., 0.])</span></span><br><span class="line">np.zeros((<span class="number">2</span>,<span class="number">4</span>))  <span class="comment"># 创建2行4列的数组（矩阵） 已省略&#x27;shape&#x27;</span></span><br><span class="line">np.zeros_like(A)  <span class="comment"># 创建与A数组形状相同的数组</span></span><br></pre></td></tr></table></figure><p><strong>full 创建指定数值的数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.full(<span class="number">8</span>,<span class="number">666</span>)  <span class="comment"># 创建1行8列值全为666的数组 array([666, 666, 666, 666, 666, 666, 666, 666])</span></span><br><span class="line">np.full((<span class="number">2</span>,<span class="number">4</span>),<span class="number">666</span>)  <span class="comment"># 创建2行4列，元素均为666</span></span><br><span class="line">np.full(A,<span class="number">666</span>)  <span class="comment"># 创建形状与A相同，元素均为666</span></span><br></pre></td></tr></table></figure><p><strong>empty 创建数组并初始化元素值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.empty(<span class="number">8</span>)  <span class="comment"># 创建1行8列的数组，不会将数组值设置为零</span></span><br><span class="line">np.empty((<span class="number">2</span>,<span class="number">4</span>))  <span class="comment"># 2行4列</span></span><br><span class="line">np.empty_like(A)  <span class="comment"># 与A形状相同</span></span><br></pre></td></tr></table></figure><h3 id="4、创建等差数组"><a href="#4、创建等差数组" class="headerlink" title="4、创建等差数组"></a>4、创建等差数组</h3><p><strong>arange 创建等差数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># arange 创建等差数组(默认步长为1)</span></span><br><span class="line">np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line">np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>)  <span class="comment"># 范围[1,10,2)，区间左闭右开，步长为2，array([1, 3, 5, 7, 9])</span></span><br><span class="line">np.arange(<span class="number">10</span>).reshape(<span class="number">2</span>,<span class="number">5</span>)  <span class="comment"># 改数组形状为2行5列，array([[0, 1, 2, 3, 4],[5, 6, 7, 8, 9]])</span></span><br></pre></td></tr></table></figure><p><strong>linspace 创建等差数组</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># linspace 创建等差数组，可指定元素数量</span></span><br><span class="line">np.linspace(<span class="number">0</span>,<span class="number">10</span>,<span class="number">5</span>)  <span class="comment"># 范围[0,10]，区间双闭合，元素数量为5</span></span><br><span class="line"><span class="comment"># array([ 0. , 2.5, 5. , 7.5, 10. ])</span></span><br></pre></td></tr></table></figure><h3 id="5、数据取整"><a href="#5、数据取整" class="headerlink" title="5、数据取整"></a>5、数据取整</h3><p><strong>向上&amp;向下</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.ceil(X)  <span class="comment"># 向上取整</span></span><br><span class="line"></span><br><span class="line">np.floor(X)  <span class="comment"># 向下取整</span></span><br></pre></td></tr></table></figure><p><strong>四舍五入取整</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1.2347</span>, <span class="number">0.4214</span>, <span class="number">42.2374</span>]).astype(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line"><span class="comment"># [ 1.2347  0.4214 42.2374]</span></span><br><span class="line"></span><br><span class="line">a.<span class="built_in">round</span>(<span class="number">2</span>)<span class="comment"># a数组中所有值均保留n位小数</span></span><br><span class="line"><span class="comment"># [ 1.23  0.42 42.24]</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">round</span>(a, <span class="number">2</span>)<span class="comment"># a数组中所有值均保留n位小数</span></span><br><span class="line"><span class="comment"># [ 1.23  0.42 42.24]</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">round</span>(X)  <span class="comment"># 四舍五入（实际是四舍六入），注意：对浮点数的取舍遵循的是四舍六入五平分</span></span><br><span class="line"><span class="comment"># 在误差理论中,当整数部分是偶数,小数部分是0.5时,向下取整;</span></span><br><span class="line"><span class="comment"># 当整数部分是奇数,小数部分是0.5时,则向上取整</span></span><br><span class="line"><span class="comment"># 也就是说,当小数部分是0.5的时候,“去奇存偶”,这样得到的结果在统计学上更精确</span></span><br></pre></td></tr></table></figure><h2 id="二-Numpy-随机数模块"><a href="#二-Numpy-随机数模块" class="headerlink" title="(二) Numpy 随机数模块"></a>(二) Numpy 随机数模块</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)</span><br><span class="line">np.random.shuffle(X)  <span class="comment"># 打乱X的顺序，会改变原数组，无返回值</span></span><br></pre></td></tr></table></figure><h3 id="1、random-模块"><a href="#1、random-模块" class="headerlink" title="1、random 模块"></a>1、random 模块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># random生成的值均在[0,1)之间，区间左闭右开</span></span><br><span class="line">np.random.random()  <span class="comment"># 返回一个[0,1)之间的随机数</span></span><br><span class="line">np.random.random(<span class="number">5</span>)  <span class="comment"># 返回一个一维数组，元素数5个，值在[0,1)之间</span></span><br><span class="line">np.random.random((<span class="number">2</span>,<span class="number">4</span>))  <span class="comment"># 返回一个2行4列的数组，值在[0,1)之间</span></span><br></pre></td></tr></table></figure><h3 id="2、rand-模块"><a href="#2、rand-模块" class="headerlink" title="2、rand 模块"></a>2、rand 模块</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类似random()用法，生成元素均在[0,1)之间的随机数，区间左闭右开</span></span><br><span class="line">np.random.rand()  <span class="comment"># 返回一个[0,1)之间的随机数</span></span><br><span class="line">np.random.rand(<span class="number">5</span>)  <span class="comment"># 返回一个一维数组，元素数5个，值在[0,1)之间</span></span><br><span class="line">np.random.rand(<span class="number">2</span>,<span class="number">4</span>)  <span class="comment"># 区别于np.random.random((2,4))，只需一个括号！</span></span><br><span class="line">np.random.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)  <span class="comment"># 返回一个三维数组</span></span><br></pre></td></tr></table></figure><h3 id="3、randint-获得随机整数"><a href="#3、randint-获得随机整数" class="headerlink" title="3、randint 获得随机整数"></a>3、randint 获得随机整数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.random.randint(<span class="number">5</span>)  <span class="comment"># 生成[0,5)之间的一个随机整数</span></span><br><span class="line">np.random.randint(<span class="number">5</span>,<span class="number">10</span>)  <span class="comment"># 生成[5,10)之间的一个随机整数</span></span><br><span class="line">np.random.randint(<span class="number">5</span>,<span class="number">10</span>,size=<span class="number">8</span>)  <span class="comment"># 在[5,10)之间生成8个随机整数，&#x27;size=&#x27;可省</span></span><br><span class="line">np.random.randint(<span class="number">5</span>,<span class="number">10</span>,size=(<span class="number">2</span>,<span class="number">4</span>))  <span class="comment"># 生成2行4列的数组，&#x27;size=&#x27;可省</span></span><br></pre></td></tr></table></figure><h3 id="4、seed-设置随机数生成器的种子"><a href="#4、seed-设置随机数生成器的种子" class="headerlink" title="4、seed 设置随机数生成器的种子"></a>4、seed 设置随机数生成器的种子</h3><p><strong>为随机数生成器设置了一个固定的种子后，每次生成的随机序列都会是相同的！</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过设置随机数种子后，重复运行np.random.randint(5,10,(2,4))时，生成的随机数将保持不变！</span></span><br><span class="line">np.random.seed(<span class="number">666</span>)</span><br><span class="line">np.random.randint(<span class="number">5</span>,<span class="number">10</span>,size=(<span class="number">2</span>,<span class="number">4</span>))   <span class="comment"># array([[9, 7, 6, 9],[8, 8, 9, 9]])</span></span><br></pre></td></tr></table></figure><h3 id="5、randn-标准正态分布"><a href="#5、randn-标准正态分布" class="headerlink" title="5、randn 标准正态分布"></a>5、randn 标准正态分布</h3><p><strong>生成标准正态分布（均值为0，标准差为1）的随机数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">5</span>)  <span class="comment"># 生成一维数组，元素数为5个，结果为标准正态分布</span></span><br><span class="line">np.random.randn(<span class="number">2</span>,<span class="number">4</span>)  <span class="comment"># 生成二维数组，2行4列，结果为标准正态分布</span></span><br></pre></td></tr></table></figure><h3 id="6、normal-高斯分布"><a href="#6、normal-高斯分布" class="headerlink" title="6、normal 高斯分布"></a>6、normal 高斯分布</h3><p><strong>可设置均值、方差，生成具有高斯分布的随机数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.normal()  <span class="comment"># 默认均值为0，标准差为1</span></span><br><span class="line">np.random.normal(<span class="number">10</span>,<span class="number">1000</span>,<span class="number">20</span>)  <span class="comment"># 均值为10，标准差为1000，生成20个数（一维数组）</span></span><br><span class="line">np.random.normal(<span class="number">10</span>,<span class="number">1000</span>,size=(<span class="number">4</span>,<span class="number">6</span>))  <span class="comment"># 生成4行6列的二维数组</span></span><br></pre></td></tr></table></figure><h3 id="7、uniform-均匀分布"><a href="#7、uniform-均匀分布" class="headerlink" title="7、uniform 均匀分布"></a>7、uniform 均匀分布</h3><p><strong>生成服从均匀分布的随机数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.uniform()  <span class="comment"># 默认生成[0,1)之间的一个随机数，区间左闭右开</span></span><br><span class="line">np.random.uniform(<span class="number">1</span>,<span class="number">5</span>)  <span class="comment"># 生成[1,5)之间的一个随机数</span></span><br><span class="line">np.random.uniform(<span class="number">1</span>,<span class="number">5</span>,(<span class="number">3</span>,<span class="number">4</span>))  <span class="comment"># 生成3行4列数组，值在[1,5)之间</span></span><br></pre></td></tr></table></figure><h1 id="二、数组的基础索引"><a href="#二、数组的基础索引" class="headerlink" title="二、数组的基础索引"></a>二、数组的基础索引</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.arange(<span class="number">10</span>)  <span class="comment"># a 为array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line">A = np.arange(<span class="number">20</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># A为：array([</span></span><br><span class="line"><span class="comment">#  [ 0, 1, 2, 3, 4],</span></span><br><span class="line"><span class="comment">#  [ 5, 6, 7, 8, 9],</span></span><br><span class="line"><span class="comment">#  [10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="comment">#  [15, 16, 17, 18, 19]</span></span><br><span class="line"><span class="comment"># ])</span></span><br></pre></td></tr></table></figure><h2 id="一-一维数组"><a href="#一-一维数组" class="headerlink" title="(一) 一维数组"></a>(一) 一维数组</h2><blockquote><p>调用方式同python切片取值</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>]  <span class="comment"># 取序号为0的值，结果为0</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>:<span class="number">5</span>]  <span class="comment"># 取序号[0,5)范围的值，区间左闭右开，array([0, 1, 2, 3, 4])</span></span><br><span class="line"></span><br><span class="line">a[:<span class="number">5</span>]  <span class="comment"># 取序号从头开始闭区间，到5结束开区间的值，array([0, 1, 2, 3, 4])</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">4</span>:]  <span class="comment"># 取序号从4开始闭区间，到数组末尾的值，array([4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">a[-<span class="number">3</span>:]  <span class="comment"># 取序号倒数第3个开始闭区间，到数组末尾的值，array([7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">a[:]  <span class="comment"># 复制一份原数组，修改此数组时不影响原数组值，array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">a[::<span class="number">2</span>]  <span class="comment"># 复制一份原数组，并调整原数组的步长为2，array([0, 2, 4, 6, 8])</span></span><br><span class="line"></span><br><span class="line">a[::-<span class="number">1</span>]  <span class="comment"># 复制一份原数组，并按原数组进行倒序，array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])</span></span><br></pre></td></tr></table></figure><h2 id="二-二维数组"><a href="#二-二维数组" class="headerlink" title="(二) 二维数组"></a>(二) 二维数组</h2><p>Numpy的二维数组<strong>切片得到的数组</strong>中的<strong>值改变</strong>时，<strong>原始值</strong>也会<strong>跟着变</strong>，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = A[:<span class="number">2</span>,:<span class="number">3</span>]</span><br><span class="line">X[<span class="number">0</span>,<span class="number">0</span>] = <span class="number">666</span>  <span class="comment"># 此时A[0,0]也会变为666</span></span><br><span class="line">A[<span class="number">0</span>,<span class="number">0</span>] = <span class="number">222</span>  <span class="comment"># 此时X[0,0]也会变为222</span></span><br></pre></td></tr></table></figure><p>使用.copy()方法将二维数组赋值给新数组，新数组变化不影响原数组，如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = A[:<span class="number">2</span>,:<span class="number">3</span>].copy()</span><br></pre></td></tr></table></figure><p>以下为二维数组常见调用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># A为:array([[ 0, 1, 2, 3, 4],</span></span><br><span class="line"><span class="comment">#      [ 5, 6, 7, 8, 9],</span></span><br><span class="line"><span class="comment">#      [10, 11, 12, 13, 14],</span></span><br><span class="line"><span class="comment">#      [15, 16, 17, 18, 19]])</span></span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>,<span class="number">0</span>]  <span class="comment"># 取第1行第1列的值，逗号前的值表行，逗号后的值表列</span></span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>,:]  <span class="comment"># 取第1行所有值，array([0, 1, 2, 3, 4])</span></span><br><span class="line"></span><br><span class="line">A[<span class="number">0</span>]  <span class="comment"># 同A[0,:]，取第1行所有值，array([0, 1, 2, 3, 4])</span></span><br><span class="line"></span><br><span class="line">A[-<span class="number">1</span>]  <span class="comment"># 取最后一行元素，array([15, 16, 17, 18, 19])</span></span><br><span class="line"></span><br><span class="line">A[:,<span class="number">0</span>]  <span class="comment"># 取第一列元素，array([ 0, 5, 10, 15])</span></span><br><span class="line"></span><br><span class="line">A[:,-<span class="number">1</span>]  <span class="comment"># 取最后一列元素，array([ 4, 9, 14, 19])</span></span><br><span class="line"></span><br><span class="line">A[:<span class="number">2</span>,:<span class="number">4</span>]  <span class="comment"># 取前2行，前3列元素，</span></span><br><span class="line"><span class="comment"># array([[0, 1, 2, 3],</span></span><br><span class="line"><span class="comment">#    [5, 6, 7, 8]])</span></span><br><span class="line"></span><br><span class="line">A[::-<span class="number">1</span>,::-<span class="number">1</span>]  <span class="comment"># 行取倒序，列取倒序</span></span><br><span class="line"><span class="comment">#array([[19, 18, 17, 16, 15],</span></span><br><span class="line"><span class="comment">#     [14, 13, 12, 11, 10],</span></span><br><span class="line"><span class="comment">#     [ 9, 8, 7, 6, 5],</span></span><br><span class="line"><span class="comment">#      [ 4, 3, 2, 1, 0]])</span></span><br></pre></td></tr></table></figure><h1 id="三、数组的合并拆分"><a href="#三、数组的合并拆分" class="headerlink" title="三、数组的合并拆分"></a>三、数组的合并拆分</h1><h2 id="一-数组的合并"><a href="#一-数组的合并" class="headerlink" title="(一) 数组的合并"></a>(一) 数组的合并</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入Numpy库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">6</span>])  <span class="comment"># array([6, 6, 6, 6])</span></span><br><span class="line">X = np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">#array([[ 0, 1,  2,  3],</span></span><br><span class="line"><span class="comment">#   [ 4, 5,  6,  7],</span></span><br><span class="line"><span class="comment">#    [ 8, 9, 10, 11]])</span></span><br><span class="line"></span><br><span class="line">B = np.full((<span class="number">3</span>,<span class="number">2</span>),<span class="number">666</span>)</span><br><span class="line"><span class="comment">#array([[666, 666],</span></span><br><span class="line"><span class="comment">#      [666, 666],</span></span><br><span class="line"><span class="comment">#   [666, 666]])</span></span><br></pre></td></tr></table></figure><h3 id="1、垂直方向（行操作）"><a href="#1、垂直方向（行操作）" class="headerlink" title="1、垂直方向（行操作）"></a>1、垂直方向（行操作）</h3><p><strong>方法1：concatenate函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A = np.concatenate((X,a.reshape(<span class="number">1</span>,<span class="number">4</span>)),axis=<span class="number">0</span>)  <span class="comment"># 把a添加进X，&#x27;axis&#x27;参数默认为0</span></span><br><span class="line"><span class="comment"># A为:array([[ 0, 1, 2, 3],</span></span><br><span class="line"><span class="comment">#      [ 4, 5, 6, 7],</span></span><br><span class="line"><span class="comment">#      [ 8, 9, 10, 11],</span></span><br><span class="line"><span class="comment">#      [ 6, 6, 6, 6]])</span></span><br></pre></td></tr></table></figure><p><strong>方法2：vstack函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.vstack((X,a))  <span class="comment"># 返回的是一个新的数组对象，原始数组不会被修改</span></span><br><span class="line"><span class="comment"># array([[ 0, 1, 2, 3],</span></span><br><span class="line"><span class="comment">#    [ 4, 5, 6, 7],</span></span><br><span class="line"><span class="comment">#    [ 8, 9, 10, 11],</span></span><br><span class="line"><span class="comment">#    [ 6, 6, 6, 6]])</span></span><br></pre></td></tr></table></figure><h3 id="2、水平方向（列操作）"><a href="#2、水平方向（列操作）" class="headerlink" title="2、水平方向（列操作）"></a>2、水平方向（列操作）</h3><p><strong>方法1：concatenate函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.concatenate((X,B),axis=<span class="number">1</span>)  <span class="comment"># &#x27;axis&#x27;参数默认为0，设为1时为列操作</span></span><br><span class="line"><span class="comment">#array([[ 0,  1,  2,  3, 666, 666],</span></span><br><span class="line"><span class="comment">#    [ 4,  5,  6,  7, 666, 666],</span></span><br><span class="line"><span class="comment">#    [ 8,  9, 10, 11, 666, 666]])</span></span><br></pre></td></tr></table></figure><p><strong>方法2：hstack函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">np.hstack((X,B))</span><br><span class="line"><span class="comment">#array([[ 0,  1,  2,  3, 666, 666],</span></span><br><span class="line"><span class="comment">#    [ 4,  5,  6,  7, 666, 666],</span></span><br><span class="line"><span class="comment">#    [ 8,  9, 10, 11, 666, 666]])</span></span><br></pre></td></tr></table></figure><h2 id="二-数组的拆分"><a href="#二-数组的拆分" class="headerlink" title="(二) 数组的拆分"></a>(二) 数组的拆分</h2><p>split函数中拆分部分按 <strong>数组[ ]</strong> 来分</p><p><strong>数组[ ]</strong> 中包含几个值，将会按照从0开始，以每个值做开区间来进行划分，具体代码如下</p><h3 id="1、按行拆分"><a href="#1、按行拆分" class="headerlink" title="1、按行拆分"></a>1、按行拆分</h3><p><strong>方法1：split函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X1,X2 = np.split(X,[-<span class="number">1</span>],axis=<span class="number">0</span>)  <span class="comment"># 按[-1]从最后一行开始拆分,分为两个区间，区间[0,-1)和区间[-1:]两部分，&#x27;axis&#x27;默认为0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># X1为:array([[0, 1, 2, 3],</span></span><br><span class="line"><span class="comment">#       [4, 5, 6, 7]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># X2为:array([[ 8, 9, 10, 11]])</span></span><br></pre></td></tr></table></figure><p><strong>方法2：vsplit函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X1,X2 = np.vsplit(X,[<span class="number">2</span>])  <span class="comment"># 按[2]从第2行开始拆分，区间[0,2)行一部分，[2:]行一部分</span></span><br><span class="line"><span class="comment"># X1为：array([[0, 1, 2, 3],[4, 5, 6, 7]])</span></span><br><span class="line"><span class="comment"># X2为：array([[ 8, 9, 10, 11]])</span></span><br><span class="line"></span><br><span class="line">np.vsplit(X,[<span class="number">1</span>,<span class="number">2</span>])  <span class="comment"># 按区间划分，区间[0,1)，区间[1,2)，区间[2:]</span></span><br><span class="line"><span class="comment"># [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])]</span></span><br></pre></td></tr></table></figure><h3 id="2、按列拆分"><a href="#2、按列拆分" class="headerlink" title="2、按列拆分"></a>2、按列拆分</h3><p><strong>方法1：split函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X1,X2 = np.split(X,[-<span class="number">1</span>],axis=<span class="number">1</span>)  <span class="comment"># 按[-1]从最后一行开始拆分,分为两个区间，区间[0,-1)和区间[-1:]两部分</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># X1为:array([[ 0, 1, 2],</span></span><br><span class="line"><span class="comment">#       [ 4, 5, 6],</span></span><br><span class="line"><span class="comment">#       [ 8, 9, 10]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># X2为:array([[ 3],</span></span><br><span class="line"><span class="comment">#        [ 7],</span></span><br><span class="line"><span class="comment">#        [11]])</span></span><br></pre></td></tr></table></figure><p><strong>方法2：hsplit函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">X1,X2 = np.hsplit(X,[-<span class="number">1</span>])  <span class="comment"># 按[-1]从最后一行开始拆分,分为两个区间，区间[0,-1)和区间[-1:]两部分</span></span><br><span class="line"><span class="comment"># array([[ 0, 1, 2],</span></span><br><span class="line"><span class="comment">#    [ 4, 5, 6],</span></span><br><span class="line"><span class="comment">#    [ 8, 9, 10]])</span></span><br><span class="line"><span class="comment"># array([[ 3],</span></span><br><span class="line"><span class="comment">#    [ 7],</span></span><br><span class="line"><span class="comment">#    [11]])</span></span><br><span class="line"></span><br><span class="line">np.hsplit(X,[<span class="number">1</span>,<span class="number">2</span>,-<span class="number">1</span>])   <span class="comment"># [1,2,-1]进行拆分，以列表形式返回，分为4个区间，[0,1)、[1,2)、[2,-1)、[-1:]</span></span><br><span class="line"><span class="comment"># [array([[0],</span></span><br><span class="line"><span class="comment">#     [4],</span></span><br><span class="line"><span class="comment">#     [8]]),</span></span><br><span class="line"><span class="comment"># array([[1],</span></span><br><span class="line"><span class="comment">#     [5],</span></span><br><span class="line"><span class="comment">#     [9]]),</span></span><br><span class="line"><span class="comment"># array([[ 2],</span></span><br><span class="line"><span class="comment">#     [ 6],</span></span><br><span class="line"><span class="comment">#     [10]]),</span></span><br><span class="line"><span class="comment"># array([[ 3],</span></span><br><span class="line"><span class="comment">#     [ 7],</span></span><br><span class="line"><span class="comment">#     [11]])]</span></span><br></pre></td></tr></table></figure><h1 id="四、Numpy矩阵运算"><a href="#四、Numpy矩阵运算" class="headerlink" title="四、Numpy矩阵运算"></a>四、Numpy矩阵运算</h1><p>对比 Python 将数组乘2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)]  <span class="comment"># A == [0, 1, 2, 3, 4]</span></span><br><span class="line">y = [i*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> A]  <span class="comment"># y == [0, 2, 4, 6, 8]</span></span><br></pre></td></tr></table></figure><p> 举例：Numpy 将数组乘2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.arange(<span class="number">5</span>)  <span class="comment"># A == array([0, 1, 2, 3, 4])</span></span><br><span class="line">Y = A * <span class="number">2</span>  <span class="comment"># Y == array([0, 2, 4, 6, 8])</span></span><br></pre></td></tr></table></figure><h2 id="一-一元运算"><a href="#一-一元运算" class="headerlink" title="(一) 一元运算"></a>(一) 一元运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">1</span>,<span class="number">11</span>,<span class="number">0.5</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># array([[ 1. , 1.5, 2. , 2.5, 3. ],</span></span><br><span class="line"><span class="comment">#    [ 3.5, 4. , 4.5, 5. , 5.5],</span></span><br><span class="line"><span class="comment">#    [ 6. , 6.5, 7. , 7.5, 8. ],</span></span><br><span class="line"><span class="comment">#    [ 8.5, 9. , 9.5, 10. , 10.5]])</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">abs</span>(X)  <span class="comment"># 绝对值</span></span><br><span class="line"></span><br><span class="line">np.sqrt(X)  <span class="comment"># 开方</span></span><br><span class="line"></span><br><span class="line">np.square(X)  <span class="comment"># 平方</span></span><br><span class="line"></span><br><span class="line">np.exp(X)  <span class="comment"># e的指数</span></span><br><span class="line"></span><br><span class="line">np.log(X)  <span class="comment"># 对数（以e为底）</span></span><br><span class="line"></span><br><span class="line">np.log2(X)  <span class="comment"># 对数（以2为底）</span></span><br><span class="line"></span><br><span class="line">np.sin(X)  <span class="comment"># 正弦</span></span><br><span class="line">np.cos(X)  <span class="comment"># 余弦</span></span><br><span class="line">np.tan(X)  <span class="comment"># 正切</span></span><br></pre></td></tr></table></figure><h2 id="二-二元运算"><a href="#二-二元运算" class="headerlink" title="(二) 二元运算"></a>(二) 二元运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">1</span>,<span class="number">11</span>,<span class="number">0.5</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># array([[ 1. , 1.5, 2. , 2.5, 3. ],</span></span><br><span class="line"><span class="comment">#    [ 3.5, 4. , 4.5, 5. , 5.5],</span></span><br><span class="line"><span class="comment">#    [ 6. , 6.5, 7. , 7.5, 8. ],</span></span><br><span class="line"><span class="comment">#    [ 8.5, 9. , 9.5, 10. , 10.5]])</span></span><br><span class="line"></span><br><span class="line">X + <span class="number">1</span>  <span class="comment"># X中所有元素 +1</span></span><br><span class="line">np.add(X,<span class="number">1</span>)  <span class="comment"># 同 X + 1</span></span><br><span class="line"></span><br><span class="line">X - <span class="number">1</span>  <span class="comment"># X中所有元素 -1</span></span><br><span class="line"></span><br><span class="line">X * <span class="number">2</span>  <span class="comment"># X中所有元素 *2</span></span><br><span class="line"></span><br><span class="line">X / <span class="number">2</span>  <span class="comment"># X中所有元素 除2，并返回浮点数结果</span></span><br><span class="line"></span><br><span class="line"><span class="number">2</span> / X  <span class="comment"># 2 除 X中所有元素 </span></span><br><span class="line"></span><br><span class="line">X // <span class="number">2</span>  <span class="comment"># X中所有元素 整除2，并丢弃小数部分，只保留整数部分</span></span><br><span class="line"></span><br><span class="line">X % <span class="number">2</span>  <span class="comment"># X中所有元素 对2取余</span></span><br><span class="line"></span><br><span class="line">X ** <span class="number">3</span>  <span class="comment"># X中所有元素 乘方操作，X所有元素扩大为3次幂</span></span><br></pre></td></tr></table></figure><h2 id="三-矩阵运算"><a href="#三-矩阵运算" class="headerlink" title="(三) 矩阵运算"></a>(三) 矩阵运算</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X = np.linspace(<span class="number">10</span>,<span class="number">40</span>,<span class="number">4</span>).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># array([[10., 20.],</span></span><br><span class="line"><span class="comment">#    [30., 40.]])</span></span><br><span class="line"></span><br><span class="line">Y = np.linspace(<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>).reshape(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment"># array([[1., 2.],</span></span><br><span class="line"><span class="comment">#    [3., 4.]])</span></span><br></pre></td></tr></table></figure><p><strong>矩阵常用运算</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">X + Y  <span class="comment"># 矩阵加法，位对位相加：array([[11., 22.],[33., 44.]])</span></span><br><span class="line"></span><br><span class="line">X - Y  <span class="comment"># 矩阵减法，位对位相减：array([[ 9., 18.],[27., 36.]])</span></span><br><span class="line"></span><br><span class="line">X * Y  <span class="comment"># 不是矩阵乘法！！！注意：该乘法是位对位相乘！array([[ 10., 40.],[ 90., 160.]])</span></span><br><span class="line"></span><br><span class="line">X.dot(Y)  <span class="comment"># 矩阵乘法，前行 乘 后列 再 求和 的形式，array([[ 70., 100.],[150., 220.]])</span></span><br><span class="line"></span><br><span class="line">X / Y  <span class="comment"># 位对位除法，array([[10., 10.],[10., 10.]])</span></span><br><span class="line"></span><br><span class="line">X.T  <span class="comment"># 转置矩阵，array([[10., 30.],[20., 40.]])</span></span><br><span class="line"></span><br><span class="line">np.transpose(X)  <span class="comment"># 对二维矩阵为转置，array([[10., 30.],[20., 40.]])</span></span><br><span class="line"></span><br><span class="line">np.linalg.inv(X)  <span class="comment"># 求逆矩阵，原阵必须为方阵，array([[-0.2 , 0.1 ],[ 0.15, -0.05]])</span></span><br><span class="line"></span><br><span class="line">np.linalg.det(X)  <span class="comment"># 求矩阵行列式，必须为方阵，结果为：-200.0000000000001</span></span><br><span class="line"></span><br><span class="line">np.linalg.eig(X)  <span class="comment"># 求矩阵的特征值和特征向量，</span></span><br><span class="line"><span class="comment"># EigResult(eigenvalues=array([-3.72281323, 53.72281323]), </span></span><br><span class="line"><span class="comment"># eigenvectors=array([[-0.82456484, -0.41597356], [ 0.56576746, -0.90937671]]))</span></span><br></pre></td></tr></table></figure><h1 id="五、Numpy统计运算"><a href="#五、Numpy统计运算" class="headerlink" title="五、Numpy统计运算"></a>五、Numpy统计运算</h1><p><strong>统计最值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 引入numpy</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对一维数组操作</span></span><br><span class="line">X = np.arange(<span class="number">5000000</span>)</span><br><span class="line">np.<span class="built_in">min</span>(X)  <span class="comment"># 最小值</span></span><br><span class="line">np.<span class="built_in">max</span>(X)  <span class="comment"># 最大值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对二维数组操作</span></span><br><span class="line">Y = X.reshape(<span class="number">5</span>,-<span class="number">1</span>)  <span class="comment"># 将X转换为5行的二维数组</span></span><br><span class="line">np.<span class="built_in">min</span>(Y,axis=<span class="number">0</span>)  <span class="comment"># 按 列 求最小值，将每列的最小值找出并按顺序放进数组中</span></span><br><span class="line">np.<span class="built_in">min</span>(Y,axis=<span class="number">1</span>)  <span class="comment"># 按 行 求最小值，将每行的最小值找出并按顺序放进数组中</span></span><br></pre></td></tr></table></figure><p><strong>计算其他数值</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">X = np.linspace(<span class="number">1</span>,<span class="number">20</span>,<span class="number">20</span>).reshape(<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(X)  <span class="comment"># 求和</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(X,axis=<span class="number">0</span>)  <span class="comment"># 按列求和，计算每列元素之和</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(X,axis=<span class="number">1</span>)  <span class="comment"># 按行求和，计算每行元素之和</span></span><br><span class="line"></span><br><span class="line">np.mean(X)  <span class="comment"># 求均值</span></span><br><span class="line"></span><br><span class="line">np.median(X)  <span class="comment"># 求中位数</span></span><br><span class="line"></span><br><span class="line">np.std(X)  <span class="comment"># 求标准差</span></span><br><span class="line"></span><br><span class="line">np.var(X)  <span class="comment"># 求方差</span></span><br><span class="line"></span><br><span class="line">np.ptp(X)  <span class="comment"># 计算数组中最大值和最小值的差</span></span><br><span class="line"></span><br><span class="line">np.percentile(X, <span class="number">90</span>)  <span class="comment"># 百分位数，求取X数列第90%分位的数值，结果：18.1</span></span><br><span class="line">q = np.linspace(<span class="number">0</span>,<span class="number">100</span>,<span class="number">5</span>)  <span class="comment"># array([ 0., 25., 50., 75., 100.])</span></span><br><span class="line">np.percentile(X,q)  <span class="comment"># 百分位数，按数组q中的值来返回数组X的第n个百分位数，array([1., 5.75, 10.5, 15.25, 20.])</span></span><br><span class="line"></span><br><span class="line">np.cumsum(X,axis=<span class="number">0</span>)  <span class="comment"># 求累加，&#x27;axis&#x27;默认为0，第一行值不变，其后每一行的值 = 原值 + 前一行的值</span></span><br><span class="line">np.cumsum(X,axis=<span class="number">1</span>)  <span class="comment"># 求累加，第一列值不变，其后 每一列的值 = 原值 + 前一列的值</span></span><br><span class="line"></span><br><span class="line">np.diff(X,axis=<span class="number">0</span>)  <span class="comment"># &#x27;axis&#x27;默认为0，计算每行元素之间的差</span></span><br><span class="line">np.diff(X,axis=<span class="number">1</span>)  <span class="comment"># 计算每列元素之间的差</span></span><br><span class="line"></span><br><span class="line">np.prod(X,axis=<span class="number">0</span>)  <span class="comment"># &#x27;axis&#x27;不可省，计算每列元素之间的乘积</span></span><br><span class="line">np.prod(X,axis=<span class="number">1</span>)  <span class="comment"># 计算每行元素的乘积</span></span><br></pre></td></tr></table></figure><h1 id="六、arg运算和排序"><a href="#六、arg运算和排序" class="headerlink" title="六、arg运算和排序"></a>六、arg运算和排序</h1><h3 id="1、arg运算"><a href="#1、arg运算" class="headerlink" title="1、arg运算"></a>1、arg运算</h3><p>arg主要用来提取索引值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,size=<span class="number">1000000</span>)  <span class="comment"># 生成随机数组，均值为0，标准差为1，1000000个数</span></span><br><span class="line">np.<span class="built_in">min</span>(X)  <span class="comment"># 找最小值</span></span><br><span class="line">np.argmin(X)  <span class="comment"># 找到最小值所在的位置序号</span></span><br><span class="line">X[np.argmin(X)]  <span class="comment"># 查看最小值</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">max</span>(X)  <span class="comment"># 找最大值</span></span><br><span class="line">np.argmax(X)  <span class="comment"># 找到最大值的位置</span></span><br></pre></td></tr></table></figure><h3 id="2、排序"><a href="#2、排序" class="headerlink" title="2、排序"></a>2、排序</h3><p>先使用np.random.shuffle(X)打乱原数组的顺序，再进行排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)</span><br><span class="line">np.random.shuffle(X)  <span class="comment"># 打乱X的顺序，会改变原数组，无返回值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序操作</span></span><br><span class="line">np.sort(X)  <span class="comment"># 返回从小到大的排序，不改变原数组</span></span><br><span class="line"></span><br><span class="line">X.sort()  <span class="comment"># python自带的排序法，无返回值，会改变原数组</span></span><br><span class="line"></span><br><span class="line">X[::-<span class="number">1</span>]  <span class="comment"># 将原数组改为降序</span></span><br></pre></td></tr></table></figure><h3 id="3、arg应用"><a href="#3、arg应用" class="headerlink" title="3、arg应用"></a>3、arg应用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)</span><br><span class="line">np.random.shuffle(X)  <span class="comment"># 打乱X的顺序，会改变原数组，无返回值</span></span><br><span class="line"></span><br><span class="line">np.argsort(X)  <span class="comment"># 按从小到大顺序先排列，而后将排列后的元素在原数组中的位置去到</span></span><br></pre></td></tr></table></figure><h3 id="3、partition分区"><a href="#3、partition分区" class="headerlink" title="3、partition分区"></a>3、partition分区</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)</span><br><span class="line">np.random.shuffle(X)  <span class="comment"># 打乱X的顺序，会改变原数组，无返回值</span></span><br><span class="line"></span><br><span class="line">np.partition(X,<span class="number">5</span>)  <span class="comment"># 5表示有序排序中第5个元素，小于5的在5左边，大于5的在5右边，不改变原数组</span></span><br><span class="line"></span><br><span class="line">np.argpartition(X,<span class="number">5</span>)  <span class="comment"># 对np.partition(X,5)中数组元素取在原数组X中的位置</span></span><br></pre></td></tr></table></figure><h3 id="4、高维数组排序"><a href="#4、高维数组排序" class="headerlink" title="4、高维数组排序"></a>4、高维数组排序</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = np.random.randint(<span class="number">10</span>,size=(<span class="number">5</span>,<span class="number">5</span>))  <span class="comment"># 生成5行5列的数组</span></span><br><span class="line">np.sort(X)  <span class="comment"># 将每行中的元素进行升序排序</span></span><br><span class="line"></span><br><span class="line">np.argsort(X,axis=<span class="number">1</span>)  <span class="comment"># 按行操作，将每行中元素升序排列后的序号返回</span></span><br><span class="line"></span><br><span class="line">np.argsort(X,axis=<span class="number">1</span>)  <span class="comment"># 按列操作，将每列中元素升序排列（小数在上）的序号返回</span></span><br></pre></td></tr></table></figure><h1 id="七、神奇索引和布尔索引"><a href="#七、神奇索引和布尔索引" class="headerlink" title="七、神奇索引和布尔索引"></a>七、神奇索引和布尔索引</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">index = [<span class="number">2</span>,<span class="number">5</span>,<span class="number">8</span>]</span><br><span class="line">X[index]  <span class="comment"># 取X[2]，X[5]，X[8]，array([2, 5, 8])</span></span><br><span class="line"></span><br><span class="line">X[<span class="number">2</span>:<span class="number">8</span>]  <span class="comment"># 对X切片，array([2, 3, 4, 5, 6, 7])</span></span><br><span class="line"></span><br><span class="line">X[<span class="number">2</span>:<span class="number">8</span>:<span class="number">2</span>]  <span class="comment"># 对X切片，且步长为2，array([2, 4, 6])</span></span><br><span class="line"></span><br><span class="line">index = np.array([[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>],[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]])</span><br><span class="line">X[index]  <span class="comment"># 结果与index的维度一致，array([[1, 3, 5],[2, 4, 6]])</span></span><br></pre></td></tr></table></figure><h3 id="1、神奇索引"><a href="#1、神奇索引" class="headerlink" title="1、神奇索引"></a>1、神奇索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于二维数组</span></span><br><span class="line">X = np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line">X = X.reshape(<span class="number">2</span>,-<span class="number">1</span>)  <span class="comment"># X变为两行的二维数组</span></span><br><span class="line">row = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>])  <span class="comment"># 表示行坐标</span></span><br><span class="line">col = np.array([<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>])  <span class="comment"># 表示列坐标</span></span><br><span class="line"></span><br><span class="line">X[row,col]  <span class="comment"># 取X[0][0]，X[1][2]，X[0][4]的值，array([0, 7, 4])</span></span><br><span class="line"></span><br><span class="line">X[<span class="number">0</span>,col]  <span class="comment"># 取第0行中序号为col的值，array([0, 2, 4])</span></span><br><span class="line"></span><br><span class="line">X[row,:<span class="number">2</span>]  <span class="comment"># 取row行中序号在[0,2)的值，array([[0, 1],[5, 6],[0, 1]])</span></span><br></pre></td></tr></table></figure><h3 id="2、布尔索引"><a href="#2、布尔索引" class="headerlink" title="2、布尔索引"></a>2、布尔索引</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line">X = X.reshape(<span class="number">2</span>,-<span class="number">1</span>)  <span class="comment"># X变为两行的二维数组</span></span><br><span class="line"></span><br><span class="line">col = [<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>]  <span class="comment"># 相当于[0,2,4]</span></span><br><span class="line">X[<span class="number">0</span>,col]  <span class="comment"># 取第0行中序号为col的值，array([0, 2, 4])</span></span><br></pre></td></tr></table></figure><p><strong>比较用法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">X &lt; <span class="number">5</span>  <span class="comment"># array([ True, True, True, True, True, False, False, False, False, False])</span></span><br><span class="line"></span><br><span class="line">X &gt;= <span class="number">2</span>  <span class="comment"># array([False, False, True, True, True, True, True, True, True, True])</span></span><br><span class="line"></span><br><span class="line">X == <span class="number">3</span>  <span class="comment"># array([False, False, False, True, False, False, False, False, False,False])</span></span><br><span class="line"></span><br><span class="line">X != <span class="number">3</span>  <span class="comment"># array([ True, True, True, False, True, True, True, True, True, True])</span></span><br><span class="line"></span><br><span class="line">X * X == <span class="number">9</span>  <span class="comment"># array([False, False, False, True, False, False, False, False, False, False])</span></span><br></pre></td></tr></table></figure><p><strong>比较用法应用</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">X[X &gt; <span class="number">3</span>]  <span class="comment"># array([4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">X[X % <span class="number">2</span>]  <span class="comment"># array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])</span></span><br><span class="line"></span><br><span class="line">np.count_nonzero(X &lt; <span class="number">5</span>)  <span class="comment"># 找出X&lt;5的个数，5个</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(X &lt; <span class="number">5</span>)  <span class="comment"># 计算X中小于5的个数，5个</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">any</span>(X &lt; <span class="number">0</span>)  <span class="comment"># X中是否有小于0的数，False</span></span><br><span class="line">np.<span class="built_in">any</span>(X &gt; <span class="number">5</span>)  <span class="comment"># X中是否有大于5的数，True</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">all</span>(X &gt; <span class="number">0</span>)  <span class="comment"># X中元素是否都大于0，False</span></span><br><span class="line">np.<span class="built_in">all</span>(X &lt; <span class="number">10</span>)  <span class="comment"># X中元素是否都小于10，True</span></span><br><span class="line"></span><br><span class="line">X = X.reshape(<span class="number">2</span>,-<span class="number">1</span>)  <span class="comment"># 将X变为2行的二维数组</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(X % <span class="number">2</span> == <span class="number">0</span>)  <span class="comment"># 计算X中所有偶数的个数，5个</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>(X % <span class="number">2</span> == <span class="number">0</span>, axis=<span class="number">1</span>)  <span class="comment"># 统计每行有几个偶数，array([3, 2])</span></span><br><span class="line">np.<span class="built_in">sum</span>(X % <span class="number">2</span> == <span class="number">0</span>, axis=<span class="number">0</span>)  <span class="comment"># 统计每列有几个偶数，array([1, 1, 1, 1, 1])</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">all</span>(X &gt; <span class="number">2</span>, axis=<span class="number">0</span>)  <span class="comment"># 每列是否都大于2，array([False, False, False, True, True])</span></span><br></pre></td></tr></table></figure><h1 id="八、与-或-非"><a href="#八、与-或-非" class="headerlink" title="八、与&amp; 或| 非~"></a>八、与&amp; 或| 非~</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(<span class="number">10</span>)  <span class="comment"># array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span></span><br><span class="line"></span><br><span class="line">np.<span class="built_in">sum</span>((X&lt;<span class="number">5</span>) &amp; (X&gt;<span class="number">1</span>))  <span class="comment"># 找小于5 且 大于1 的数的个数，3个</span></span><br><span class="line">np.<span class="built_in">sum</span>(~(X==<span class="number">0</span>))  <span class="comment"># 除了0都是，9个</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 编程应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Anaconda 中常用conda命令</title>
      <link href="/20240901/"/>
      <url>/20240901/</url>
      
        <content type="html"><![CDATA[<h1 id="常用的conda命令"><a href="#常用的conda命令" class="headerlink" title="常用的conda命令"></a>常用的conda命令</h1><ul><li><code>conda -V</code> 查看conda版本</li><li><code>conda info</code> 查看该环境的详细信息</li><li><code>conda activate env_name</code>切换至env_name环境</li><li><code>conda activate</code>默认进入base环境</li><li><code>conda deactivate</code>退出环境</li><li>显示所有已经创建的环境<ul><li><code>conda info -e</code></li><li><code>conde env list</code></li></ul><p></p></li><li><code>conda list</code> 查看所有已安装的包</li><li><code>conda install package_name</code>在当前环境中安装包</li><li><code>conda remove package</code>删除当前环境中的包</li><li><code>conda create -n env_name package_name</code><ul><li>创建名为<code>env_name</code>的新环境，并在该环境下安装名为<code>package_name</code>的包</li><li>可以指定新环境的版本号（Python版本号），如：<code>conda create -n env_name python=3.8</code></li></ul></li><li><code>conda remove --name env_name --all</code>删除名为env_name的环境</li></ul><h1 id="conda安装32位的python环境"><a href="#conda安装32位的python环境" class="headerlink" title="conda安装32位的python环境"></a>conda安装32位的python环境</h1><p><strong>已安装 Anaconda 版本为64位版本，默认安装的 Python 版本是64 位</strong></p><ol><li>首先运行conda info 可查看该环境的详细信息<ul><li>注意查看环境信息中的 <code>platform : win-64</code></li><li>说明当前环境为64位</li></ul></li><li><code>set CONDA_FORCE_32BIT=1</code><ul><li>临时进入32位模式</li><li>此命令并不是一直有效的，所以退出命令行窗口之后再进入这个环境就会发现无法得到预期的结果</li></ul></li><li>使用 <code>conda info</code> 可查看该环境的详细信息<ul><li>注意查看环境信息中的 <code>platform : win-32</code></li><li>说明当前环境已经切换为32位</li></ul></li><li><code>conda create -n python38_32 python=3.8</code><ul><li>创建32为的python环境</li><li><code>python38-32</code>是新配置的环境名称</li><li>python&#x3D;3.8指定安装的32位python版本</li></ul></li><li><code>conda activate python38_32</code><ul><li>切换到新的虚拟环境</li><li>python38-32是新配置的环境名称</li></ul></li><li><code>set CONDA_FORCE_32BIT=0</code><ul><li>恢复64位模式</li></ul></li><li><strong>注意</strong><ul><li>在运行32位python的过程中可能会有些包无法使用</li><li>及时切换环境位32位再使用：<code>set CONDA_FORCE_32BIT=1</code></li><li>切换好环境后再运行目标程序，注意此切换方法只有临时效果！！！</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> conda命令 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>科研技能应用——笔记</title>
      <link href="/20240808/"/>
      <url>/20240808/</url>
      
        <content type="html"><![CDATA[<blockquote><p>主要内容来源：B站UP主 <a href="https://space.bilibili.com/1668113965"><strong>做科研的大师兄</strong></a></p></blockquote><h1 id="一、科研思维建立"><a href="#一、科研思维建立" class="headerlink" title="一、科研思维建立"></a>一、科研思维建立</h1><h2 id="一-研究领域入门"><a href="#一-研究领域入门" class="headerlink" title="(一) 研究领域入门"></a>(一) 研究领域入门</h2><ul><li>阅读硕博学位论文，初学者只需选择性阅读论文中的<strong>综述部分</strong>。</li><li>通过几篇研究方向<strong>高度相关</strong>的论文综述部分学习，可以保证快速高效在<strong>研究方向</strong>上进行观念和思维入门。</li><li>完成研究方向的观念入门后，就是对具体的<strong>研究内容</strong>进行快速入门，这时<strong>重点关注</strong>论文的研究成果和讨论部分，指导该领域研究主要做哪些实验，证明什么问题，用了什么方法等。</li><li>最后，需要进行<strong>研究方法入门</strong>，集中将搜集到的论文中方法部分进行整理学习，相互比较，确定为了回答某种问题而采用哪种研究方法，具体如何实施等细节。</li><li>外文文献研究确定创新性 + 课题思路及实验设计 + 实验方法和实验用品准备</li></ul><p><strong>1、检索筛选优秀博士论文</strong></p><ol><li>检索方法<ul><li><a href="https://www.cnki.net/">CNKI</a>——高级检索——主题（大方向）&#x2F;篇名（小领域）——博士论文</li></ul></li><li>筛选方法（找出优秀论文）<ul><li>985等高校博士论文——下载后人工筛选——<strong>目录</strong> &#x2F; <strong>结果</strong> &#x2F; <strong>已发表文章</strong></li><li>找出5-10篇优秀论文，进行科研入门学习</li></ul></li></ol><p><strong>2、整体认识博士论文框架</strong></p><p>扉页—中英文摘要—<strong>目录</strong>—前言—方法—<strong>结果</strong>—讨论—参考文献—附录（文献综述、材料及结果附图）—致谢—读博期间成果（<strong>已发表文章</strong>）</p><p>文章采用的方法部分在后续了解时重点查看</p><p><strong>3、博士论文阅读及笔记整理 ※※※※※</strong></p><p>思维导图构建科研逻辑（<strong>Logic + Think level</strong>）</p><ol><li>结合<strong>目录</strong>构建文献整体逻辑框架<ul><li>对目录进行<strong>提炼总结</strong>或<strong>直接引用</strong>来做脑图节点</li><li>对节点进行<strong>表型（现象）</strong>和<strong>机制</strong>分类</li><li>找出<strong>主体框架</strong>来分析该文章</li></ul></li><li>查找<strong>结果</strong>部分填充实验方法细节<ul><li>对机制部分进行细分，构建逻辑关系（递进及因果）</li><li>用箭头指出每个逻辑之间的关系，注明关系后在进行分析</li></ul></li><li>文献阅读的目的<ul><li>不是以记忆知识点为目的！！！</li><li>记忆上层<strong>主体逻辑框架</strong></li><li>知道具体细节如何查找&#x2F;询问（细节无需记忆，做了就自然记住了）</li></ul></li></ol><p><img src="/20240808/IMG_20240828_111056.jpg" alt="IMG_20240828_111056"></p><h2 id="二-逻辑思维与设计能力"><a href="#二-逻辑思维与设计能力" class="headerlink" title="(二) 逻辑思维与设计能力"></a>(二) 逻辑思维与设计能力</h2><p><strong>1、什么是科研逻辑？</strong></p><p>本质上是因果关系论证逻辑，做课题的过程是对一段因果上下游关系的论证。</p><p><u>科研逻辑中的因果关系如何论证？</u></p><ul><li>通过rescue实验——柯霍氏法则（因果关系论证法则）<ul><li>病原体和疾病表型具有相关性</li><li>去除病原体，疾病表型减弱</li><li>相同病原体再次感染，可以重新出现疾病表型（rescue）</li></ul></li></ul><p><strong>2、什么是科研逻辑思维？</strong></p><blockquote><p>我们应该具有的一种思路，通过这种思路来帮助我们，在帮助我们做科研的一个过程中，将存在的逻辑关系给它建立起来</p></blockquote><ul><li>科研——常见形式是课题研究，主要是提出科学问题，针对科学问题提出解释它的科学假说，并通过实验来验证该假说的过程</li><li>逻辑——本质上是关系，包括是非&#x2F;因果&#x2F;上下等各种关系</li><li>思维——构建起逻辑关系的思路、想法</li></ul><p><strong>3、什么是课题设计能力？</strong></p><ul><li><p>课题设计能力就是能够提出可靠<strong>work model</strong>的能力</p></li><li><p><u>基于文献</u>  +  <u>实验结果</u>  +  <u>逻辑推理（排除法、类比法……）</u></p></li></ul><p><strong>4、如何提升逻辑思维与设计能力</strong></p><ol><li><strong>研究课题</strong>是如何进行课题切入的（选题）？<ul><li>通过哪些基础数据，通过哪些文献，通过哪些怎样的逻辑推理，让他提出了这样一个科研问题</li></ul></li><li><strong>数据</strong>是如何进行挑选的（数据分析）？<ul><li>为了去解释这个科研问题，做了什么数据？做了什么实验？</li><li>常常会做数据分析，通过这些数据，需要从中挑选一个切入点</li><li>切入点是怎么选择的，有什么依据，通过读文献！</li></ul></li><li>研究机制是如何进行锁定的（逻辑关联）？<ul><li>基于第二步的<strong>数据</strong>提出一种科学<strong>假说</strong>来解释第一步的<strong>科研问题</strong></li><li>主体的机制通过很多个逻辑环路，根据因果<strong>一环一环</strong>的关联起来</li><li><strong>研究解释</strong>它每个因果关系是怎么关联起来的，是怎么去把它给证实了</li></ul></li><li>调控模式是如何进行架构的（整体架构串联）？<ul><li>work model（模式图案）<strong>整体上</strong>是怎么架构起来的，第三步中每一个小块都能够串联起来，那么整体就是怎么关联的</li></ul></li><li>研究模型是如何选择的（模型）？<ul><li>需要证实的科学假说，是基于实验的</li><li>实验依赖的模型都是有一定要求的，每一篇文献都会有一些讲解！！！</li></ul></li><li>调控关系是如何证明的（因果论证的逻辑严密性）？<ul><li>每一小块的逻辑关系是怎么证明这个因果的，实验的严谨性是如何保证的，读文献时注意作者是怎么证明的</li></ul></li></ol><h2 id="三-如何自己找课题"><a href="#三-如何自己找课题" class="headerlink" title="(三) 如何自己找课题"></a>(三) 如何自己找课题</h2><p><strong>参考：</strong><a href="https://www.bilibili.com/video/BV1zD4y1a7eU?vd_source=6c9d491042982f363ce749e6f5504631">如何通过已知的文献报道提出创新性课题？</a></p><blockquote><p>文献：<strong>已知性</strong>（精读解析，逻辑重建）</p><p>课题：<strong>创新性</strong>（未报道过，文献检索），<strong>意义</strong>（理论拓展，应用价值）</p><p>通过已知文献报道提出创新性课题为什么会觉得矛盾？</p><ul><li>根本原因：对<strong>概念</strong>或者<strong>对象</strong>的认识过于粗浅，缺乏深入思考！</li></ul></blockquote><p><strong>文献已知</strong>和<strong>课题创新</strong>矛盾破局方法</p><ol><li>当对一个概念有一个很细化或很深入的理解之后，应该对课题的大方向进行细化</li><li>首先找到它里面的<strong>关键概念</strong>，然后对里面的每一个关键概念进行<strong>子概念的拆分</strong>，并将子概念进行记录分类（<strong>通过多读文献实现</strong>）</li><li>积累一定量的文献后，才能知道各个子概念之间的关系</li><li>将各子概念列举出来，并用箭头画出它们之间的关系，可以清晰的得到哪些概念是研究过的，哪些概念是没有研究过的</li><li>通过这些文献<strong>已经得到的关系</strong>来提出自己的一个创新性课题（常用逻辑关联推理方法有：类比&#x2F;排除）</li><li>提出的课题要有一定依据，不能随意、随机的组合，要注意没有报道的可能是因为前人发现不成立因此没有报道，要谨慎！！</li></ol><h1 id="二、文献资料应用"><a href="#二、文献资料应用" class="headerlink" title="二、文献资料应用"></a>二、文献资料应用</h1><h2 id="一-文献分区介绍"><a href="#一-文献分区介绍" class="headerlink" title="(一) 文献分区介绍"></a>(一) 文献分区介绍</h2><p>国外常用<strong>JCR分区</strong>，国内常用<strong>中科院分区</strong></p><h3 id="1、英文期刊分区"><a href="#1、英文期刊分区" class="headerlink" title="1、英文期刊分区"></a>1、英文期刊分区</h3><table><thead><tr><th align="center">分区</th><th align="center">占比</th><th align="center">分区</th><th align="center">占比</th></tr></thead><tbody><tr><td align="center">中科院一区</td><td align="center">~ 5%</td><td align="center">JCR一区（Q1）</td><td align="center">~ 25%</td></tr><tr><td align="center">中科院二区</td><td align="center">5% ~ 20%</td><td align="center">JCR二区（Q2）</td><td align="center">25% ~ 50%</td></tr><tr><td align="center">中科院三区</td><td align="center">20% ~ 50%</td><td align="center">JCR三区（Q3）</td><td align="center">50% ~ 75%</td></tr><tr><td align="center">中科院四区</td><td align="center">50% ~ 100%</td><td align="center">JCR四区（Q4）</td><td align="center">75% ~ 100%</td></tr></tbody></table><p><strong>开源期刊</strong>，简称<strong>OA（Open Access）</strong>，任何人都可以免费的获取文献，这种期刊一般是作者付费出版，读者免费获取，相对而言，OA期刊容易发表一些。</p><table><thead><tr><th align="left">OA期刊</th><th align="center"></th><th align="left">非OA期刊</th></tr></thead><tbody><tr><td align="left">1.审核周期较短<br />2.需要出版面费<br />3.认可度相对较低</td><td align="center"><strong>VS</strong></td><td align="left">1.审核周期较长<br />2.不需要缴费<br />3.认可度较高</td></tr></tbody></table><h3 id="2、中文期刊分区"><a href="#2、中文期刊分区" class="headerlink" title="2、中文期刊分区"></a>2、中文期刊分区</h3><blockquote><p>中文核心期刊有哪些</p></blockquote><ol><li>北京大学图书馆“中文核心期刊”（又称北大核心或中文核心）</li><li>南京大学“中文社会科学引文索引（CSSCI）来源期刊”（又称南大核心或C刊）</li><li>中国科学技术信息研究所“中国科技论文统计源期刊CSTPCD”（又称中国科技核心期刊）</li><li>中国科学院文献情报中心“中国科学引文数据库（CSCD）来源期刊”</li><li>中国社会科学院文献信息中心“中国人文社会科学核心期刊CHSSCD”（社科院核心）</li><li>中国人文社会科学学报学会“中国人文社科学报核心期刊”</li><li>万方数据股份有限公司的”中国核心期刊遴选数据库“</li></ol><h2 id="二-如何查全、查准文献"><a href="#二-如何查全、查准文献" class="headerlink" title="(二) 如何查全、查准文献"></a>(二) 如何查全、查准文献</h2><p>查文献时涉及的问题：</p><ul><li>什么是主题词？——<strong>人工定义</strong>的一个词，在主题词检索时可以关联到所有同义词</li></ul><p><img src="/20240808/image-20240830231205108.png" alt="image-20240830231205108"></p><ul><li><p>什么是副主题词？——在主题词的范围下，在进行限定的词：如肺癌的<strong>治疗</strong></p></li><li><p>如何确定<strong>主题词</strong>？——通过Mesh检索，随便输入一个同义词即可找到主题词</p></li><li><p>如何确定<strong>同义词</strong>？——通过Mesh检索，找到主题词后，就可以找到所有同义词</p></li><li><p>如何查全？——主题词检索 + 所有同义词高级检索（<strong>Why</strong>：文献主题词录入时间滞后与发表时间）</p><ul><li>例如 A是主题词，B是副主题词：</li><li><strong>‘A&#x2F;B’[Mesh] or ‘A1’[Title&#x2F;Abstract] or ‘A2’[Title&#x2F;Abstract]…</strong></li></ul></li></ul><h3 id="1、查全文献"><a href="#1、查全文献" class="headerlink" title="1、查全文献"></a>1、查全文献</h3><ul><li>特点：全面性，无遗漏</li><li>方法：pubmed(医学领域)主题词检索+高级检索</li><li>应用场景：课题 <strong>立题阶段</strong> 时查新，确保创新性</li></ul><p><strong>查全文献示例</strong>：巨噬细胞在肺癌治疗中的所有文献查全</p><p>主题词1：肺癌</p><p>副主题词1：治疗</p><p>主题词2：巨噬细胞</p><p>高价检索代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(肺癌/治疗[Mesh] AND 巨噬细胞[Mesh]) OR </span><br><span class="line">(肺癌所有同义词[Title/Abstract] AND </span><br><span class="line">therap*[Title/Abstract] AND</span><br><span class="line">巨噬细胞所有同义词[Title/Abstract])</span><br></pre></td></tr></table></figure><p><strong>therap*</strong> 代表所有以therap开头的词，包括了治疗的所有形式（therapy、therapeutic）</p><h3 id="2、查准文献"><a href="#2、查准文献" class="headerlink" title="2、查准文献"></a>2、查准文献</h3><ul><li>特点：精准性、高效</li><li>方法：pubmed(医学领域)高级检索 + Google搜索 + 图片搜索</li><li>应用场景：<ul><li>课题设计时查文献支持证据，找逻辑关联（通路与表型、基因与通路之间等关联</li><li>数据分析挑选下游和上游机制切入点：通路或靶标（基因与基因，基因与通路之间关联）</li><li>特定实验步骤、结果展示（实验方法及结果展示）</li></ul></li></ul><h2 id="三-SCI文献该怎么读"><a href="#三-SCI文献该怎么读" class="headerlink" title="(三) SCI文献该怎么读"></a>(三) SCI文献该怎么读</h2><h3 id="1、读哪些文献？"><a href="#1、读哪些文献？" class="headerlink" title="1、读哪些文献？"></a>1、读哪些文献？</h3><blockquote><p>影响因子（Impact Factor, IF）是通过计算<strong>期刊的平均引用次数</strong>来评估学术期刊影响力的一种指标。通常，期刊的影响因子越高，表示该期刊的文章被其他学者引用的频次越高，其影响力也就越大。高影响因子期刊中的文章往往代表着该领域的高质量学术成果，因此影响因子也成为了研究者们选择投稿期刊的重要参考。</p></blockquote><ul><li>主刊&#x2F;大子刊(IF20+，如Cell&#x2F;Nature&#x2F;Science…)：获取科研热点，借鉴科研思路，学习科研逻辑。</li><li>IF10 - IF20分文献：借鉴经典技术方法，学习经典科研套路</li><li>IF5 - IF10分文献：建议不看，在检索课题创新性时关注一下</li><li>优秀博士论文：开始科研入门时一定要看。（研究生刚入门时）</li></ul><h3 id="2、读文献方法（概述）"><a href="#2、读文献方法（概述）" class="headerlink" title="2、读文献方法（概述）"></a>2、读文献方法（概述）</h3><p><strong>①速读</strong></p><p>此方法适用于刚开始进行课题研究。对于研究领域及研究方法等不熟悉的情况。此时，首先要确定研究方向的关键词或主题词，在pubmed进行文献检索，将相关文献导入endnote管理软件（主要选择10-20分的文献，一 般200篇左右），然后一定要集中精力逐个阅读每个文献的<strong>题目</strong>和<strong>摘要</strong>，并同时<strong>进行记录</strong>、<strong>整理归类</strong>。通过这种方式，可以在一周内快速完成200-篇文献阅读，从而达到在短时间内，集中快速阅读大量研究领域的文献，快速实现对研究领域的熟悉及了解并初步形成自己的研究思路。</p><p><strong>②跳读</strong></p><p>此方法适用于开展课题研究中涉及到一些研究<strong>方法细节</strong>、<strong>引物序列</strong>、<strong>试剂信息</strong>等查询的情况。此时无需从头到尾读文献，在确定文献中有自己所需特定信息情况下，直击主题，可以通过在文献文件中进行查找的方式（PDF打开用<u>Ctrl+F</u>调出查找功能），快速跳到文章对应区域，获取自己信息。</p><p><strong>③精读</strong></p><p>此方法适用于科研思维能力的更高阶提升，培养个人课题设计能力。通过速读大概了解研究方向和领域的前提下，我们仍然需要进一步通过更深入的文献学习，全面提升科研思维和课题设计能力。此时读文献没有技巧可言，就是不断追踪研究领域最前沿的文章（主刊或大子刊，一般是20+以上的文献）通篇阅读。通过仔细阅读Introduction部分，学习整个研究是如何切入的；Results部分，学习整个研究的逻辑是如何逐步切换递进的；Discussion部分，学习本研究的长处和不足以及未来研究趋势；Methods部分，学习文章的模型设计有哪些具体条件等。整体而言，文献精读是最重要的，是提升个人科研思维和课题设计能力最重要的途径！</p><h3 id="3、读文献顺序（详细）"><a href="#3、读文献顺序（详细）" class="headerlink" title="3、读文献顺序（详细）"></a>3、读文献顺序（详细）</h3><p>从中文文献到英文文献阅读顺序从<strong>6种途径</strong>展开：</p><p><strong>中文硕博→中文综述→中文期刊→英文综述→英文期刊→英文硕博</strong></p><ol><li>中文硕博（学位论文）<ul><li>看完后大致对该领域有所了解</li><li>接下来找创新点</li></ul></li><li>中文综述（近几年）<ul><li>看一下中国有哪些科学家对这方面进行综述了</li><li>看创新点的描述，有哪些是解决了的，哪些是没有解决的</li><li>有大概了解的方向，如哪个领域有xx些问题，基本的技能在中文硕博论文已有了解</li><li>提出想要做的方向</li></ul></li><li>中文期刊<ul><li>找对应想要做的方向</li><li>看有哪些人，研究了什么，用了什么方法来研究的，看有哪些方面的不足</li></ul></li><li>英文综述（近几年）<ul><li>看创新点，综合国内外的问题来看，并且提出将来的发展方向</li><li>对应自己已了解的信息，发现该研究方向确实是个问题</li><li>看人家的综述是如何描写的，是否有方法自己没有了解到，对应的找英文期刊</li></ul></li><li>英文期刊<ul><li>此时已经了解到了国内、国外在这个方向到底有什么样的问题</li><li>看英文期刊中是否有更高深的、更好的办法已经解决了这样的问题</li><li>开始自己创新点的研究和学习</li></ul></li><li>英文硕博</li></ol><h2 id="四-读懂SCI文献—实战篇"><a href="#四-读懂SCI文献—实战篇" class="headerlink" title="(四) 读懂SCI文献—实战篇"></a>(四) 读懂SCI文献—实战篇</h2><p><strong>参考：</strong></p><ul><li><a href="https://www.bilibili.com/video/BV1q14y1H7o4/?spm_id_from=333.788">985博后大师兄手把手带你读懂SCI文献—实战篇！</a></li><li><a href="https://www.bilibili.com/video/BV1sG4y1g7Fi/?spm_id_from=333.337.search-card.all.click">英文SCI文献，最强实战流程！</a></li></ul><h2 id="五-文献汇报PPT制作讲解"><a href="#五-文献汇报PPT制作讲解" class="headerlink" title="(五) 文献汇报PPT制作讲解"></a>(五) 文献汇报PPT制作讲解</h2><h3 id="1、文献汇报PPT该怎么做？"><a href="#1、文献汇报PPT该怎么做？" class="headerlink" title="1、文献汇报PPT该怎么做？"></a>1、文献汇报PPT该怎么做？</h3><p><strong>①总体框架</strong>（7项）</p><ol><li>标题页面</li><li>通讯作者介绍页面</li><li>研究背景到科学问题（猜想&#x2F;选题）确定页面</li><li>本研究最终证实（结果）的work model简介页面</li><li>结果部分图片逐步展示页面</li><li>再次介绍work model页面</li><li>批判性思考页面</li></ol><p><strong>②具体做法</strong></p><ol><li>标题页面——<strong>讲解文章发表时间及杂志名称</strong><ul><li>包括题目，杂志名称，杂志接收文章的时间点、发表的时间点</li></ul></li><li>通讯作者介绍页面——<strong>Google搜索，介绍通讯作者研究领域及所取得的成果</strong><ul><li>通过文献找到通讯作者，使用谷歌搜索：作者名 + lab，找到实验室主页</li><li>实验室主页会介绍他的研究领域及近期所取得的成果</li><li>可以了解到作者是不是一个很有权威或影响力的人，这样就加入到订阅</li></ul></li><li>研究背景到科学问题（猜想&#x2F;选题）确定页面——<strong>如何引出科学问题</strong><ul><li>将文章的introduction部分逐条摘录出来</li><li>看作者如何根据这些背景提出怎样的科学猜想的，把科学猜想也放在这部分</li></ul></li><li>本研究最终证实（结果）的work model简介页面——<strong>预先了解文章整体框架</strong><ul><li>通过对work model部分（文章整体逻辑图）介绍，使听众了解文章整体框架</li></ul></li><li>结果部分图片逐步展示页面——<strong>重点介绍科研逻辑如何逐步递进展开 ※※※※※</strong><ul><li>按文献结果部分的小标题一个一个填进来，一个小标题一页ppt</li><li>为了证明小标题做了哪些实验，把实验结果等信息填充进去，必要的细节进行标注</li><li>每一个结果开始讲的时候，一定要讲作者为什么想去做这部分结果，文献原文每部分开头会有介绍，总结后填充到PPT</li><li>有关联的结果单独做总结，将已证实的逻辑关系做出来图插到PPT中</li><li>重点要介绍每一步的<strong>科研逻辑</strong>是<strong>如何展开</strong>的</li></ul></li><li>再次介绍work model页面——<strong>总结文章整体研究模式</strong><ul><li>总结上步介绍的各项结果</li></ul></li><li>批判性思考页面——<strong>借鉴之处 + 不足之处</strong><ul><li>辩证地思考一下这篇文章有哪些值得借鉴之处，发现哪些地方是不足之处</li></ul></li></ol><p><strong>③重要原则</strong></p><ul><li>总原则：让听众听懂（细节+逻辑）</li><li><strong>必要的细节</strong>需要标注、讲述（细节提示文字）</li><li>逻辑关系<strong>递进过渡</strong>之处要标注</li><li><strong>总分总</strong>逻辑结构展开</li></ul><h3 id="2、文献汇报PPT该怎么讲？"><a href="#2、文献汇报PPT该怎么讲？" class="headerlink" title="2、文献汇报PPT该怎么讲？"></a>2、文献汇报PPT该怎么讲？</h3><p><strong>①重要原则</strong></p><ul><li>总原则：让听众听懂（细节+逻辑提示）</li><li>必要的实验细节（样本分类、模型、条件…）需要标注、讲述</li><li>不要赘述实验细节，<strong>重点讲清楚逻辑过渡及串联，结果到结论</strong></li><li><strong>每部分结果的因果逻辑关系必须交代</strong>（每一个结果部分：为什么做？做了什么？得到什么结果？推出什么结论？）</li><li>独立、批判性思考结果（辩证思考）</li></ul><p><strong>②框架流程</strong></p><ol><li>标题页面——讲解文章<strong>发表时间</strong>及<strong>杂志名称</strong></li><li>通讯作者介绍页面——介绍<strong>通讯作者</strong>是谁，来自<strong>哪个单位</strong>，近几年来<strong>研究成果</strong>是什么，<strong>研究领域及方向</strong>是什么</li><li>研究背景到科学问题（猜想&#x2F;选题）确定页面<ul><li><strong>Introduction部分</strong>引出科学问题，定位原文中的<strong>逻辑关系词语</strong></li><li>每一段的最后几句，逻辑关系词：However、Therefore、Nevertheless</li><li>琢磨逻辑整合的过程，讲述整合了什么样的内容，通过什么样的方法进行推理比较，提出了什么样的猜想</li></ul></li><li>本研究最终证实（结果）的work model简介页面——先对<strong>研究结论</strong>进行讲解，让听众对<strong>整体框架</strong>有个了解</li><li>结果部分图片逐步展示页面<ul><li>重点介绍科研逻辑如何逐步递进展开的，讲解每个小结果是怎样展开的，讲清楚为什么要做这个实验，做这个实验的目的是为什么</li><li>每个<strong>Results部分</strong>定位原文中的<strong>逻辑推理类词语+目的词汇</strong></li><li>每一段的前两句找<strong>逻辑推理类词</strong>：<u>Based on</u>，<u>Given that</u>，<u>According to</u>……；<strong>目的词汇</strong>：<u>To</u>，<u>In order to</u>，<u>For this purpose</u>……</li></ul></li><li>再次介绍work model页面——总结文章<strong>整体研究模式</strong>，<strong>从细节</strong>重新<strong>回到整体</strong></li><li>批判性思考页面——分两部分：<strong>借鉴之处 + 不足之处</strong></li></ol><ul><li>借鉴之处<ol><li><strong>思路：</strong>他这篇文章是如何提出科学猜想的？他的选题思路（表型）是如何经过逻辑推理来确定下来的？<br>（实际应用：当独立做课题不知道从何下手时，应该怎么去思考，怎样去得到这样的科学猜想或选题）</li><li><strong>方法：</strong>看文章中是如何找到每一个逻辑关系的下游靶标的，借鉴他的分析方法。（应用：比如他发现了某x的功能，接下来怎么去找该x是如何发挥这个功能的，看他做了哪些工作，当我们遇到类似问题时，也可以使用同样的方法）</li></ol></li><li>不足之处<ol><li><strong>创新型是否足够：</strong>比如文章提出的创新模型是不是别人已经报到过的，他的逻辑关系中有几步是他自己发现的</li><li><strong>逻辑严谨性是否充分：</strong>比如他提出了一个研究模式，他有没有在研究模式的逻辑环路中每个关键环节进行<strong>回补实验</strong>，如果没有，就可以怀疑这个逻辑是不够严谨的</li><li><strong>结果和结论是否一致：</strong>把文章从前到后完整的精读完后，要去考量一下实验结果与结论是否前后一致</li></ol></li></ul><p><strong>实例示范：</strong><a href="https://www.bilibili.com/video/BV1F84y1t7SW?vd_source=6c9d491042982f363ce749e6f5504631">收藏版 | 文献汇报PPT该怎么讲？-大师兄亲自实例示范！</a></p><h2 id="六-文献学习中应如何提问"><a href="#六-文献学习中应如何提问" class="headerlink" title="(六) 文献学习中应如何提问"></a>(六) 文献学习中应如何提问</h2><p>提问是我们将文献中的科研方法思路内化为个人科研能力的最佳方式。</p><p>有<strong>五个方面</strong>可供提出问题：<strong>概念，实验方法细节，实验结果解读，课题设计，逻辑框架</strong></p><h3 id="1、初级入门提问"><a href="#1、初级入门提问" class="headerlink" title="1、初级入门提问"></a>1、初级入门提问</h3><p><strong>①概念</strong></p><ol><li>实验技术、实验方法、专有名词</li><li>对以上三类词汇提出疑问，这些词汇是什么意思</li><li>如果得不到清晰的解答，就把这些名词单独提取出来做好标记，作为一个概念进行查找记录学习掌握</li></ol><p><strong>②实验方法细节</strong></p><ol><li>答案位置定位到<strong>Results</strong>、<strong>Methods</strong>中</li><li>对方法细节中涉及到的内容进行提问，如果不能得到清晰的解答，需要进行文献定位查找，找到对应的答案</li></ol><p><strong>③实验结果解读</strong></p><ol><li>答案位置定位到<strong>Results</strong>中</li><li>对实验结果提出问题，结果证明了什么、为什么要做xx等等</li></ol><h3 id="2、高级进阶提问"><a href="#2、高级进阶提问" class="headerlink" title="2、高级进阶提问"></a>2、高级进阶提问</h3><p><strong>④课题设计</strong></p><ol><li>如何提出科学猜想？（选题思路）<ul><li>定位到Introduction部分，找到其中的逻辑转折词，如However或逻辑因果关系的词Therefore，这种词后面的话基本就是<strong>科学猜想</strong>、科学问题</li><li>前面的几句话就是<strong>提出科学猜想</strong>或科研问题的文献背景及其隐含的逻辑推理</li></ul></li><li>如何确定逻辑节点<ul><li>需要定位到每个result小标题下面的第一段前两句话都会提到</li><li>前两句话一般都会有结合文献背景，结合他们自己的一个实验结果等内容，找到他们下游的一个逻辑节点（就是下游实验是为了什么而做，做这种实验为了干嘛…）</li></ul></li><li>work model哪里创新？<ul><li>定位到文章的研究模型，结合通篇的研究结果，去考虑两个问题：第一，这个逻辑环路中有哪些环节是基于已知文献来建立起来的；第二，哪些环节是他们这次研究所发现的</li></ul></li></ol><p><strong>⑤逻辑框架</strong></p><ol><li>逻辑严谨性是否充分<ul><li>需要定位到文献的模式图和results部分，查找有无进行<strong>rescue实验</strong>（挽救实验或回补实验）</li><li>如果做了，查找rescue实验对应的逻辑节点是什么</li></ul></li><li>结果结论的前后一致性<ul><li>思考结果结论是否一致，需要定位到results部分，并综合前后结果进行一个批判性思考</li><li>如果前期发现的结果和后面发现的机制不相吻合，甚至前后矛盾，那么这点就是该文章的不足之处，这种地方不能学习</li></ul></li></ol><h1 id="三、科研工具介绍"><a href="#三、科研工具介绍" class="headerlink" title="三、科研工具介绍"></a>三、科研工具介绍</h1><h2 id="一-文献来源与阅读工具"><a href="#一-文献来源与阅读工具" class="headerlink" title="(一) 文献来源与阅读工具"></a>(一) 文献来源与阅读工具</h2><p><strong>1、文献查找来源</strong></p><ul><li>浏览器插件：easyScholar、沉浸式翻译</li><li><strong>国内文献</strong>：<a href="https://www.cnki.net/">中国知网</a>（可使用句子搜索，推荐使用翻译助手）、<a href="https://www.wanfangdata.com.cn/index.html">万方</a>、<a href="https://qikan.cqvip.com/">维普</a>、<a href="https://xueshu.baidu.com/">百度学术</a>等</li><li><strong>国外文献</strong>：<a href="https://www.webofscience.com/wos/">web of science</a>（可跟踪相关领域最新发展）、<a href="https://www.x-mol.com/">X-mol</a>、<a href="https://link.springer.com/">Springer</a>、<a href="https://ieeexplore.ieee.org/Xplore/home.jsp">IEEE（电气电子工程师学会）</a>、<a href="https://scholar.google.com/">Google Scholar</a>、SCI-Hub（SCI-Hub是下载文献全文，不是查文献，通过标题、<strong>DOI</strong> 或者 <strong>PMID</strong>、官方的Online链接进行查找 ）</li><li><strong>关联文献</strong>：<a href="https://openknowledgemaps.org/">Open Knowledge Maps</a>、<a href="https://www.connectedpapers.com/">Connected Papers</a>、<a href="https://researchrabbitapp.com/">Research Rabbit</a>等</li><li>综合平台：<a href="https://panda985.com/">熊猫学术</a>、<a href="http://5638.org/">科研宝库</a>、<a href="https://keyanxiazi.bepass.cn/">科研霞子</a>、<a href="https://www.ablesci.com/daohang">科研导航</a>等</li></ul><p>查询<strong>中文期刊</strong>一般用<u>知网</u>：<a href="https://navi.cnki.net/knavi/">https://navi.cnki.net/knavi/</a></p><p>查询SCI<strong>外文期刊</strong>一般用：<a href="https://www.letpub.com.cn/?page=journalapp">https://www.letpub.com.cn/?page=journalapp</a></p><p>查询<strong>计算机CCF评级</strong>的期刊会议一般用<u>中国计算机协会（CCF）</u>：<a href="https://www.ccf.org.cn/Academic_Evaluation/By_category/">https://www.ccf.org.cn/Academic_Evaluation/By_category/</a></p><p><strong>2、文献阅读工具</strong></p><ul><li>打印纸质版</li><li><a href="https://www.zhiyunwenxian.cn/">知云文献翻译</a>（<strong>免费</strong>，可注释，看英文文献时可翻译）</li><li><a href="https://www.zotero.org/">Zotero</a>（免费，文献管理软件，收集、管理和引用参考书目）</li><li><a href="https://endnote.com/">EndNote</a>（收费，文献管理软件）</li><li><a href="https://updf.com/">UPDF阅读器</a>（需会员才能用完整功能）</li><li><a href="https://www.xljsci.com/">小绿鲸</a>（需会员才能用英文翻译）</li></ul><h2 id="二-数据分析及作图"><a href="#二-数据分析及作图" class="headerlink" title="(二) 数据分析及作图"></a>(二) 数据分析及作图</h2><p>入门：<strong>Origin</strong>、<strong>Visio</strong>、Matlab、Python、R语言（适用于大型数据）等</p><p>强化：Adobe Illustrator（数据作图及排版，画模式图）、Adobe Photoshop等</p><p>进阶：Blender、Keyshot、C4D、3Dmax等</p><h2 id="三-写作工具"><a href="#三-写作工具" class="headerlink" title="(三) 写作工具"></a>(三) 写作工具</h2><ul><li>Microsoft Word </li><li>Latex：Overleaf、Textpage、Textlive+TextStudio、Vscode、Pycharm</li></ul><h1 id="四、论文写作要点"><a href="#四、论文写作要点" class="headerlink" title="四、论文写作要点"></a>四、论文写作要点</h1><h2 id="一-发表文章（小论文）"><a href="#一-发表文章（小论文）" class="headerlink" title="(一) 发表文章（小论文）"></a>(一) 发表文章（小论文）</h2><p><strong>1、SCI写作经验分享——原则篇</strong></p><p>参考：</p><p><a href="https://www.bilibili.com/video/BV1uP4y1U7ap/?p=4">985博后分享八年科研历程总结的SCI论文写作经验干货！</a></p><p><strong>2、综述写作</strong></p><p>参考：</p><p><a href="https://www.bilibili.com/video/BV1kK411m7A9/?spm_id_from=333.337.search-card.all.click">实战流程-1：迈出综述写作第一步</a></p><p><a href="https://www.bilibili.com/video/BV1Uy4y1D7jR/?spm_id_from=333.337.search-card.all.click">实战流程-2：如何“抄”综述框架</a></p><h2 id="二-毕业论文（大论文）"><a href="#二-毕业论文（大论文）" class="headerlink" title="(二) 毕业论文（大论文）"></a>(二) 毕业论文（大论文）</h2><p>参考：</p><p><a href="https://www.bilibili.com/video/av862205898/">硕博毕业论文详细写作指南！</a></p><p><a href="https://www.bilibili.com/video/BV1XR4y1e72N/?spm_id_from=333.337.search-card.all.click">发了两篇一区SCI的我，因为盲审没过，被延毕了。。。</a></p><h2 id="三-基金申请事项"><a href="#三-基金申请事项" class="headerlink" title="(三) 基金申请事项"></a>(三) 基金申请事项</h2><ol><li>协助导师完成一份基金标书</li><li>如何申请国自然<ul><li>参考：<a href="https://www.bilibili.com/video/BV1HD4y1K7Bm/?spm_id_from=333.337.search-card.all.click">国自然标书辛苦准备半年，被拒只需5分钟！！！</a></li></ul></li><li>如何写国自然结题报告<ul><li>参考：<a href="https://www.bilibili.com/video/BV1A841177cz/">如何写国家自然科学标书结题报告？</a></li></ul></li></ol><p><strong>参考：</strong></p><ol><li><a href="https://www.bilibili.com/video/BV16m421G71Z?vd_source=6c9d491042982f363ce749e6f5504631">硕士研究生科研培训01</a></li><li><a href="%E3%80%90%E5%AD%A6%E4%BC%9A%E8%BF%99%E4%BA%9B%E7%A7%91%E7%A0%94%E6%8A%80%E8%83%BD%E5%90%8E%EF%BC%8C%E6%88%91%E5%BC%80%E5%A7%8B%E5%8F%8D%E5%90%91push%E8%80%81%E6%9D%BF%E8%BF%9B%E5%BA%A6%E5%A4%AA%E6%85%A2%E3%80%82%E3%80%82%E3%80%82%E3%80%91https://www.bilibili.com/video/BV1to4y1r7da?vd_source=6c9d491042982f363ce749e6f5504631">学会这些科研技能后，我开始反向push老板进度太慢。。。</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Skills </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sublime Text4 4169 激活(各平台) + 安装中文语言包</title>
      <link href="/20240807/"/>
      <url>/20240807/</url>
      
        <content type="html"><![CDATA[<p>此教程用于各主流系统下Sublime Text4 <strong>4169版本</strong>的安装和激活。（4代相近版本同理）</p><p>无需安装其他软件，无需下载替换文件，无需<a href="https://so.csdn.net/so/search?q=%E6%B3%A8%E5%86%8C%E6%9C%BA&spm=1001.2101.3001.7020">注册机</a>等。</p><p>sublime text官网： <a href="https://www.sublimetext.com/">https://www.sublimetext.com</a></p><h1 id="一-、Windows激活方法"><a href="#一-、Windows激活方法" class="headerlink" title="一 、Windows激活方法"></a>一 、Windows激活方法</h1><p>1、sublime text安装地址：<a href="https://www.sublimetext.com/download">Download - Sublime Text</a></p><p>2、打开程序和其目录</p><ul><li><p>打开sublime text应用程序</p></li><li><p>默认安装路径：<code>C:\Program Files\Sublime Text</code></p></li><li><p>找到默认安装路径中的“<strong>sublime_text.exe</strong>”文件，<strong>拖动</strong>到sublime text<strong>应用程序窗口</strong></p></li></ul><p>3、内容修改</p><p><strong>Ctrl + F 搜索：</strong></p><pre><code>80 7805 000f 94c1</code></pre><p><strong>Ctrl + H 替换：</strong></p><pre><code>c6 4005 0148 85c9</code></pre><p>4、替换文件</p><ul><li>点击“另存为”，保存文件到本地，文件名设定为<code>sublime_text.exe</code></li><li>备份<strong>原</strong><code>sublime_text.exe</code>文件（修改为如<code>sublime_text_bk.exe</code>）</li><li>将新保存的<code>sublime_text.exe</code>复制到原sublime text 4安装目录中</li></ul><p>点击<strong>sublime text菜单栏</strong> Help—&gt;About Sublime Text 可检测激活状态。</p><h1 id="二、Ubuntu-Debain激活方法"><a href="#二、Ubuntu-Debain激活方法" class="headerlink" title="二、Ubuntu&#x2F;Debain激活方法"></a>二、Ubuntu&#x2F;Debain激活方法</h1><p>1、sublime text安装地址：<a href="https://www.sublimetext.com/docs/linux_repositories.html">Download - Sublime Text</a></p><p>注：我是用星火应用商店安装的，安装路径为：<code>/opt/sublime_text</code></p><p>2、打开程序和其目录</p><ul><li><p>打开sublime text应用程序</p></li><li><p>默认安装路径：<code>/opt/sublime_text</code></p></li><li><p>找到“**&#x2F;opt&#x2F;sublime_text&#x2F;sublime_text<strong>”文件，</strong>拖动<strong>到sublime text</strong>应用程序窗口**</p></li></ul><p>3、内容修改</p><p><strong>Ctrl + F 搜索：</strong></p><pre><code>80 7805 000f 94c1</code></pre><p><strong>Ctrl + H 替换：</strong></p><pre><code>c6 4005 0148 85c9</code></pre><p>4、替换文件</p><ul><li>点击“另存为”，保存文件到本地，文件名设定为<code>sublime_text</code></li><li>备份<strong>原</strong><code>sublime_text</code>文件（修改为如<code>sublime_text_bk</code>）</li><li>将新保存的<code>sublime_text</code>复制到原sublime text 4安装目录中</li></ul><p>点击<strong>sublime text菜单栏</strong> Help—&gt;About Sublime Text 可检测激活状态。</p><h1 id="三、Mac激活方法"><a href="#三、Mac激活方法" class="headerlink" title="三、Mac激活方法"></a>三、Mac激活方法</h1><ul><li>sublime text安装地址：<a href="https://www.sublimetext.com/download">Download - Sublime Text</a></li><li>下载并安装APP“Hex Fiend”，<a href="https://hexfiend.com/">Hex Fiend, a fast and clever hex editor for macOS</a></li><li>打开Sublime Text所在的目录：</li></ul><blockquote><p>Sublime Text -&gt; Show Package Contents(右键) -&gt; Mac OS -&gt; Sublime_text</p></blockquote><ul><li><p>打开Hex Fiend，将Sublime_text拖至Hex Fiend中</p></li><li><p>在搜索框中输入<code>80 7805 000f 94c1</code>，找到后，替换为<code>c6 4005 0148 85c9</code></p></li><li><p>保存后，退出</p></li><li><p>在Terminal下，进入Sublime Text目录：</p><pre><code>codesign --remove-signature Sublime\ Text.app/</code></pre></li></ul><p>点击<strong>sublime text菜单栏</strong> Help—&gt;About Sublime Text 可检测激活状态。</p><h1 id="四、软件安装中文语言包"><a href="#四、软件安装中文语言包" class="headerlink" title="四、软件安装中文语言包"></a>四、软件安装中文语言包</h1><p>1、点击菜单栏Tools—&gt;Install Package Control 跳出弹框点击 确定按钮</p><p>2、点击菜单栏 Preferences – Package Control</p><p>3、弹框中输入 install package，选中 Package Control: Install Package 然后等待弹窗(可能需要耐心等待一段时间)</p><p>4、等出现如下弹窗后, 在弹窗的搜索框中输入 ChineseLocalizations , 点击出现的选项</p><p>5、等待一段时间，如果出现了如下画面则汉化步骤全部完成</p><p>6、后续若需要更改，可点击菜单栏 帮助—&gt;Language 选择进行更改。</p><p><strong>参考</strong></p><ul><li><a href="https://zhuanlan.zhihu.com/p/603640137">Sublime Text 4143 激活码</a></li><li><a href="https://blog.csdn.net/u010151855/article/details/135951058">Sublime Text4 4169 安装激活</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件操作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Sublime_Text </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下挂载硬盘并设置开机自动挂载</title>
      <link href="/20240806/"/>
      <url>/20240806/</url>
      
        <content type="html"><![CDATA[<p>首先介绍如何设置<strong>挂载硬盘</strong>并设置<strong>开机自动挂载</strong>，默认硬盘<strong>已分区</strong>。</p><p>如硬盘需要格式化分区操作，请看文末部分，设置<strong>磁盘格式化</strong>，<strong>磁盘管理</strong>、<strong>挂载新硬盘</strong>。</p><h1 id="一、查看硬盘挂载情况"><a href="#一、查看硬盘挂载情况" class="headerlink" title="一、查看硬盘挂载情况"></a>一、查看硬盘挂载情况</h1><p>1、整体磁盘空间使用情况</p><p>首先列出文件系统的整体磁盘空间使用情况。可以用来查看磁盘已被使用多少空间和还剩余多少空间。</p><pre><code>df -h</code></pre><p>2、查看硬盘绑定信息</p><p>列出所有可用块设备的信息，显示他们之间的依赖关系</p><pre><code>lsblk</code></pre><p>3、查询设备文件系统类型</p><p>查询所有设备的文件系统类型，如：硬盘的文件类型 <code>TYPE=&quot;ext4&quot;</code></p><pre><code>blkid</code></pre><h1 id="二、自动挂载"><a href="#二、自动挂载" class="headerlink" title="二、自动挂载"></a>二、自动挂载</h1><p>下面开始配置开机自动挂载，确保你的硬盘已经正确连接到Linux系统，并且系统已经识别并分配了设备名称（如 <code>/dev/sdb1</code>）。</p><p>创建一个用于挂载的目录。可以选择任意目录作为挂载点，例如 <code>/mnt/mydisk</code>。</p><p>打开终端，编辑 <code>/etc/fstab</code> 文件。使用 root 或具有管理员权限的用户进行操作。</p><p>在 &#x2F;etc&#x2F;fstab 文件中添加一行，指定要挂载的设备、挂载点、文件系统类型以及其他选项。例如：</p><pre><code>/dev/sdb1  /mnt/mydisk  ext4  defaults  0  0 </code></pre><p>这行表示将 <code>/dev/sdb1</code> 设备挂载到 <code>/mnt/mydisk</code> 目录，文件系统类型为 ext4，并使用默认的挂载选项。</p><p>保存并关闭 <code>/etc/fstab</code> 文件（后续开机登录时将会自动挂载）。</p><p>运行以下命令，使更改生效：</p><pre><code>systemctl daemon-reload</code></pre><p>通过<code>df -h</code> 查看是否挂载上，如果没有问题，再重启机器。</p><p><strong>&#x2F;etc&#x2F;fstab文件格式说明</strong></p><table><thead><tr><th align="left">&#x2F;vdb1</th><th align="left">ext4</th><th align="left">defaults</th><th align="left">0</th><th align="left">0</th></tr></thead><tbody><tr><td align="left">挂载点</td><td align="left">文件系统类型</td><td align="left">挂载选项</td><td align="left">是否备份</td><td align="left">是否检测</td></tr></tbody></table><h1 id="三、手动挂载"><a href="#三、手动挂载" class="headerlink" title="三、手动挂载"></a>三、手动挂载</h1><p>下面开始配置手动挂载，确保你的硬盘已经正确连接到Linux系统，并且系统已经识别并分配了设备名称（如 <code>/dev/sdb1</code>）。</p><p>挂载需要将硬盘挂载在挂载点上（一个文件夹），</p><p>但是mount并<strong>不会</strong>创建文件夹，所以在使用mount命令之前首先创建挂载点</p><pre><code>mkdir /mnt/storage</code></pre><p>将新分区 &#x2F;dev&#x2F;vdb1 挂载到&#x2F;mnt&#x2F;storage挂载点下</p><pre><code>mount /dev/vdb1 /mnt/storage/</code></pre><p>查看是否挂载成功</p><pre><code>mount</code></pre><h1 id="四、硬盘分区"><a href="#四、硬盘分区" class="headerlink" title="四、硬盘分区"></a>四、硬盘分区</h1><p>如有一块 3T 的磁盘 <strong>vdb</strong>，我们现在将它进行磁盘分区</p><pre><code>fdisk /dev/vdb</code></pre><p>输入 **m **获取帮助，**p **查看分区表</p><p>当前分区里面没有任何信息，继续操作，输入 <strong>n</strong> 创建一个新的分区</p><p>选择默认 **p **选择主分区  <strong>e</strong> 扩展分区 直接默认回车就是选择 <strong>p</strong></p><p>输入分区号，默认从1开始，默认<strong>回车</strong></p><p>sector 起始扇区 (2048-4294967295, 默认 2048)：默认<strong>回车</strong></p><p>+ 多少扇区 或多大空间，不会计算的话 可以 写 <strong>+1G</strong> 或者 选择默认<strong>回车</strong></p><p>最后输入<strong>w</strong> 保存</p><p>查看，新建的区分已显示出来</p><p>格式化分区vdb1 </p><pre><code>mkfs.ext4 /dev/vdb1</code></pre><p>创建一个挂载点</p><pre><code>mkdir /vdb1</code></pre><p>挂载</p><pre><code>mount /dev/vdb1 /vdb1</code></pre><p><strong>参考</strong></p><ul><li><a href="https://www.cnblogs.com/sirdong/p/11969148.html">Linux下mount挂载新硬盘和开机自动挂载</a></li><li><a href="https://blog.csdn.net/bill_wjn/article/details/106013425">linux挂载硬盘步骤（简单易用，直接上手，无废话）</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 系统相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>修改GitHub项目的语言类型</title>
      <link href="/20240805/"/>
      <url>/20240805/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>众所周知，当我们使用<code>Github</code> 的时候，我们的项目语言很大程度上会影响别人的搜索和自己的强迫症被逼犯，那么我们是不是可以对我们的<code>Github</code> 项目的编程语言进行修改或者进行自主标明呢？</p><h1 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h1><blockquote><p>在<code>Github</code> 中，采用<code>Linguist</code> 来自动识别<strong>代码语言</strong> 。<br>我们要做的就是对<code>linguist-language</code> 进行赋值，强制它识别某一种语言文件为我们想要它显示的语言。</p></blockquote><p>我们需要在仓库的根目录下添加<code>.gitattributes</code>文件:并写入</p><pre><code>*.js linguist-language=python*.css linguist-language=python*.html linguist-language=python*.xml linguist-language=python</code></pre><p>这行代码的意思是：将文件中所有的<code>.js</code>、<code>.css</code>、<code>.html</code>、<code>.xml</code> 结尾的文件固定为<code>python</code> 语言。当然，我们也可以指定其它文件和其它语言。</p><p><strong>参考</strong></p><ul><li><a href="https://zhuanlan.zhihu.com/p/130319542">修改Github仓库中项目语言类型</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo搭建网站并一键部署至GitHub</title>
      <link href="/20240804/"/>
      <url>/20240804/</url>
      
        <content type="html"><![CDATA[<p>hexo搭建网站步骤以官网为准！！！</p><p>hexo官网文档：<a href="https://hexo.io/zh-cn/docs/setup">https://hexo.io/zh-cn/docs/setup</a></p><p>hexo如果还未安装，请参考hexo官网文档完成安装操作</p><h1 id="一、Hexo建站"><a href="#一、Hexo建站" class="headerlink" title="一、Hexo建站"></a>一、Hexo建站</h1><p>安装 Hexo 完成后，请执行下列命令，Hexo 将会在指定文件夹中新建所需要的文件。</p><p>如果已经在目标目录(文件夹)下，可以省略<code>&lt;folder&gt;</code></p><pre><code>hexo init &lt;folder&gt;</code></pre><p>新建完成后，指定文件夹的目录如下：</p><pre><code>.├── _config.yml   #网站的 配置 信息，您可以在此配置大部分的参数。├── package.json#应用程序的信息。EJS, Stylus 和 Markdown 渲染引擎 已默认安装，您可以自由移除。├── scaffolds#模版 文件夹。当您新建文章时，Hexo 会根据 scaffold 来创建文件。Hexo 的模板是指在新建的文章文件中默认填充的内容。例如，如果您修改 scaffold/post.md 中的 Front-matter 内容，那么每次新建一篇文章时都会包含这个修改。├── source|   ├── _drafts|   └── _posts#资源文件夹是存放用户资源的地方。除 _posts 文件夹之外，开头命名为 _ (下划线)的文件 / 文件夹和隐藏的文件将会被忽略。Markdown 和 HTML 文件会被解析并放到 public 文件夹，而其他文件会被拷贝过去。└── themes#主题 文件夹。Hexo 会根据主题来生成静态页面。</code></pre><h1 id="二、在-GitHub-Pages-上部署-Hexo"><a href="#二、在-GitHub-Pages-上部署-Hexo" class="headerlink" title="二、在 GitHub Pages 上部署 Hexo"></a>二、在 GitHub Pages 上部署 Hexo</h1><p>Hexo 提供了快速方便的一键部署功能，让您只需一条命令就能将网站部署到服务器上。</p><h2 id="1、生成SSH密钥"><a href="#1、生成SSH密钥" class="headerlink" title="1、生成SSH密钥"></a>1、<strong>生成SSH密钥</strong></h2><p>设置帐号和名称</p><pre><code>git config --global user.email &quot;you@example.com&quot;git config --global user.name &quot;Your Name&quot;</code></pre><p>windows系统</p><p>打开<a href="https://git-scm.com/download/win">Git</a>-Bash，输入以下命令，然后一直回车和按【y】即可：</p><pre><code>ssh-keygen -t rsa -C &quot;GitHub注册的邮箱账号&quot;</code></pre><p>最后获取到的<code>ssh</code>认证在<code>C:\Users\yourname\.ssh</code>中</p><p>2.Linux系统</p><pre><code>cd ~ssh-keygen -t rsa -C &quot;Github的注册邮箱帐号&quot;</code></pre><ol><li>在.ssh目录会有两个文件<strong>id_rsa</strong>和<strong>id_rsa.pub</strong>,&#x20;</li><li>接下来要将<strong>id_rsa.pub</strong>里面的所有内容添加到github里，即SSH keys里的Key，Title随便填，</li><li>然后Add SSH key</li></ol><h2 id="2、测试连接"><a href="#2、测试连接" class="headerlink" title="2、测试连接"></a>2、测试连接</h2><p>通过SSH方式测试</p><pre><code>ssh -T git@github.com</code></pre><p>连接成功显示：</p><pre><code>$ ssh -T git@github.comThe authenticity of host &#39;github.com (198.18.0.38)&#39; can&#39;t be established.ECDSA key fingerprint is SHA256:p2QAMXNIC1TJOttrVc98/R1BUFWu3/LiyKgUfQM.Are you sure you want to continue connecting (yes/no/[fingerprint])? yesWarning: Permanently added &#39;github.com,198.18.0.38&#39; (ECDSA) to the list of known hosts.Hi xxxx/xxxx.github.io! You&#39;ve successfully authenticated, but GitHub does not provide shell access.</code></pre><h2 id="3、一键部署插件Git"><a href="#3、一键部署插件Git" class="headerlink" title="3、一键部署插件Git"></a>3、一键部署插件Git</h2><p>1.安装 <a href="https://github.com/hexojs/hexo-deployer-git">hexo-deployer-git</a></p><pre><code>npm install hexo-deployer-git --save</code></pre><p>2.修改**_config.yml**配置</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: # https://github.com/xxxx/xxxx.github.io.git   # 库（Repository）地址 </span><br><span class="line">  branch: [branch]# 分支名称   例：main</span><br><span class="line">  </span><br></pre></td></tr></table></figure><p>3、生成站点文件并推送至远程库。执行</p><pre><code> hexo clean &amp;&amp; hexo deploy</code></pre><p><strong>参考</strong></p><ol><li><a href="https://zhuanlan.zhihu.com/p/188625365">Ubuntu下GitHub+Hexo搭建博客教程一</a></li><li><a href="https://blog.csdn.net/qq_36761831/article/details/88725670">Git 远程仓库操作（生成SSH key、测试连接、添加到远程仓库）</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Xfce桌面系统中 文件夹菜单栏消失不见的恢复办法</title>
      <link href="/20240803/"/>
      <url>/20240803/</url>
      
        <content type="html"><![CDATA[<p>适用于各版本的 xfce 桌面系统</p><p>目前在用系统为Zorin-OS-Lite-16.3，高度定制化的xfce桌面系统。</p><p>菜单栏隐藏后不知道如何显示</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">方法：按Ctrl + M 组合键</span><br></pre></td></tr></table></figure><p>其他部分功能按键</p><pre><code>## 文件Ctrl + T 新标签Ctrl + N 新窗口Ctrl + W 关闭标签Ctrl + Q 关闭窗口## 视图Ctrl + R 刷新Ctrl + B 侧边栏-快捷方式Ctrl + E 侧边栏-树形Ctrl + M 菜单栏Ctrl + H 显示隐藏文件Ctrl + 0 正常大小Ctrl + 1 图标视图Ctrl + 2 列表视图Ctrl + 3 紧凑视图</code></pre><p><strong>参考</strong></p><ul><li><a href="https://linux.zone/148">Thunar菜单栏消失不见的恢复办法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 系统相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> Xfce </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用APT安装最新版本 nodejs npm</title>
      <link href="/20240802/"/>
      <url>/20240802/</url>
      
        <content type="html"><![CDATA[<p>主要应用系统：Ubuntu&#x2F;Debain系列</p><p>&#x9;以及其他安装有apt包管理工具的系统</p><p>Nodejs官网：<a href="https://nodejs.org/">https://nodejs.org/</a></p><h1 id="一、apt安装nodejs和npm"><a href="#一、apt安装nodejs和npm" class="headerlink" title="一、apt安装nodejs和npm"></a>一、apt安装nodejs和npm</h1><p>1、刷新apt</p><pre><code>sudo apt update</code></pre><p>2、使用apt安装nodejs和npm</p><pre><code>sudo apt install nodejs npm</code></pre><p>3、升级npm（如果报错则先进行下一步：升级Node版本）</p><pre><code>sudo npm install npm -g</code></pre><h1 id="二、使用-n-Node版本管理工具升级到-最新版"><a href="#二、使用-n-Node版本管理工具升级到-最新版" class="headerlink" title="二、使用 n Node版本管理工具升级到 最新版"></a>二、使用 <code>n</code> Node版本管理工具升级到 最新版</h1><p>1、下载nNode管理工具</p><pre><code>sudo npm install n -g</code></pre><p>2、下载最新稳定版</p><pre><code>sudo n stable</code></pre><p><strong>或</strong>下载最新版</p><pre><code>sudo n lastest</code></pre><p>3、查看已下载的版本</p><pre><code>sudo n ls</code></pre><p>4、切换Node版本（通过步骤3查看版本号）</p><pre><code> sudo n v20.xx.x</code></pre><p><strong>参考</strong></p><ul><li><a href="https://blog.csdn.net/weixin_55719805/article/details/128094550">Ubuntu安装最新版本NodeJs和Npm的方法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件操作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python创建虚拟环境，批量导出或安装pip包到 .txt文件</title>
      <link href="/20240801/"/>
      <url>/20240801/</url>
      
        <content type="html"><![CDATA[<h1 id="Python虚拟环境的应用"><a href="#Python虚拟环境的应用" class="headerlink" title="Python虚拟环境的应用"></a>Python虚拟环境的应用</h1><p><code>venv</code> 模块支持创建轻量的“虚拟环境”，每个虚拟环境将拥有它们自己独立的安装在其 <a href="https://docs.python.org/zh-cn/3/library/site.html#module-site">site</a> 目录中的 Python 软件包集合。 虚拟环境是在现有的 Python 安装版基础之上创建的，这被称为虚拟环境的“基础”Python，并且还可选择与基础环境中的软件包隔离开来，这样只有在虚拟环境中显式安装的软件包才是可用的。</p><h2 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h2><p>通过执行 <code>venv</code> 指令来创建一个 <a href="https://docs.python.org/zh-cn/3/library/venv.html#venv-def">虚拟环境</a>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv /path/to/new/virtual/environment</span><br></pre></td></tr></table></figure><p>简短命令：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m venv venv</span><br></pre></td></tr></table></figure><h2 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a>激活虚拟环境</h2><p>在Windows系统和Linux系统中激活命令类似。</p><p><strong>在 macOS 和 Linux 上激活虚拟环境</strong></p><p>在终端中执行以下命令来激活虚拟环境：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source &lt;虚拟环境名称&gt;/bin/activate</span><br></pre></td></tr></table></figure><p>例如，如果虚拟环境名称是 <code>myenv</code>，则应执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source myenv/bin/activate</span><br></pre></td></tr></table></figure><p><strong>在 Windows 上激活虚拟环境：</strong></p><p>在命令提示符（cmd）中执行以下命令来激活虚拟环境：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;虚拟环境名称&gt;\Scripts\activate</span><br></pre></td></tr></table></figure><p>例如，如果虚拟环境名称是 <code>myenv</code>，则应执行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myenv\Scripts\activate</span><br></pre></td></tr></table></figure><h2 id="退出虚拟环境"><a href="#退出虚拟环境" class="headerlink" title="退出虚拟环境"></a>退出虚拟环境</h2><p>要退出虚拟环境，只需在终端或命令提示符中运行以下命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure><p>这将会将从当前虚拟环境中退出，恢复到系统的全局 Python 环境中。</p><h1 id="Python批量导出或安装pip包文件"><a href="#Python批量导出或安装pip包文件" class="headerlink" title="Python批量导出或安装pip包文件"></a>Python批量导出或安装pip包文件</h1><p>python新迁移环境后需要将环境完善，如果涉及pip包太多则需要批量设置。</p><h2 id="批量导出类库包到-txt文件"><a href="#批量导出类库包到-txt文件" class="headerlink" title="批量导出类库包到.txt文件"></a>批量导出类库包到.txt文件</h2><p>Python批量导出项目中本地所安装的<strong>所有</strong>类库包到.txt文件，命令如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; 你的文件名.txt</span><br></pre></td></tr></table></figure><p>如果txt文件名为requirements,则输入命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; requirements.txt</span><br></pre></td></tr></table></figure><p>这种方式，会将环境中的依赖包全都加入，如果使用的全局环境，则下载的所有包都会在里面。因为这种方法只适用于单虚拟环境。</p><p>导出的文件内容如该示例：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">beautifulsoup4==4.12.3</span><br><span class="line">certifi==2024.7.4</span><br><span class="line">charset-normalizer==3.3.2</span><br><span class="line">et-xmlfile==1.1.0</span><br><span class="line">idna==3.7</span><br><span class="line">jsonpath==0.82.2</span><br><span class="line">openpyxl==3.1.5</span><br><span class="line">requests==2.32.3</span><br><span class="line">soupsieve==2.5</span><br><span class="line">urllib3==2.2.2</span><br></pre></td></tr></table></figure><h2 id="批量安装-txt文件中的类库包"><a href="#批量安装-txt文件中的类库包" class="headerlink" title="批量安装.txt文件中的类库包"></a>批量安装.txt文件中的类库包</h2><h4 id="首先进入到存放txt文件的目录，然后在cmd中执行如下命令"><a href="#首先进入到存放txt文件的目录，然后在cmd中执行如下命令" class="headerlink" title="首先进入到存放txt文件的目录，然后在cmd中执行如下命令"></a>首先进入到存放txt文件的目录，然后在cmd中执行如下命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r 你的文件名.txt</span><br></pre></td></tr></table></figure><p>如果txt文件名为requirements,则输入命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>如果是在虚拟环境中安装，则先激活虚拟环境后再执行 <strong>pip install -r requirements.txt</strong></p></blockquote><p><strong>Windows系统下操作步骤</strong></p><ul><li><p>打开cmd，使用cd命令进入目标文件路径，然后执行 <strong>pip install -r requirements.txt</strong></p></li><li><p>在目标文件目录下使用ctrl+shift+鼠标右键，选择在此处打开Powershell窗口。</p></li></ul><p>然后执行 <strong>pip install -r requirements.txt</strong></p><p><strong>参考</strong></p><ul><li><p><a href="https://blog.csdn.net/xiangxi1204/article/details/139497252">Python Virtualenv：创建独立的 Python 开发环境</a></p></li><li><p><a href="https://docs.python.org/zh-cn/3/library/venv.html#creating-virtual-environments">venv — 虚拟环境的创建</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语口语练习文章</title>
      <link href="/20240602/"/>
      <url>/20240602/</url>
      
        <content type="html"><![CDATA[<p>👇🏻👇🏻👇🏻👇🏻👇🏻👇🏻👇🏻👇🏻👇🏻👇🏻</p><div calss='anzhiyu-tag-link'><a class="tag-Link" target="_blank" href="https://www.dyeddie.top/music/?id=3778678&server=netease">    <div class="tag-link-tips">引用站外地址</div>    <div class="tag-link-bottom">        <div class="tag-link-left" style="">          <i class="anzhiyufont anzhiyu-icon-link" style=""></i>        </div>        <div class="tag-link-right">            <div class="tag-link-title">英语文章对应听力材料</div>            <div class="tag-link-sitename">Passage_1 to passage_30</div>        </div>        <i class="anzhiyufont anzhiyu-icon-angle-right"></i>    </div>    </a></div><p>👆🏻👆🏻👆🏻👆🏻👆🏻👆🏻👆🏻👆🏻👆🏻👆🏻</p><blockquote><p>链接打开后点击<strong>左边第一个</strong>悬浮按钮，进行<strong>切换歌单</strong>，切换后为文章对应的听力材料</p></blockquote><h1 id="PASSAGE-1"><a href="#PASSAGE-1" class="headerlink" title="PASSAGE 1"></a>PASSAGE 1</h1><p><strong>Success in Life</strong> </p><p>​Success means different things for different people. Some may equate it with fame, some with wealth and still some with accomplishments.  </p><p>​For me, it means fulfilling one’s dreams. Whatever your dreams are, you have a goal there and then focus all your attention on it. </p><p>​Dreams bring you hope and happiness. In the process of struggling for it, you cry, sweat, complain or even curse, but the joy of harvesting makes you forget all the pains and troubles you have gone through. So an old proverb says that the sweetest fruit is one that has undergone the bitterest ordeal. </p><p>​There are several keys to success. First, your goal must be practical and practicable. If you set your goal too high, chances are that you will never attain it. Next, you have to make a plan of doing it. You can take some steps to realize it. Since the process is quite tough, you need to be diligent, patient and persevering. </p><p>​Even if you meet with some difficulties or frustrations, just take them in your stride. You can always tell yourself that there is nothing insurmountable. With this will and determination, success is sure to wait for you at the end of the tunnel!</p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-1">passage_1</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-2"><a href="#PASSAGE-2" class="headerlink" title="PASSAGE 2"></a>PASSAGE 2</h1><p><strong>Learning：a Lifelong Career</strong> </p><p>​As food is to the body, so is learning to the mind. Our bodies grow and muscles develop with the intake of food. Likewise, we should keep learning day by day to maintain our mental power and expand our intellectual capacity. </p><p>​Constant learning supplies us with fuel for driving us to sharpen our power of reasoning, analysis, and judgement. Continuous learning is the best way to keep pace with the times in the information age. </p><p>​It is a common misunderstanding to regard school as the only place for the acquisition of knowledge. On the contrary, learning should be an endless process, from the cradle to the grave. With the world changing so fast, to stop learning for just a few days will make a person lag behind.  </p><p>​Lack of learning will inevitably lead to the stagnation of the mind, or even worse, its fossilization. Therefore, to stay mentally young, we have to take learning as a lifelong career. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-2">passage_2</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-1">passage_1</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-3"><a href="#PASSAGE-3" class="headerlink" title="PASSAGE 3"></a>PASSAGE 3</h1><p><strong>JOGGING</strong> </p><p>​Every day, in all kinds of weather, thousands of men and women go jogging. Why has jogging—running slowly for long distances—become so popular? Most joggers like it because they have heard it is a very good exercise. Jogging makes the heart stronger and helps people lose weight. It can also help them feel better about themselves.  </p><p>​How fast should you go? Jog with a friend and talk to each other as you run. If you have difficulty talking, you are going too fast. How far should you jog? Remember not to go too far too soon. In fact you should walk, not run, the first few times. Then do some short jogs, but no more than what you can do comfortably.  </p><p>​After that, increase your distance a quarter or half of a mile every two weeks or so. Maybe in a few years, you, too, can run in a marathon like thousands of other people do. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-3">passage_3</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-2">passage_2</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-4"><a href="#PASSAGE-4" class="headerlink" title="PASSAGE 4"></a>PASSAGE 4</h1><p><strong>Body Language</strong> </p><p>​Americans feel that physical contact can be important in breaking down barriers between people. Such physical acts as hugging, shaking hands and kissing in public are seen as displays of affection.  </p><p>​In China, on the other hand, this is not common because physical contact is not the custom. Interestingly enough, most Chinese who spend some time in America come to like hugging; they learn that Americans use body language to express feelings. It is a way of saying “I like you and I care about you.”  </p><p>​Shaking hands is another traditional way of greeting someone in America. Shaking hands is a more formal way of greeting than hugging. Whereas hugging shows closeness, shaking hands means that there is a little more formality in the relationship.  </p><p>​Western people usually shake hands when meeting someone for the first time, or when they see someone again after a certain amount of time has passed. In Japan, people traditionally bow as a form of greeting. Other countries have different customs, like patting someone on the back or kissing. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-4">passage_4</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-3">passage_3</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-5"><a href="#PASSAGE-5" class="headerlink" title="PASSAGE 5"></a>PASSAGE 5</h1><p><strong>Sleep</strong> </p><p>​Scientists suggest that today, most adults get up to two hours’ less sleep than they need. As a result, those lacking in sleep are forgetful, tired, less productive and grumpy. Sleep is the time when our bodies and minds are rested and restored.  </p><p>​There are two types of sleep: Rapid Eye Movement and Non-REM. We need both types to be healthy. Rapid Eye Movement sleep is when we dream. Our brains are active, and our eyes have fast short movements, as if watching something. This type of sleep helps restore our mind.  </p><p>​Non-REM sleep is when our brain and body slow down. Our heart rate slows and our breathing is regular. This type of sleep helps restore our body. We have twice as much Non-REM sleep as Rapid Eye Movement sleep each night. To get better sleep, have a regular time for sleeping and a routine before you go to bed.  </p><p>​This will give your body the signal that it is time to sleep. Make sure the bed and bedroom are quiet and comfortable. If your bed is too hard or too soft, if you are too cold or too hot, you will not sleep well. For two hours before going to bed do not drink coffee or alcohol, or do not smoke or exercise. All these activities stimulate your body and make sleep difficult. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-5">passage_5</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-4">passage_4</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-1">passage_1</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-6"><a href="#PASSAGE-6" class="headerlink" title="PASSAGE 6"></a>PASSAGE 6</h1><p><strong>Swimming</strong> </p><p>​Swimming is a competitive sport played around the world and on many different levels, including the Olympics. It’s also an easy sport to do on your own, both for fun and for exercise. Many doctors and athletes agree that swimming is one of the best workouts you can give your body, because it works almost on all of your major muscle groups at the same time.  </p><p>​Since you’re floating in the water and not in contact with any hard surfaces, there’s less pressure on your joints and bones. You’re less likely to suffer injuries than with other sports.  </p><p>​There are many ways to push yourself and get better as a swimmer. If you’re into speed, you can race the clock and see how long it takes you to do a certain number of laps. If you’re into endurance, you can slow down and see how many laps you can do before you have to take a rest.  </p><p>​Many teenagers like swimming best. One of the students says, “The best part about swimming is when you beat your time. It doesn’t even matter if you lose the race as long as you tried your best; you can walk away with a smile on your face. When you’re in the water you forget that people may be watching and all you care about is getting to the other side of the pool and touching the wall. You can just let go and fly.” </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-6">passage_6</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-5">passage_5</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-2">passage_2</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-7"><a href="#PASSAGE-7" class="headerlink" title="PASSAGE 7"></a>PASSAGE 7</h1><p><strong>Dragon Boat Festival</strong> </p><p>​The Dragon Boat Festival is one of three major Chinese holidays, along with the Spring and Moon Festivals. Of the three, it is possibly the oldest, dating back to the Warring States Period in 227 B.C. The festival commemorates Qu Yuan, a minister in the service of the Chu Emperor.  </p><p>​Despairing over corruption at court, Qu threw himself into a river. Townspeople jumped into their boats and tried in vain to save him. Then, hoping to distract hungry fish from his body, the people scattered rice on the water.  </p><p>​Over the years, the story of Qu’s demise transformed into the traditions of racing dragon boats and eating zongzi—-a kind of rice wrapped in bamboo leaves. The races have certainly captured the imagination of people from all over the world.  </p><p>​Every spring there are nearly 60 dragon boat races held outside of China in cities from Vancouver to Sydney. Canada alone has nearly 50 dragon boat teams and Germany has nearly 30.  </p><p>​So what is it about the Dragon Boat Festival that appeals to foreigners? “It’s an unusual sport,” says one racer from Germany. “It’s not like everybody’s doing it. That’s one of the reasons that there’s such great team spirit in a dragon boat team—-everybody feels like we’re doing something special.” And what about the zongzi? “Ehhh, they’re not bad, I guess,” he says. “Something of an acquired taste. I just haven’t really acquired it yet.” </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-7">passage_7</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-6">passage_6</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-3">passage_3</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-8"><a href="#PASSAGE-8" class="headerlink" title="PASSAGE 8"></a>PASSAGE 8</h1><p><strong>The Government of the United States</strong> </p><p>​These days, the American President is one of the most powerful men in the world. He can make war, or peace. He can touch the lives of millions in many different countries, but the President cannot do just what he wants. The Congress must agree first.  </p><p>​In some ways, the United States is like fifty small countries and not one large one. Every state has its own governor, its own police, and its own laws. For example, in some states you can buy an alcoholic drink when you are 18. In others, you must wait until you are 21.  </p><p>​The government of the whole country (the ‘federal’ government) works from Washington, the capital city. This is where Congress is based. Congress has two parts, the Senate, and the House of Representatives.  </p><p>​There are 100 members of the Senate (called Senators), two from each state. The President cannot act without the Senate’s agreement. The House of Representatives has 435 members. Like the senators, they can make new laws.  </p><p>​Americans choose a new President every four years. The election is a great occasion. It is serious business, of course, but the Americans make sure that it is fun too. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-8">passage_8</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-7">passage_7</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-4">passage_4</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-9"><a href="#PASSAGE-9" class="headerlink" title="PASSAGE 9"></a>PASSAGE 9</h1><p><strong>Globalization</strong> </p><p>​In the years after World War Il, the world greatly changed. Much of this was due to new technology. For instance, the jet was developed. This increased the speed that people could travel. There were also advances in telecommunications. Computers and the Internet were invented. It became much easier for people to communicate with others all around the world. This has led to the spread of globalization.  </p><p>​Basically, the world is becoming a smaller place. In the past, what happened in one country rarely affected other countries. Or it took a long time for any effects to occur. But the world is different today. Because of globalization, what happens in one part of the world can affect places all around it.  </p><p>​Thanks to globalization, people can now do business more easily with those in other countries. When you go to the supermarket, you can see various foods from all of the different countries. This happens because of globalization. Also, people are learning more about other countries these days. This leads to more understanding about other countries. In the age of globalization, there has not been a single world war. And the world is becoming richer. Globalization has surely been good for the world. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-9">passage_9</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-8">passage_8</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-5">passage_5</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-10"><a href="#PASSAGE-10" class="headerlink" title="PASSAGE 10"></a>PASSAGE 10</h1><p><strong>Resources</strong> </p><p>​There are many kinds of resources on the Earth. Four of them are very important. They are renewable, nonrenewable, human, and capital resources.  </p><p>​Renewable resources can be used again and again. They can be replaced within a short time. Some energy resources are renewable. The energy from the sun, tides, water, and wind is renewable. Also, trees and animals are renewable. But humans still need to take good care of them. We should not waste them at all.  </p><p>​Nonrenewable resources are limited in supply. Once we use them, they disappear forever. They can’t be replaced. Many energy resources are like this. Coal, gas, and oil are nonrenewable.  </p><p>​Human resources are people and the skills they have. This also includes the knowledge and information that humans have.  </p><p>​People make products using renewable and nonrenewable resources. Machines are often used to produce goods. The machines and tools that are used to produce goods are called capital resources. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-10">passage_10</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-9">passage_9</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-6">passage_6</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-11"><a href="#PASSAGE-11" class="headerlink" title="PASSAGE 11"></a>PASSAGE 11</h1><p><strong>Caring for the Five Senses</strong> </p><p>​Everyone has five senses. The five senses are sight, hearing, smell, taste and touch. We use different body parts for different senses. We need to take care of the parts of our bodies that let us use our senses.  </p><p>​For example, you use your eyes for seeing. You should protect your eyes and have a doctor regularly check your eyesight. Don’t sit too close to the TV or computer monitor, and don’t read in the dark or in dim light. Never look directly at the sun or at very bright lights.  </p><p>​Your ears let you hear the things around you. You should clean your ears all the time. Don’t listen to loud music, and try to avoid places that are really loud. Protect your ears when you play sports.  </p><p>​Your nose cleans the air you breathe and lets you smell things. Avoid things that have very strong smells.  </p><p>​Your tongue helps you taste things you eat and drink. Your skin protects your body from germs and gives you sense of touch. Always wash your hands after blowing your nose, playing outside, or using the restroom. Protect your skin from sunburns. Use sunscreen to protect your skin from the sun. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-11">passage_11</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-10">passage_10</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-7">passage_7</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-1">passage_1</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-12"><a href="#PASSAGE-12" class="headerlink" title="PASSAGE 12"></a>PASSAGE 12</h1><p><strong>How Technology Helps People?</strong></p><p>​Nowadays, we live in an advanced world. We use many new inventions that people long ago never imagined. In the past, people could not regularly communicate with others. It took days ,weeks, or even months just to send a letter. There were no telephones. So people had to talk face to face.  </p><p>​Nowadays, we use cell phones to call anyone anywhere in the world. And we send email to people instantly thanks to the internet.  </p><p>​In the past, traveling short distances took a long time. People either walked or rode on a horse. Now ,most people own cars. They can drive long distances in short period of time. And people can even fly around the world on airplanes now.  </p><p>​In the past, people often died because of poor medical treatment. Even a toothache could sometimes kill a person! Now, vaccines protect people from disease. And doctors are making more and more discoveries every day. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-12">passage_12</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-11">passage_11</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-8">passage_8</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-2">passage_2</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-13"><a href="#PASSAGE-13" class="headerlink" title="PASSAGE 13"></a>PASSAGE 13</h1><p><strong>The American Way of Marriage</strong> </p><p>​“I do.” To Americans those two words carry great meaning. They can even change your life. Especially if you say them at your own wedding. Making wedding vows is like signing a contract, Now Americans don’t really think marriage is a business deal. But marriage is serious business.  </p><p>​It all begins with engagement. Traditionally, a young man asks the father of his sweetheart for permission to marry her. If the father agrees, the man later proposes to her. Often he tries to surprise her by “popping the question” in a romantic way. Sometimes the couple just decides together that the time is right to get married. The man usually gives his fiancée a diamond ring as a symbol of their engagement. They may be engaged for weeks, months or even years.  </p><p>​As the big day approaches, bridal showers and bachelor’s parties provide many useful gifts. Today many couples also receive counseling during engagement. This prepares them for the challenges of married life. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-13">passage_13</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-12">passage_12</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-9">passage_9</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-3">passage_3</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-14"><a href="#PASSAGE-14" class="headerlink" title="PASSAGE 14"></a>PASSAGE 14</h1><p><strong>How to run a company</strong> </p><p>​If you want to run a company well, you should remember that there are four general policies to follow with employees.  </p><p>​First, pay them what they are worth. When people begin with your company, they are rarely worth what they are paid, because you are investing in their future productivity. You should pay people low at the start, with higher payments once they are established and productive.  </p><p>​Second, you should make employees feel important. Build them positively and openly when praise is deserved. Temper this with negative motivation, pointing out where minor shortcomings are diluting the success that could be enjoyed. Avoid complacency by keeping employees slightly off-balance.  </p><p>​Third, make employees think for themselves. Sometimes by being nonspecific, you create the environment where employees adapt other ideas and learn how to apply them in the new situation.  </p><p>​Fourth, separate office life from social life. It is impossible to let your hair down with someone one night, and come down hard on them the next day.  </p><p>​You should also remember that if you have to fire employees, take a creative approach. Try to find a job for them somewhere else, and let the other company steal them away. However, if extreme action is called for, act decisively while respecting the feeling of the employee.  </p><p>​Never expect from your employees anything that you are not willing to do yourself. It’s more than a cliché—it’s sound business practice. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-14">passage_14</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-13">passage_13</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-10">passage_10</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-4">passage_4</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-15"><a href="#PASSAGE-15" class="headerlink" title="PASSAGE 15"></a>PASSAGE 15</h1><p><strong>Importance of communication</strong> </p><p>​Communication is extremely important in any relationship. Importance of Communication has been stressed time and again. Lack of communication creates a number of problems that only increase with time.  </p><p>​It is often seen that the families where people talk to each other frequently are happier. They share their joys, sorrows and other emotions with each other. There is a lively atmosphere in such families. They laugh it out together and celebrate every moment whole-heartedly. Such is the importance of communication.  </p><p>​Compared to this, the families where all the members are busy with their own tasks are usually not as happy. The atmosphere in such families is quite dull and gloomy. They plan outings with their friends and colleagues rather than their family members.  </p><p>​This is because they have neglected the importance of communication all along and are thus not comfortable communicating with each other. And without good communication there is no way one can enjoy. People in such families often find themselves lonely and even get into depression.  </p><p>​Similarly, lack of communication is harmful in every situation. We must all realise the importance of communication.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-15">passage_15</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-14">passage_14</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-11">passage_11</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-5">passage_5</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-16"><a href="#PASSAGE-16" class="headerlink" title="PASSAGE 16"></a>PASSAGE 16</h1><p><strong>The Dangers of Plastic Bags</strong> </p><p>​Most of us have seen the heart wrenching images of birds being terminally entangled by plastic bags, and of the deaths of animals due to plastic bag ingestion.  </p><p>​According to the latest report, nearly 200 different species of sea life including whales, dolphins, seals, and turtles die because of plastic bags.  </p><p>​Bags end up in landfills, oceans, seas, and lakes.  </p><p>​Unlike items that naturally biodegrade, plastic bags photodegrade, meaning that over time the plastic breaks down into smaller, more toxic substances. These contaminants poison our soil and water and then enter our food chain.  </p><p>​If we trade our plastic bags for reusable cloth bags, we can help this plastic bag crisis.  </p><p>​Because plastic bags are made from oil, reducing them would decrease foreign oil dependency.  </p><p>​We’d start preserving the thousands of marine animals and more than 1 million of birds who die from plastic bags each year.  </p><p>​It’s a small investment to give ourselves and our children a better future.</p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-16">passage_16</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-15">passage_15</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-12">passage_12</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-6">passage_6</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-17"><a href="#PASSAGE-17" class="headerlink" title="PASSAGE 17"></a>PASSAGE 17</h1><p><strong>Promotion</strong> </p><p>​Before you ask for a promotion, be sure that you’re doing all the right things to help ensure that the answer will be positive. </p><p>​1.Do a Great Job.  </p><p>​How you perform in your current position is going to be important when you’re considered for a promotion. Excellent performance reviews and your reputation as an above-average employee will carry a lot of weight when the company is making staffing decisions. </p><p>​2.Be a Team Player.  </p><p>​Volunteer to help with new projects in the office. Offer to help your boss and co-workers whenever time permits. You’ll be known as a team player and an individual that colleagues want to work with.  </p><p>​3.Don’t Miss Work.  </p><p>​Be on time for work and don’t take more time off than you are allocated. If you’re known as a slouch and someone who misses more work than is appropriate, it will be held against you.  </p><p>​4.Network and Get Noticed.  </p><p>​Attend company parties and gatherings. The more connected and engaged you are with your colleagues, the more they will know about you and the more you’ll stand out when it comes time to consider you for promotion.  </p><p>​Managers are more likely to promote an employee they know well than a random applicant they don’t know much about. </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-17">passage_17</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-16">passage_16</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-13">passage_13</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-7">passage_7</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-18"><a href="#PASSAGE-18" class="headerlink" title="PASSAGE 18"></a>PASSAGE 18</h1><p><strong>Parents and teenagers</strong> </p><p>​Parents feel that it is difficult to live with teenagers. Teenagers have similar feelings about their parents, saying that it is not easy living with them. According to a recent research, the most common argument between parents and teenagers is that regarding untidiness and daily routine tasks.  </p><p>​On the one hand, parents go mad over messy rooms, clothes thrown on the floor and their children’s refusal to help with the housework.  </p><p>​On the other hand, teenagers lose their patience continually when parents blame them for dropping the towel in the bathroom, not cleaning up their room or refusing to do the shopping at the supermarket.  </p><p>​Psychologists say that communication is the most important thing in parent-child relationships. Parents should talk to their children but at the same time they should lend an ear to what they have to say.  </p><p>​Parents may scold their children when they are untidy but they should also understand that their room is their own private space. Communication is a two-way process. It is only by listening to and understanding each other that problems between parents and children can be settled.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-18">passage_18</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-17">passage_17</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-14">passage_14</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-8">passage_8</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-19"><a href="#PASSAGE-19" class="headerlink" title="PASSAGE 19"></a>PASSAGE 19</h1><p><strong>Develop Better Relationships with Neighbors</strong> </p><p>​Good neighbors are a lot like electricity or running water: we don’t know how much we depend on them until we don’t have them. They make our lives more pleasant and give us a sense of who we are, both as an individual and as a member of the community. Here’s how to develop your relationships with these very important people in your life.  </p><p>​Strike up a conversation. Often neighbors don’t even know each other’s names. But it’s okay to be the one to break the ice , even if you’ve lived next door for years. Most neighbors enjoy making small talk with the folks on the other side of the fence.  </p><p>​Avoid causing inconveniences. Return anything that you borrow from a neighbor, such as tools, in good repair and as soon as you’ve finished with them. Replace anything that belongs to your neighbor that you, your children, or your pets break or soil.  </p><p>​Care about the details. In a good relationship, it’s really the little things that count. Help to bring in the mail for the elderly neighbor when there is a heavy rain.  </p><p>​All it takes to develop your relationship with neighbors is the respect for their feelings.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-19">passage_19</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-18">passage_18</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-15">passage_15</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-9">passage_9</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-20"><a href="#PASSAGE-20" class="headerlink" title="PASSAGE 20"></a>PASSAGE 20</h1><p><strong>Why Should We Protect the Environment?</strong> </p><p>​It is important to protect the environment because man-made disruptions to ecosystems can cause extinction and because pollution creates dangers for both animals and people.  </p><p>​Many of the dangers to the environment come from practices designed to make human life easier but actually threaten the long-term health and prosperity of humans.  </p><p>​One of the most important reasons to protect the environment is because it helps protect different ecosystems. Over the years, humans have discovered that many ecosystems are more complex than was originally thought and are deeply connected to other ecosystems.  </p><p>​Human changes to one ecosystem, whether through environmental pollution or deliberate manipulation, can actually put multiple species in danger of extinction, necessitating the need to protect the environment.  </p><p>​Protecting the environment also protects humanity. A great deal of pollution, especially ocean pollution, ends up affecting creatures that humans later consume, meaning that humans are ingesting toxins.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-20">passage_20</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-19">passage_19</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-16">passage_16</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-10">passage_10</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-21"><a href="#PASSAGE-21" class="headerlink" title="PASSAGE 21"></a>PASSAGE 21</h1><p><strong>Tipping</strong> </p><p>​“Tipping” is handled very differently in the UK and America.  </p><p>​In the UK, the reason for tipping is to show that you really liked the service. In the UK people have a high minimum wage, and no one can be paid below this amount for any work, so when you tip, it is mainly showing politeness to the service person, and to acknowledge that they did an excellent job.  </p><p>​The average tip is around 10-20%. If you didn’t like the service, you can always refuse to pay. Or if you are a student or a traveler on a budget, it’s OK not to spend the extra money.  </p><p>​In the US, you will often hear “tips” called “service charges”. It is essential to tip in the US, because the waitresses and bar staff are paid a low wage which they cannot afford to live on, and therefore your tip is much needed by them.  </p><p>​The business and the servers all expect that a tip will be given. It’s considered to be part of their salary. In some restaurants, the tip will be added to the bill, so it’s not optional. The expected amount of a tip is usually 15%. Of course, if the server did a really good job, you can always give them more.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-21">passage_21</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-20">passage_20</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-17">passage_17</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-11">passage_11</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-22"><a href="#PASSAGE-22" class="headerlink" title="PASSAGE 22"></a>PASSAGE 22</h1><p><strong>Happy Vacation</strong> </p><p>​Many of us invest valuable time, energy and money in planning our vacations. Vacations help us perform better at work, improve our sleep quality and cushion us against depression.  </p><p>​Yet, despite these benefits, many of us return home with a feeling that our last vacation was OK - but not great. In order to change this, some mistakes should be avoided.  </p><p>​A classic one for vacation planners is attempting to maximize value for money by planning trips that have too many components. Hopping from one place to the next hardly gives an opportunity to experience what psychologists call mindfulness - time to take in our new surroundings, time to be present and absorb our travel experiences.  </p><p>​Another mistake is that we worry too much about strategic issues such as how to find a good flight deal, or which destinations to add or subtract from our journey. These issues may seem important, but our psychological state of mind is far more important.  </p><p>​Actually, vacation happiness is based on the following top rules. First, choose your travel companions wisely, because nothing contributes more significantly to a trip than the right companions.  </p><p>​Second, don’t spend your vacation time in a place where everything is too expensive so as to maintain a positive mood.  </p><p>​Third, shop wisely, for meaningful experiences provide more long-term happiness than physical possessions.   </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-22">passage_22</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-21">passage_21</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-18">passage_18</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-12">passage_12</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-23"><a href="#PASSAGE-23" class="headerlink" title="PASSAGE 23"></a>PASSAGE 23</h1><p><strong>Small businesses</strong> </p><p>​Today, I’d like to turn our attention to an area of management often overlooked in traditional management courses: small-business management.  </p><p>​Small businesses have gone from being traditional small-town stores selling food or clothing to sophisticated, high-tech enterprises.  </p><p>​And in addition to the important products they produce, they create jobs for lots of people because there are so many of them.  </p><p>​Writing a business plan that everyone in the organization understands and follows will help to provide the necessary focus and direction.  </p><p>​It’s important to state clearly what the purpose of the business is.  </p><p>​Now, if for some reason a business plan doesn’t work, try hard to discover why not, rewrite it, and immediately focus on the new plan.  </p><p>​A word of caution, however, never give up a bad plan without replacing it.  </p><p>​A business has to have a plan because it can’t afford to waste its limited resources.  </p><p>​And, as you know, waste leads to unnecessarily high production costs.  </p><p>​Production costs are at the heart of the company’s ability to make a high-quality product and sell it at an affordable price.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-23">passage_23</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-22">passage_22</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-19">passage_19</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-13">passage_13</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-24"><a href="#PASSAGE-24" class="headerlink" title="PASSAGE 24"></a>PASSAGE 24</h1><p><strong>Catch a mood</strong> </p><p>​Did you know you can catch a mood?  </p><p>​A bad mood isn’t spread by a virus like the flu is, but it can be contagious.  </p><p>​Moods sort of drift from person to person unconsciously. Slight, unintentional signals carry the mood.  </p><p>​You’ve probably experienced it yourself.  </p><p>​You’re around someone who’s feeling down and showing it:  </p><p>slumped shoulders, downcast mouth, subdued voice all that sort of thing.  </p><p>​Pretty soon you begin to feel depressed too. Of course, good moods are also catching, not just bad ones.  </p><p>​Moods spread in steps.  </p><p>​One person’s facial expression or whatever is observed by another, who then unconsciously begins to mimic.  </p><p>​The person isn’t even aware of the copying.  </p><p>​Not everyone picks up moods to the same degree.  </p><p>​Those who’re most susceptible often have strong physiological responses to what’s going on around them.  </p><p>​People don’t all send moods equally well either.  </p><p>​The best mood senders are expressive people because mood contagion can’t happen without signals.  </p><p>​If they aren’t there, that is, the person gives no indication of the mood they’re in, nobody will pick up the mood.   </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-24">passage_24</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-23">passage_23</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-20">passage_20</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-14">passage_14</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-25"><a href="#PASSAGE-25" class="headerlink" title="PASSAGE 25"></a>PASSAGE 25</h1><p><strong>UN&#x2F; Message on the United Nations Day</strong> </p><p>​United Nations Day marks the birthday of our founding Charter – the landmark document that embodies the hopes, dreams and aspirations of “we the peoples”.  </p><p>​Every day, the women and men of the United Nations work to give practical meaning to that Charter.  </p><p>​Despite the odds and the obstacles, we never give up.  </p><p>​Extreme poverty is being reduced but we see inequality growing.  </p><p>​Yet we don’t give up because we know by reducing inequality we increase hope and opportunity and peace around the world.  </p><p>​Climate change is moving faster than we are, but we don’t give up because we know that climate action is the only path.  </p><p>​Human rights are being violated in so many places. But we don’t give up because we know respect for human rights and human dignity is a basic condition for peace.  </p><p>​Conflicts are multiplying - people are suffering. But we don’t give up because we know every man, woman and child deserves a life of peace.  </p><p>​On United Nations Day, let us reaffirm our commitment.  </p><p>​To repair broken trust.  </p><p>​To heal our planet.  </p><p>​To leave no one behind.  </p><p>​To uphold dignity for one and all, as united nations.   </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-25">passage_25</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-24">passage_24</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-21">passage_21</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-15">passage_15</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-26"><a href="#PASSAGE-26" class="headerlink" title="PASSAGE 26"></a>PASSAGE 26</h1><p><strong>AI</strong> </p><p>​Many researchers use the term artificial intelligence (AI) to describe the thinking and intelligent behavior demonstrated by machines. While AI can be helpful to human beings, scientists warn, it can also be a threat.  </p><p>​In the future, many of today’s jobs that require a lot of physical labor will be replaced by machines with artificial intelligence. These include agricultural jobs, and ones with repetitive duties like telephone call centers.  </p><p>​Artificial intelligence is changing the economy and current way of life.  </p><p>​The health care industry is one area artificial intelligence is already changing. AI can process huge amounts of data, so doctors can use the most up-to-date information to diagnose and treat patients.  </p><p>​There is also a dangerous side to artificial intelligence. One example is combining drone aircraft technology and AI to create autonomous weapons. It means that people will use this technology to develop a kind of “poor man’s nuclear weapon.”  </p><p>​The debate about whether artificial intelligence is good – or bad – for humans continues. Researchers agree there is no question that the technology is here, and will continue to change life on our planet.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-26">passage_26</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-25">passage_25</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-22">passage_22</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-16">passage_16</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-27"><a href="#PASSAGE-27" class="headerlink" title="PASSAGE 27"></a>PASSAGE 27</h1><p><strong>The Benefits of E-commerce</strong> </p><p>​There is a reason why E-commerce has demonstrated such explosive growth in the past couple of years. Indeed, with the internet becoming an essential requirement of everyday life, businesses are learning to take advantage of the numerous benefits of E-commerce, the most notable of which include:  </p><p>​Global market. A physical store will always be limited by a geographical area it can serve. An online store has the whole world as its market. Going from a local customer base to a global market at no additional cost is really one of the greatest advantages of trading online.  </p><p>​Around-the-clock availability. Another great benefit of running an online business is that it is always open. For a merchant, it’s a dramatic increase in sales opportunities; for a customer, it’s a convenient and immediately available option.  </p><p>​Reduced costs. E-commerce businesses benefit from significantly lower running costs. As there’s no need to hire sales staff or maintain a physical storefront, the merchants are able to save on operational costs and they can offer better deals and discounts to their customers.  </p><p>​Inventory management. E-commerce businesses can automate their inventory management by using electronic tools to accelerate ordering, delivery and payment procedures.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-27">passage_27</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-26">passage_26</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-23">passage_23</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-17">passage_17</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-28"><a href="#PASSAGE-28" class="headerlink" title="PASSAGE 28"></a>PASSAGE 28</h1><p><strong>Differences Between Chinese &amp; American Education</strong> </p><p>​What are the main differences between the Chinese and American approach to education?  </p><p>​The Chinese and American education systems have different aims. Many parents and educators view Chinese education as important for foundation, and American education as being helpful for the cultivation of students’ creativity.  </p><p>​Chinese education focuses on the accumulation of knowledge , on how students manage and use the knowledge they learned in school, and on understandings of knowledge systems and structures.  </p><p>​Americans are interested in how students use their knowledge in society. The American system lets students criticize ideas, and challenge as well as create concepts.  </p><p>​The ways that American and Chinese students receive knowledge are also different. When they are doing homework, Chinese students just take notes and memorize the facts in textbooks.  </p><p>​American teachers also assess students’ creativity, leadership, and cooperation skills. Emphasizing these skills encourages American students to take part in extracurricular activities. They believe that education is a part of life, not just part of school.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-28">passage_28</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-27">passage_27</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-24">passage_24</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-18">passage_18</a></strong></td></tr></tbody></table><hr><p> </p><h1 id="PASSAGE-29"><a href="#PASSAGE-29" class="headerlink" title="PASSAGE 29"></a>PASSAGE 29</h1><p><strong>The 4 Stages of Culture Shock</strong> </p><p>​Living abroad can be an exhilarating experience that encourages new world views, increases cultural curiosity and supports willingness to explore unfamiliar terrains. However, it may also invite a sense of feeling a little lost in the world.  </p><p>​Culture shock generally moves through four different phases: honeymoon, frustration, adjustment and acceptance. </p><p>​1. The Honeymoon Stage  </p><p>​The first stage of culture shock is often overwhelmingly positive during which travelers become infatuated with the language, people and food in their new surroundings. </p><p>​2. The Frustration Stage  </p><p>​At this stage, the fatigue of not understanding gestures, signs and the language sets in and miscommunications may be happening frequently. </p><p>​3. The Adjustment Stage  </p><p>​Frustrations are often subdued as travelers begin to feel more familiar and comfortable with the cultures, people, food and languages of new environments. </p><p>​4. The Acceptance Stage  </p><p>​During the acceptance stage, travelers have the familiarity and are able to draw together the resources they need to feel at ease.  </p><p>​Though it can be one of the hardest part of traveling, culture shock is just as integral to the experience as food, people and scenery. By recognizing it for what it is and finding ways to cope, you can prevent culture shock from ruining an otherwise fantastic experience abroad.   </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-29">passage_29</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-28">passage_28</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-25">passage_25</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-19">passage_19</a></strong></td></tr></tbody></table><hr><h1 id="PASSAGE-30"><a href="#PASSAGE-30" class="headerlink" title="PASSAGE 30"></a>PASSAGE 30</h1><p><strong>SMOG IN BEIJING</strong> </p><p>​Earlier this week, a thick layer of smog rolled into China’s capital city, turning skyscrapers into shadows and clear air into a yellow fog.   </p><p>​Why does smog keep blanketing Beijing? Smog in China has many causes, including pollution from industries and traffic, but it tends to happen more often in the winter, when plummeting temperatures cause electricity demand to soar, and coal-fired power plants send more tiny particles of dust into the air.  </p><p>​Particles smaller than 10 micrometers are of particular concern to health experts, as particles that small can damage the lungs, aggravate asthma, and even cause heart attacks.  </p><p>​China is taking steps to address the issue. The government has restricted driving in an effort to curb air pollution from cars. But it has a long way to go. China is still the world’s largest consumer of coal by far, and plans to continue using the sooty source of power.  </p><p>​There is some cause for optimism. Like Beijing, Los Angeles also had a serious smog problem. But strict regulations implemented over the past few decades have helped reduce the amount of smog in Los Angeles, bringing back blue skies. There’s hope that they could return to China too, though residents shouldn’t expect to breathe easy any time soon.  </p><table><thead><tr><th align="center">Learning And Review</th></tr></thead><tbody><tr><td align="center"><strong><a href="/20240602/#PASSAGE-30">passage_30</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-29">passage_29</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-26">passage_26</a></strong></td></tr><tr><td align="center"><strong><a href="/20240602/#PASSAGE-20">passage_20</a></strong></td></tr></tbody></table><hr><h1 id="Learning-And-Review"><a href="#Learning-And-Review" class="headerlink" title="Learning And Review"></a>Learning And Review</h1><table><thead><tr><th align="center">DAY 31</th><th align="center">DAY 32</th><th align="center">DAY 33</th><th align="center">DAY 34</th><th align="center">DAY 35</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-27">passage_27</a></td><td align="center"><a href="/20240602/#PASSAGE-28">passage_28</a></td><td align="center"><a href="/20240602/#PASSAGE-29">passage_29</a></td><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td><td align="center"></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-21">passage_21</a></td><td align="center"><a href="/20240602/#PASSAGE-22">passage_22</a></td><td align="center"><a href="/20240602/#PASSAGE-23">passage_23</a></td><td align="center"><a href="/20240602/#PASSAGE-24">passage_24</a></td><td align="center"><a href="/20240602/#PASSAGE-25">passage_25</a></td></tr></tbody></table><hr><table><thead><tr><th align="center">DAY 36</th><th align="center">DAY 37</th><th align="center">DAY 38</th><th align="center">DAY 39</th><th align="center">DAY 40</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-26">passage_26</a></td><td align="center"><a href="/20240602/#PASSAGE-27">passage_27</a></td><td align="center"><a href="/20240602/#PASSAGE-28">passage_28</a></td><td align="center"><a href="/20240602/#PASSAGE-29">passage_29</a></td><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td></tr></tbody></table><hr>]]></content>
      
      
      <categories>
          
          <category> 英语学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oral_Practice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语口语练习——学习计划</title>
      <link href="/20240601/"/>
      <url>/20240601/</url>
      
        <content type="html"><![CDATA[<h1 id="初窥门径"><a href="#初窥门径" class="headerlink" title="初窥门径"></a>初窥门径</h1><table><thead><tr><th align="center">DAY 1</th><th align="center">DAY 2</th><th align="center">DAY 3</th><th align="center">DAY 4</th><th align="center">DAY 5</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-1">passage_1</a></td><td align="center"><a href="/20240602/#PASSAGE-2">passage_2</a></td><td align="center"><a href="/20240602/#PASSAGE-3">passage_3</a></td><td align="center"><a href="/20240602/#PASSAGE-4">passage_4</a></td><td align="center"><a href="/20240602/#PASSAGE-5">passage_5</a></td></tr><tr><td align="center"></td><td align="center"><a href="/20240602/#PASSAGE-1">passage_1</a></td><td align="center"><a href="/20240602/#PASSAGE-2">passage_2</a></td><td align="center"><a href="/20240602/#PASSAGE-3">passage_3</a></td><td align="center"><a href="/20240602/#PASSAGE-4">passage_4</a></td></tr><tr><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"><a href="/20240602/#PASSAGE-1">passage_1</a></td></tr></tbody></table><hr><h1 id="渐入佳境"><a href="#渐入佳境" class="headerlink" title="渐入佳境"></a>渐入佳境</h1><table><thead><tr><th align="center">DAY 6</th><th align="center">DAY 7</th><th align="center">DAY 8</th><th align="center">DAY 9</th><th align="center">DAY 10</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-6">passage_6</a></td><td align="center"><a href="/20240602/#PASSAGE-7">passage_7</a></td><td align="center"><a href="/20240602/#PASSAGE-8">passage_8</a></td><td align="center"><a href="/20240602/#PASSAGE-9">passage_9</a></td><td align="center"><a href="/20240602/#PASSAGE-10">passage_10</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-5">passage_5</a></td><td align="center"><a href="/20240602/#PASSAGE-6">passage_6</a></td><td align="center"><a href="/20240602/#PASSAGE-7">passage_7</a></td><td align="center"><a href="/20240602/#PASSAGE-8">passage_8</a></td><td align="center"><a href="/20240602/#PASSAGE-9">passage_9</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-2">passage_2</a></td><td align="center"><a href="/20240602/#PASSAGE-3">passage_3</a></td><td align="center"><a href="/20240602/#PASSAGE-4">passage_4</a></td><td align="center"><a href="/20240602/#PASSAGE-5">passage_5</a></td><td align="center"><a href="/20240602/#PASSAGE-6">passage_6</a></td></tr></tbody></table><hr><h1 id="出类拔萃"><a href="#出类拔萃" class="headerlink" title="出类拔萃"></a>出类拔萃</h1><table><thead><tr><th align="center">DAY 11</th><th align="center">DAY 12</th><th align="center">DAY 13</th><th align="center">DAY 14</th><th align="center">DAY 15</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-11">passage_11</a></td><td align="center"><a href="/20240602/#PASSAGE-12">passage_12</a></td><td align="center"><a href="/20240602/#PASSAGE-13">passage_13</a></td><td align="center"><a href="/20240602/#PASSAGE-14">passage_14</a></td><td align="center"><a href="/20240602/#PASSAGE-15">passage_15</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-10">passage_10</a></td><td align="center"><a href="/20240602/#PASSAGE-11">passage_11</a></td><td align="center"><a href="/20240602/#PASSAGE-12">passage_12</a></td><td align="center"><a href="/20240602/#PASSAGE-13">passage_13</a></td><td align="center"><a href="/20240602/#PASSAGE-14">passage_14</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-7">passage_7</a></td><td align="center"><a href="/20240602/#PASSAGE-8">passage_8</a></td><td align="center"><a href="/20240602/#PASSAGE-9">passage_9</a></td><td align="center"><a href="/20240602/#PASSAGE-10">passage_10</a></td><td align="center"><a href="/20240602/#PASSAGE-11">passage_11</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-1">passage_1</a></td><td align="center"><a href="/20240602/#PASSAGE-2">passage_2</a></td><td align="center"><a href="/20240602/#PASSAGE-3">passage_3</a></td><td align="center"><a href="/20240602/#PASSAGE-4">passage_4</a></td><td align="center"><a href="/20240602/#PASSAGE-5">passage_5</a></td></tr></tbody></table><hr><h1 id="炉火纯青"><a href="#炉火纯青" class="headerlink" title="炉火纯青"></a>炉火纯青</h1><table><thead><tr><th align="center">DAY 16</th><th align="center">DAY 17</th><th align="center">DAY 18</th><th align="center">DAY 19</th><th align="center">DAY 20</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-16">passage_16</a></td><td align="center"><a href="/20240602/#PASSAGE-17">passage_17</a></td><td align="center"><a href="/20240602/#PASSAGE-18">passage_18</a></td><td align="center"><a href="/20240602/#PASSAGE-19">passage_19</a></td><td align="center"><a href="/20240602/#PASSAGE-20">passage_20</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-15">passage_15</a></td><td align="center"><a href="/20240602/#PASSAGE-16">passage_16</a></td><td align="center"><a href="/20240602/#PASSAGE-17">passage_17</a></td><td align="center"><a href="/20240602/#PASSAGE-18">passage_18</a></td><td align="center"><a href="/20240602/#PASSAGE-19">passage_19</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-12">passage_12</a></td><td align="center"><a href="/20240602/#PASSAGE-13">passage_13</a></td><td align="center"><a href="/20240602/#PASSAGE-14">passage_14</a></td><td align="center"><a href="/20240602/#PASSAGE-15">passage_15</a></td><td align="center"><a href="/20240602/#PASSAGE-16">passage_16</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-6">passage_6</a></td><td align="center"><a href="/20240602/#PASSAGE-7">passage_7</a></td><td align="center"><a href="/20240602/#PASSAGE-8">passage_8</a></td><td align="center"><a href="/20240602/#PASSAGE-9">passage_9</a></td><td align="center"><a href="/20240602/#PASSAGE-10">passage_10</a></td></tr></tbody></table><hr><h1 id="臻至化境"><a href="#臻至化境" class="headerlink" title="臻至化境"></a>臻至化境</h1><table><thead><tr><th align="center">DAY 21</th><th align="center">DAY 22</th><th align="center">DAY 23</th><th align="center">DAY 24</th><th align="center">DAY 25</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-21">passage_21</a></td><td align="center"><a href="/20240602/#PASSAGE-22">passage_22</a></td><td align="center"><a href="/20240602/#PASSAGE-23">passage_23</a></td><td align="center"><a href="/20240602/#PASSAGE-24">passage_24</a></td><td align="center"><a href="/20240602/#PASSAGE-25">passage_25</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-20">passage_20</a></td><td align="center"><a href="/20240602/#PASSAGE-21">passage_21</a></td><td align="center"><a href="/20240602/#PASSAGE-22">passage_22</a></td><td align="center"><a href="/20240602/#PASSAGE-23">passage_23</a></td><td align="center"><a href="/20240602/#PASSAGE-24">passage_24</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-17">passage_17</a></td><td align="center"><a href="/20240602/#PASSAGE-18">passage_18</a></td><td align="center"><a href="/20240602/#PASSAGE-19">passage_19</a></td><td align="center"><a href="/20240602/#PASSAGE-20">passage_20</a></td><td align="center"><a href="/20240602/#PASSAGE-21">passage_21</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-11">passage_11</a></td><td align="center"><a href="/20240602/#PASSAGE-12">passage_12</a></td><td align="center"><a href="/20240602/#PASSAGE-13">passage_13</a></td><td align="center"><a href="/20240602/#PASSAGE-14">passage_14</a></td><td align="center"><a href="/20240602/#PASSAGE-15">passage_15</a></td></tr></tbody></table><hr><h1 id="超凡脱俗"><a href="#超凡脱俗" class="headerlink" title="超凡脱俗"></a>超凡脱俗</h1><table><thead><tr><th align="center">DAY 26</th><th align="center">DAY 27</th><th align="center">DAY 28</th><th align="center">DAY 29</th><th align="center">DAY 30</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-26">passage_26</a></td><td align="center"><a href="/20240602/#PASSAGE-27">passage_27</a></td><td align="center"><a href="/20240602/#PASSAGE-28">passage_28</a></td><td align="center"><a href="/20240602/#PASSAGE-29">passage_29</a></td><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-25">passage_25</a></td><td align="center"><a href="/20240602/#PASSAGE-26">passage_26</a></td><td align="center"><a href="/20240602/#PASSAGE-27">passage_27</a></td><td align="center"><a href="/20240602/#PASSAGE-28">passage_28</a></td><td align="center"><a href="/20240602/#PASSAGE-29">passage_29</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-22">passage_22</a></td><td align="center"><a href="/20240602/#PASSAGE-23">passage_23</a></td><td align="center"><a href="/20240602/#PASSAGE-24">passage_24</a></td><td align="center"><a href="/20240602/#PASSAGE-25">passage_25</a></td><td align="center"><a href="/20240602/#PASSAGE-26">passage_26</a></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-16">passage_16</a></td><td align="center"><a href="/20240602/#PASSAGE-17">passage_17</a></td><td align="center"><a href="/20240602/#PASSAGE-18">passage_18</a></td><td align="center"><a href="/20240602/#PASSAGE-19">passage_19</a></td><td align="center"><a href="/20240602/#PASSAGE-20">passage_20</a></td></tr></tbody></table><hr><h1 id="登峰造极"><a href="#登峰造极" class="headerlink" title="登峰造极"></a>登峰造极</h1><table><thead><tr><th align="center">DAY 31</th><th align="center">DAY 32</th><th align="center">DAY 33</th><th align="center">DAY 34</th><th align="center">DAY 35</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td><td align="center"></td><td align="center"></td><td align="center"></td><td align="center"></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-27">passage_27</a></td><td align="center"><a href="/20240602/#PASSAGE-28">passage_28</a></td><td align="center"><a href="/20240602/#PASSAGE-29">passage_29</a></td><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td><td align="center"></td></tr><tr><td align="center"><a href="/20240602/#PASSAGE-21">passage_21</a></td><td align="center"><a href="/20240602/#PASSAGE-22">passage_22</a></td><td align="center"><a href="/20240602/#PASSAGE-23">passage_23</a></td><td align="center"><a href="/20240602/#PASSAGE-24">passage_24</a></td><td align="center"><a href="/20240602/#PASSAGE-25">passage_25</a></td></tr></tbody></table><hr><h1 id="返璞归真"><a href="#返璞归真" class="headerlink" title="返璞归真"></a>返璞归真</h1><table><thead><tr><th align="center">DAY 36</th><th align="center">DAY 37</th><th align="center">DAY 38</th><th align="center">DAY 39</th><th align="center">DAY 40</th></tr></thead><tbody><tr><td align="center"><a href="/20240602/#PASSAGE-26">passage_26</a></td><td align="center"><a href="/20240602/#PASSAGE-27">passage_27</a></td><td align="center"><a href="/20240602/#PASSAGE-28">passage_28</a></td><td align="center"><a href="/20240602/#PASSAGE-29">passage_29</a></td><td align="center"><a href="/20240602/#PASSAGE-30">passage_30</a></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 英语学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oral_Practice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>安装Cloudflare WARP+ ，并设置6种模式使用（Linux）</title>
      <link href="/20240402/"/>
      <url>/20240402/</url>
      
        <content type="html"><![CDATA[<p><strong>注：由于Cloudflare被墙，该方法应用后无效</strong></p><p><strong>前言  Cloudflare 提供的 6 种模式</strong></p><ol><li><code>warp</code>: 连接远端服务器, 并使用 tun 进行全局代理</li><li><code>doh</code>: 仅开启 DNS over HTTPS 功能</li><li><code>warp+doh</code>: 开启 Warp 和 DOH 功能</li><li><code>dot</code>: 仅开启 DNS over TLS 功能</li><li><code>warp+dot</code>: 开启 Warp 和 DOT 功能</li><li><code>proxy</code>：连接远端服务器，并在本地开启代理端口</li></ol><p>如果不理解什么是<code>DOH</code>和<code>DOT</code>，参考 Cloudflare 的介绍：<a href="https://www.cloudflare.com/zh-cn/learning/dns/dns-over-tls/">https://www.cloudflare.com/zh-cn/learning/dns/dns-over-tls/</a> ，也可以自行在互联网搜索其他资料，例如：</p><p>DOH：<a href="https://zh.wikipedia.org/wiki/DNS_over_HTTPS">https://zh.wikipedia.org/wiki/DNS_over_HTTPS</a></p><p>DOT：<a href="https://zh.wikipedia.org/wiki/DNS_over_TLS">https://zh.wikipedia.org/wiki/DNS_over_TLS</a></p><p>带有<code>warp</code>的三种模式，都是依托于 <code>TUN</code> 实现的全局代理。<code>doh</code>和<code>dot</code>又只解决了 DNS 安全问题。</p><p>如果此时需要依托于规则进行流量分配，以上 5 种模式都无法满足。</p><p>如果不理解什么是<code>TUN</code>, 参考这篇文章：<a href="https://www.sobyte.net/post/2022-04/tun-mode/">https://www.sobyte.net/post/2022-04/tun-mode/</a></p><p>Cloudflare Warp提供了Proxy模式，可以不启用全局模式，且在本地监听一个http+socks5混合端口。</p><h1 id="一、cloudflare安装"><a href="#一、cloudflare安装" class="headerlink" title="一、cloudflare安装"></a>一、cloudflare安装</h1><p>官方网址: <a href="https://1.1.1.1/">https://1.1.1.1/</a></p><p>ArchLinux 从AUR安装 cloudflare-warp-bin</p><p>​——以AUR中详细介绍为准</p><h1 id="二、cloudflare使用"><a href="#二、cloudflare使用" class="headerlink" title="二、cloudflare使用"></a>二、cloudflare使用</h1><p><strong>注：第一次使用要在终端应用临时代理</strong></p><p>临时配置：命令行设置——全部走代理</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export all_proxy=&quot;http://局域网ip:1080/&quot;</span><br></pre></td></tr></table></figure><p>测试代理是否生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -vv https://www.google.com</span><br></pre></td></tr></table></figure><h2 id="WARP"><a href="#WARP" class="headerlink" title="WARP"></a>WARP</h2><p>只需要 3 个命令就能使用 WARP，先启动 warp-svc.server，第一次使用时需要 register 进行身份验证，然后 connect 将启用客户端，创建从设备到 Cloudflare 网络的 WireGuard 隧道。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜ sudo systemctl start warp-svc.service // 启动服务 </span><br><span class="line"></span><br><span class="line">➜ warp-cli register // 进行身份验证 </span><br><span class="line">Success </span><br><span class="line"></span><br><span class="line">➜ warp-cli connect // 连接 </span><br><span class="line">Success</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>此时查看帐户信息可以看到 Account type: Free</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">➜ warp-cli account </span><br><span class="line">Account type: Free </span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>通过 cloudflare trace 看到 warp&#x3D;on</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ curl https://www.cloudflare.com/cdn-cgi/trace/ </span><br><span class="line">... </span><br><span class="line">warp=on </span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>至此 WARP 就可以使用了。</p><h2 id="WARP-1"><a href="#WARP-1" class="headerlink" title="WARP+"></a>WARP+</h2><p>使用 WARP+ 需要去 <a href="https://t.me/generatewarpplusbot">Warp+ Bot</a> 获得 <strong>License Key</strong>，然后 set-license使用 Key</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ warp-cli set-license xxxxxxxx-xxxxxxxx-xxxxxxxx // 得到的 License Key </span><br><span class="line">Success</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>这时查看帐户信息，可以看到 Account type: Limited，有 <strong>24598562000000000 B &#x3D; 24.5986 PB</strong> 流量配额，根本用不完。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">➜ warp-cli account                     </span><br><span class="line">Account type: Limited </span><br><span class="line">... </span><br><span class="line">Quota: 24598562000000000 </span><br><span class="line">Premium Data: 24598562000000000</span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>通过 cloudflare trace 看到 warp&#x3D;plus</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">➜ curl https://www.cloudflare.com/cdn-cgi/trace/ </span><br><span class="line">... </span><br><span class="line">warp=plus </span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>至此 WARP+ 就可以使用了。</p><h2 id="MODE"><a href="#MODE" class="headerlink" title="MODE"></a>MODE</h2><p>mode 有多种，根据需求设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">➜ warp-cli set-mode -h </span><br><span class="line">Set the mode </span><br><span class="line"></span><br><span class="line">Usage: warp-cli set-mode &lt;MODE&gt; </span><br><span class="line">或：warp-cli mode &lt;MODE&gt; </span><br><span class="line">Arguments:  </span><br><span class="line">&lt;MODE&gt;  [possible values: warp, doh, warp+doh, dot, warp+dot, proxy, tunnel_only] </span><br><span class="line"></span><br><span class="line">Options:  </span><br><span class="line">-h, --help  Print help</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>我需要 proxy 模式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜ warp-cli set-mode proxy // 默认端口是 40000, 127.0.0.1:40000 </span><br><span class="line">Success</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>查看一下 ip 信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜ curl ipinfo.io -x socks5://127.0.0.1:40000 </span><br><span class="line">&#123;  </span><br><span class="line">&quot;ip&quot;: &quot;104.28.xxx.xxx&quot;,  </span><br><span class="line">&quot;city&quot;: &quot;Fremont&quot;,  </span><br><span class="line">&quot;region&quot;: &quot;California&quot;,  </span><br><span class="line">&quot;country&quot;: &quot;US&quot;,  </span><br><span class="line">&quot;loc&quot;: &quot;xx.5483,-xxx.9886&quot;,  </span><br><span class="line">&quot;org&quot;: &quot;AS13335 Cloudflare, Inc.&quot;,  </span><br><span class="line">&quot;postal&quot;: &quot;94536&quot;,  </span><br><span class="line">&quot;timezone&quot;: &quot;America/Los_Angeles&quot;,  </span><br><span class="line">&quot;readme&quot;: &quot;https://ipinfo.io/missingauth&quot; </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>更多命令warp-cli –help</p><h1 id="三、开机自启动"><a href="#三、开机自启动" class="headerlink" title="三、开机自启动"></a>三、开机自启动</h1><h2 id="1、将服务设置成开机自启动"><a href="#1、将服务设置成开机自启动" class="headerlink" title="1、将服务设置成开机自启动"></a>1、将服务设置成开机自启动</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查看是否设置为开机自启动 </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl <span class="built_in">enable</span> warp-svc.service</span> </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl list-unit-files|grep warp-svc</span> </span><br><span class="line">-------------------------- warp-svc.service                      enabled</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="2、将服务段开启自启取消"><a href="#2、将服务段开启自启取消" class="headerlink" title="2、将服务段开启自启取消"></a>2、将服务段开启自启取消</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl <span class="built_in">disable</span> warp-svc.service</span> </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl list-unit-files|grep warp-svc</span></span><br><span class="line">-------------------------- warp-svc.service                      disabled</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参考</strong></p><ul><li><a href="https://developers.cloudflare.com/warp-client/get-started/linux/">https://developers.cloudflare.com/warp-client/get-started/linux/</a></li><li><a href="https://blog.csdn.net/sayyy/article/details/79276575">https://blog.csdn.net/sayyy/article/details/79276575</a></li><li><a href="https://www.antergone.com/archives/34.html">https://www.antergone.com/archives/34.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 软件操作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ubuntu </tag>
            
            <tag> ArchLinux </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/20240401/"/>
      <url>/20240401/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
